#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ¨¡å‹è¯„ä¼°å‘½ä»¤è¡Œå·¥å…·
æä¾›å‘½ä»¤è¡Œæ¥å£ï¼Œç”¨äºè¿è¡Œæ¨¡å‹è¯„ä¼°å’ŒåŸºå‡†æµ‹è¯•
"""

import os
import sys
import argparse
from typing import List, Dict, Tuple, Any
import importlib
import json
import time
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from core_modules import logger_manager, data_manager, cache_manager

# å¯¼å…¥æ¨¡å‹è¯„ä¼°æ¡†æ¶
try:
    from improvements.model_evaluation import get_model_evaluator
    EVALUATOR_AVAILABLE = True
except ImportError:
    EVALUATOR_AVAILABLE = False
    print("âš ï¸ æ¨¡å‹è¯„ä¼°æ¡†æ¶æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿improvements/model_evaluation.pyæ–‡ä»¶å­˜åœ¨")

# å¯¼å…¥æ¨¡å‹åŸºå‡†æµ‹è¯•æ¡†æ¶
try:
    from improvements.model_benchmark import get_model_benchmark
    BENCHMARK_AVAILABLE = True
except ImportError:
    BENCHMARK_AVAILABLE = False
    print("âš ï¸ æ¨¡å‹åŸºå‡†æµ‹è¯•æ¡†æ¶æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿improvements/model_benchmark.pyæ–‡ä»¶å­˜åœ¨")

# å¯¼å…¥æ¨¡å‹ä¼˜åŒ–å™¨
try:
    from improvements.model_optimizer import ModelOptimizer
    OPTIMIZER_AVAILABLE = True
except ImportError:
    OPTIMIZER_AVAILABLE = False
    print("âš ï¸ æ¨¡å‹ä¼˜åŒ–å™¨æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿improvements/model_optimizer.pyæ–‡ä»¶å­˜åœ¨")


def load_predictor_module(module_path, class_name=None):
    """åŠ è½½é¢„æµ‹å™¨æ¨¡å—
    
    Args:
        module_path: æ¨¡å—è·¯å¾„
        class_name: ç±»åï¼ˆå¯é€‰ï¼‰
        
    Returns:
        module or class: åŠ è½½çš„æ¨¡å—æˆ–ç±»
    """
    try:
        module = importlib.import_module(module_path)
        if class_name:
            return getattr(module, class_name)
        return module
    except ImportError:
        print(f"âš ï¸ æ— æ³•å¯¼å…¥æ¨¡å—: {module_path}")
        return None
    except AttributeError:
        print(f"âš ï¸ æ¨¡å— {module_path} ä¸­æ²¡æœ‰æ‰¾åˆ°ç±»: {class_name}")
        return None


def register_default_predictors(benchmark):
    """æ³¨å†Œé»˜è®¤é¢„æµ‹å™¨
    
    Args:
        benchmark: åŸºå‡†æµ‹è¯•å®ä¾‹
        
    Returns:
        int: æ³¨å†Œçš„é¢„æµ‹å™¨æ•°é‡
    """
    registered_count = 0
    
    # æ³¨å†Œéšæœºé¢„æµ‹å™¨ï¼ˆåŸºå‡†ï¼‰
    def mock_random_predict(historical_data):
        """éšæœºé¢„æµ‹"""
        import numpy as np
        return [(sorted(np.random.choice(range(1, 36), 5, replace=False)), 
                sorted(np.random.choice(range(1, 13), 2, replace=False))) for _ in range(3)]
    
    benchmark.register_model("éšæœºé¢„æµ‹", mock_random_predict, "å®Œå…¨éšæœºçš„é¢„æµ‹æ–¹æ³•", "baseline")
    registered_count += 1
    
    # å°è¯•æ³¨å†Œä¼ ç»Ÿé¢„æµ‹å™¨
    try:
        from predictor_modules import get_traditional_predictor
        traditional = get_traditional_predictor()
        
        benchmark.register_model(
            "é¢‘ç‡é¢„æµ‹", 
            lambda data: traditional.frequency_predict(3), 
            "åŸºäºå†å²é¢‘ç‡çš„é¢„æµ‹æ–¹æ³•", 
            "traditional"
        )
        
        benchmark.register_model(
            "å†·çƒ­å·é¢„æµ‹", 
            lambda data: traditional.hot_cold_predict(3), 
            "åŸºäºå†·çƒ­å·çš„é¢„æµ‹æ–¹æ³•", 
            "traditional"
        )
        
        benchmark.register_model(
            "é—æ¼å€¼é¢„æµ‹", 
            lambda data: traditional.missing_predict(3), 
            "åŸºäºé—æ¼å€¼çš„é¢„æµ‹æ–¹æ³•", 
            "traditional"
        )
        
        registered_count += 3
    except ImportError:
        print("âš ï¸ ä¼ ç»Ÿé¢„æµ‹å™¨æ¨¡å—æœªæ‰¾åˆ°")
    
    # å°è¯•æ³¨å†Œé«˜çº§é¢„æµ‹å™¨
    try:
        from predictor_modules import get_advanced_predictor
        advanced = get_advanced_predictor()
        
        benchmark.register_model(
            "é©¬å°”å¯å¤«é¢„æµ‹", 
            lambda data: advanced.markov_predict(3), 
            "åŸºäºé©¬å°”å¯å¤«é“¾çš„é¢„æµ‹æ–¹æ³•", 
            "advanced"
        )
        
        benchmark.register_model(
            "è´å¶æ–¯é¢„æµ‹", 
            lambda data: advanced.bayesian_predict(3), 
            "åŸºäºè´å¶æ–¯åˆ†æçš„é¢„æµ‹æ–¹æ³•", 
            "advanced"
        )
        
        benchmark.register_model(
            "é›†æˆé¢„æµ‹", 
            lambda data: advanced.ensemble_predict(3), 
            "åŸºäºå¤šç§ç®—æ³•é›†æˆçš„é¢„æµ‹æ–¹æ³•", 
            "advanced"
        )
        
        registered_count += 3
    except ImportError:
        print("âš ï¸ é«˜çº§é¢„æµ‹å™¨æ¨¡å—æœªæ‰¾åˆ°")
    
    # å°è¯•æ³¨å†Œå¢å¼ºé©¬å°”å¯å¤«é¢„æµ‹å™¨
    try:
        from improvements.enhanced_markov import get_markov_predictor
        markov_predictor = get_markov_predictor()
        
        benchmark.register_model(
            "äºŒé˜¶é©¬å°”å¯å¤«", 
            lambda data: markov_predictor.multi_order_markov_predict(3, 300, 2), 
            "åŸºäºäºŒé˜¶é©¬å°”å¯å¤«é“¾çš„é¢„æµ‹æ–¹æ³•", 
            "enhanced"
        )
        
        benchmark.register_model(
            "ä¸‰é˜¶é©¬å°”å¯å¤«", 
            lambda data: markov_predictor.multi_order_markov_predict(3, 300, 3), 
            "åŸºäºä¸‰é˜¶é©¬å°”å¯å¤«é“¾çš„é¢„æµ‹æ–¹æ³•", 
            "enhanced"
        )
        
        registered_count += 2
    except ImportError:
        print("âš ï¸ å¢å¼ºé©¬å°”å¯å¤«æ¨¡å—æœªæ‰¾åˆ°")
    
    # å°è¯•æ³¨å†ŒLSTMé¢„æµ‹å™¨
    try:
        from advanced_lstm_predictor import AdvancedLSTMPredictor, TENSORFLOW_AVAILABLE
        if TENSORFLOW_AVAILABLE:
            lstm_predictor = AdvancedLSTMPredictor()
            
            benchmark.register_model(
                "LSTMæ·±åº¦å­¦ä¹ ", 
                lambda data: lstm_predictor.lstm_predict(3), 
                "åŸºäºLSTMæ·±åº¦å­¦ä¹ çš„é¢„æµ‹æ–¹æ³•", 
                "deep_learning"
            )
            
            registered_count += 1
        else:
            print("âš ï¸ TensorFlowæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨LSTMé¢„æµ‹å™¨")
    except ImportError:
        print("âš ï¸ LSTMé¢„æµ‹å™¨æ¨¡å—æœªæ‰¾åˆ°")
    
    return registered_count


def register_custom_predictor(benchmark, module_path, class_name, method_name, model_name, category):
    """æ³¨å†Œè‡ªå®šä¹‰é¢„æµ‹å™¨
    
    Args:
        benchmark: åŸºå‡†æµ‹è¯•å®ä¾‹
        module_path: æ¨¡å—è·¯å¾„
        class_name: ç±»å
        method_name: æ–¹æ³•å
        model_name: æ¨¡å‹åç§°
        category: æ¨¡å‹ç±»åˆ«
        
    Returns:
        bool: æ˜¯å¦æ³¨å†ŒæˆåŠŸ
    """
    try:
        # åŠ è½½æ¨¡å—å’Œç±»
        module = importlib.import_module(module_path)
        predictor_class = getattr(module, class_name)
        
        # åˆ›å»ºå®ä¾‹
        if hasattr(module, f"get_{class_name.lower()}"):
            # å¦‚æœæœ‰è·å–å®ä¾‹çš„å‡½æ•°ï¼Œä½¿ç”¨å®ƒ
            get_instance = getattr(module, f"get_{class_name.lower()}")
            predictor = get_instance()
        else:
            # å¦åˆ™ç›´æ¥å®ä¾‹åŒ–
            predictor = predictor_class()
        
        # è·å–é¢„æµ‹æ–¹æ³•
        predict_method = getattr(predictor, method_name)
        
        # æ³¨å†Œæ¨¡å‹
        benchmark.register_model(
            model_name,
            lambda data: predict_method(data) if 'data' in predict_method.__code__.co_varnames else predict_method(3),
            f"è‡ªå®šä¹‰é¢„æµ‹å™¨: {module_path}.{class_name}.{method_name}",
            category
        )
        
        print(f"âœ… æˆåŠŸæ³¨å†Œè‡ªå®šä¹‰é¢„æµ‹å™¨: {model_name}")
        return True
    except Exception as e:
        print(f"âš ï¸ æ³¨å†Œè‡ªå®šä¹‰é¢„æµ‹å™¨å¤±è´¥: {e}")
        return False


def run_benchmark(args):
    """è¿è¡ŒåŸºå‡†æµ‹è¯•
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    if not BENCHMARK_AVAILABLE:
        print("âŒ æ¨¡å‹åŸºå‡†æµ‹è¯•æ¡†æ¶æœªæ‰¾åˆ°ï¼Œæ— æ³•è¿è¡ŒåŸºå‡†æµ‹è¯•")
        return
    
    # è·å–åŸºå‡†æµ‹è¯•å®ä¾‹
    benchmark = get_model_benchmark()
    
    # æ³¨å†Œé¢„æµ‹å™¨
    if args.register_default:
        print("\nğŸ“ æ³¨å†Œé»˜è®¤é¢„æµ‹å™¨...")
        count = register_default_predictors(benchmark)
        print(f"âœ… æˆåŠŸæ³¨å†Œ {count} ä¸ªé»˜è®¤é¢„æµ‹å™¨")
    
    # æ³¨å†Œè‡ªå®šä¹‰é¢„æµ‹å™¨
    if args.custom_predictor:
        print("\nğŸ“ æ³¨å†Œè‡ªå®šä¹‰é¢„æµ‹å™¨...")
        for predictor_info in args.custom_predictor:
            parts = predictor_info.split(':')
            if len(parts) != 5:
                print(f"âš ï¸ è‡ªå®šä¹‰é¢„æµ‹å™¨æ ¼å¼é”™è¯¯: {predictor_info}")
                print("æ­£ç¡®æ ¼å¼: module_path:class_name:method_name:model_name:category")
                continue
            
            module_path, class_name, method_name, model_name, category = parts
            register_custom_predictor(benchmark, module_path, class_name, method_name, model_name, category)
    
    # åŠ è½½é¢„æµ‹å™¨é…ç½®æ–‡ä»¶
    if args.predictor_config:
        print(f"\nğŸ“ ä»é…ç½®æ–‡ä»¶åŠ è½½é¢„æµ‹å™¨: {args.predictor_config}")
        try:
            with open(args.predictor_config, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            for predictor in config.get('predictors', []):
                register_custom_predictor(
                    benchmark,
                    predictor['module_path'],
                    predictor['class_name'],
                    predictor['method_name'],
                    predictor['model_name'],
                    predictor['category']
                )
        except Exception as e:
            print(f"âš ï¸ åŠ è½½é¢„æµ‹å™¨é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
    
    # è¯„ä¼°æ¨¡å‹
    if args.evaluate_all:
        print(f"\nğŸ“Š è¯„ä¼°æ‰€æœ‰æ¨¡å‹ (æµ‹è¯•æœŸæ•°: {args.test_periods})...")
        benchmark.evaluate_all_models(test_periods=args.test_periods, verbose=True)
    elif args.evaluate:
        for model_name in args.evaluate:
            print(f"\nğŸ“Š è¯„ä¼°æ¨¡å‹: {model_name} (æµ‹è¯•æœŸæ•°: {args.test_periods})...")
            benchmark.evaluate_model(model_name, test_periods=args.test_periods, verbose=True)
    
    # æ¯”è¾ƒæ¨¡å‹
    if args.compare:
        print("\nğŸ”„ æ¯”è¾ƒæ¨¡å‹...")
        categories = args.categories.split(',') if args.categories else None
        metrics = args.metrics.split(',') if args.metrics else None
        benchmark.compare_models(metrics=metrics, categories=categories, verbose=True)
    
    # ç”ŸæˆæŠ¥å‘Š
    if args.report:
        print("\nğŸ“ ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
        report_path = args.report
        benchmark.generate_report(report_path)
        print(f"âœ… è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    
    # å¯è§†åŒ–æ¯”è¾ƒ
    if args.visualize_comparison:
        print("\nğŸ“ˆ å¯è§†åŒ–æ¨¡å‹æ¯”è¾ƒ...")
        metrics = args.metrics.split(',') if args.metrics else None
        benchmark.visualize_comparison(metrics=metrics, output_path=args.visualize_comparison)
        print(f"âœ… æ¯”è¾ƒå›¾è¡¨å·²ä¿å­˜åˆ°: {args.visualize_comparison}")
    
    # å¯è§†åŒ–æ¨¡å‹æ€§èƒ½
    if args.visualize_model:
        for model_name in args.visualize_model:
            print(f"\nğŸ“Š å¯è§†åŒ–æ¨¡å‹æ€§èƒ½: {model_name}...")
            output_path = f"output/{model_name}_performance.png"
            if args.output_dir:
                output_path = os.path.join(args.output_dir, f"{model_name}_performance.png")
            benchmark.visualize_model_performance(model_name, output_path=output_path)
            print(f"âœ… æ€§èƒ½å›¾è¡¨å·²ä¿å­˜åˆ°: {output_path}")
    
    # ä¿å­˜ç»“æœ
    if args.save_results:
        print("\nğŸ’¾ ä¿å­˜è¯„ä¼°ç»“æœ...")
        benchmark.save_results(args.save_results)
        print(f"âœ… è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {args.save_results}")
    
    # åŠ è½½ç»“æœ
    if args.load_results:
        print(f"\nğŸ“‚ åŠ è½½è¯„ä¼°ç»“æœ: {args.load_results}")
        if benchmark.load_results(args.load_results):
            print("âœ… è¯„ä¼°ç»“æœåŠ è½½æˆåŠŸ")
        else:
            print("âŒ è¯„ä¼°ç»“æœåŠ è½½å¤±è´¥")


def run_optimization(args):
    """è¿è¡Œå‚æ•°ä¼˜åŒ–
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    if not OPTIMIZER_AVAILABLE:
        print("âŒ æ¨¡å‹ä¼˜åŒ–å™¨æœªæ‰¾åˆ°ï¼Œæ— æ³•è¿è¡Œå‚æ•°ä¼˜åŒ–")
        return
    
    # åŠ è½½æ¨¡å‹ç±»
    print(f"\nğŸ“ åŠ è½½æ¨¡å‹ç±»: {args.module_path}.{args.class_name}")
    try:
        module = importlib.import_module(args.module_path)
        model_class = getattr(module, args.class_name)
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹ç±»å¤±è´¥: {e}")
        return
    
    # åŠ è½½å‚æ•°ç©ºé—´
    print(f"\nğŸ“ åŠ è½½å‚æ•°ç©ºé—´: {args.param_space}")
    try:
        with open(args.param_space, 'r', encoding='utf-8') as f:
            param_space = json.load(f)
    except Exception as e:
        print(f"âŒ åŠ è½½å‚æ•°ç©ºé—´å¤±è´¥: {e}")
        return
    
    # åˆ›å»ºæ¨¡å‹åˆ›å»ºå‡½æ•°
    def create_model(**params):
        return model_class(**params)
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = ModelOptimizer(
        model_creator=create_model,
        param_space=param_space,
        optimization_method=args.method,
        n_iter=args.n_iter
    )
    
    # æ‰§è¡Œä¼˜åŒ–
    print(f"\nğŸ” å¼€å§‹å‚æ•°ä¼˜åŒ– (æ–¹æ³•: {args.method}, è¿­ä»£æ¬¡æ•°: {args.n_iter})...")
    result = optimizer.optimize(
        train_periods=args.train_periods,
        val_periods=args.val_periods,
        metric=args.metric,
        verbose=True
    )
    
    # æ‰“å°æœ€ä½³å‚æ•°
    print(f"\nğŸ† æœ€ä½³å‚æ•°: {optimizer.best_params}")
    print(f"ğŸ¯ æœ€ä½³å¾—åˆ†: {optimizer.best_score:.4f}")
    
    # å¯è§†åŒ–ä¼˜åŒ–è¿‡ç¨‹
    if args.visualize_process:
        print("\nğŸ“ˆ å¯è§†åŒ–ä¼˜åŒ–è¿‡ç¨‹...")
        optimizer.visualize_optimization_process(args.visualize_process)
        print(f"âœ… ä¼˜åŒ–è¿‡ç¨‹å›¾è¡¨å·²ä¿å­˜åˆ°: {args.visualize_process}")
    
    # å¯è§†åŒ–å‚æ•°é‡è¦æ€§
    if args.visualize_importance:
        print("\nğŸ“Š å¯è§†åŒ–å‚æ•°é‡è¦æ€§...")
        optimizer.visualize_parameter_importance(args.visualize_importance)
        print(f"âœ… å‚æ•°é‡è¦æ€§å›¾è¡¨å·²ä¿å­˜åˆ°: {args.visualize_importance}")
    
    # ä¿å­˜ä¼˜åŒ–ç»“æœ
    if args.save_results:
        print("\nğŸ’¾ ä¿å­˜ä¼˜åŒ–ç»“æœ...")
        optimizer.save_results(args.save_results)
        print(f"âœ… ä¼˜åŒ–ç»“æœå·²ä¿å­˜åˆ°: {args.save_results}")
    
    # ä½¿ç”¨åŸºå‡†æµ‹è¯•æ¡†æ¶è¯„ä¼°æœ€ä½³æ¨¡å‹
    if args.benchmark and BENCHMARK_AVAILABLE:
        print("\nğŸ” ä½¿ç”¨åŸºå‡†æµ‹è¯•æ¡†æ¶è¯„ä¼°æœ€ä½³æ¨¡å‹...")
        benchmark_result = optimizer.benchmark_best_model(
            f"ä¼˜åŒ–åçš„{args.class_name}",
            test_periods=args.test_periods
        )
        
        # æ¯”è¾ƒåŸºå‡†æ¨¡å‹å’Œä¼˜åŒ–æ¨¡å‹
        if args.compare_baseline:
            print("\nğŸ”„ æ¯”è¾ƒåŸºå‡†æ¨¡å‹å’Œä¼˜åŒ–æ¨¡å‹...")
            
            # æ³¨å†ŒåŸºå‡†æ¨¡å‹
            optimizer.benchmark.register_model(
                f"åŸºå‡†{args.class_name}",
                lambda data: model_class().fit(data) or model_class().predict(data),
                f"æœªä¼˜åŒ–çš„{args.class_name}",
                "baseline"
            )
            
            # è¯„ä¼°åŸºå‡†æ¨¡å‹
            optimizer.benchmark.evaluate_model(f"åŸºå‡†{args.class_name}", test_periods=args.test_periods)
            
            # æ¯”è¾ƒæ¨¡å‹
            optimizer.benchmark.compare_models(categories=["baseline", "optimized"])


def run_evaluation(args):
    """è¿è¡Œæ¨¡å‹è¯„ä¼°
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    if not EVALUATOR_AVAILABLE:
        print("âŒ æ¨¡å‹è¯„ä¼°æ¡†æ¶æœªæ‰¾åˆ°ï¼Œæ— æ³•è¿è¡Œæ¨¡å‹è¯„ä¼°")
        return
    
    # è·å–æ¨¡å‹è¯„ä¼°å™¨å®ä¾‹
    evaluator = get_model_evaluator()
    
    # æ³¨å†Œé¢„æµ‹å™¨
    if args.register_default:
        print("\nğŸ“ æ³¨å†Œé»˜è®¤é¢„æµ‹å™¨...")
        count = register_default_predictors(evaluator.benchmark)
        print(f"âœ… æˆåŠŸæ³¨å†Œ {count} ä¸ªé»˜è®¤é¢„æµ‹å™¨")
    
    # æ³¨å†Œè‡ªå®šä¹‰é¢„æµ‹å™¨
    if args.custom_predictor:
        print("\nğŸ“ æ³¨å†Œè‡ªå®šä¹‰é¢„æµ‹å™¨...")
        for predictor_info in args.custom_predictor:
            parts = predictor_info.split(':')
            if len(parts) != 5:
                print(f"âš ï¸ è‡ªå®šä¹‰é¢„æµ‹å™¨æ ¼å¼é”™è¯¯: {predictor_info}")
                print("æ­£ç¡®æ ¼å¼: module_path:class_name:method_name:model_name:category")
                continue
            
            module_path, class_name, method_name, model_name, category = parts
            register_custom_predictor(evaluator.benchmark, module_path, class_name, method_name, model_name, category)
    
    # åŠ è½½é¢„æµ‹å™¨é…ç½®æ–‡ä»¶
    if args.predictor_config:
        print(f"\nğŸ“ ä»é…ç½®æ–‡ä»¶åŠ è½½é¢„æµ‹å™¨: {args.predictor_config}")
        try:
            with open(args.predictor_config, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            for predictor in config.get('predictors', []):
                register_custom_predictor(
                    evaluator.benchmark,
                    predictor['module_path'],
                    predictor['class_name'],
                    predictor['method_name'],
                    predictor['model_name'],
                    predictor['category']
                )
        except Exception as e:
            print(f"âš ï¸ åŠ è½½é¢„æµ‹å™¨é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
    
    # è¯„ä¼°æ¨¡å‹
    if args.evaluate_all:
        print(f"\nğŸ“Š è¯„ä¼°æ‰€æœ‰æ¨¡å‹ (æµ‹è¯•æœŸæ•°: {args.test_periods})...")
        evaluator.evaluate_all_models(test_periods=args.test_periods, verbose=True)
    elif args.evaluate:
        for model_name in args.evaluate:
            print(f"\nğŸ“Š è¯„ä¼°æ¨¡å‹: {model_name} (æµ‹è¯•æœŸæ•°: {args.test_periods})...")
            evaluator.evaluate_model(model_name, test_periods=args.test_periods, verbose=True)
    
    # æ¯”è¾ƒæ¨¡å‹
    if args.compare:
        print("\nğŸ”„ æ¯”è¾ƒæ¨¡å‹...")
        categories = args.categories.split(',') if args.categories else None
        metrics = args.metrics.split(',') if args.metrics else None
        evaluator.compare_models(metrics=metrics, categories=categories, verbose=True)
    
    # ä¼˜åŒ–æ¨¡å‹
    if args.optimize:
        for optimize_info in args.optimize:
            parts = optimize_info.split(':')
            if len(parts) != 3:
                print(f"âš ï¸ ä¼˜åŒ–ä¿¡æ¯æ ¼å¼é”™è¯¯: {optimize_info}")
                print("æ­£ç¡®æ ¼å¼: module_path:class_name:param_space_file")
                continue
            
            module_path, class_name, param_space_file = parts
            
            print(f"\nğŸ” ä¼˜åŒ–æ¨¡å‹: {module_path}.{class_name}")
            
            # åŠ è½½æ¨¡å‹ç±»
            try:
                module = importlib.import_module(module_path)
                model_class = getattr(module, class_name)
            except Exception as e:
                print(f"âŒ åŠ è½½æ¨¡å‹ç±»å¤±è´¥: {e}")
                continue
            
            # åŠ è½½å‚æ•°ç©ºé—´
            try:
                with open(param_space_file, 'r', encoding='utf-8') as f:
                    param_space = json.load(f)
            except Exception as e:
                print(f"âŒ åŠ è½½å‚æ•°ç©ºé—´å¤±è´¥: {e}")
                continue
            
            # åˆ›å»ºæ¨¡å‹åˆ›å»ºå‡½æ•°
            def create_model(**params):
                return model_class(**params)
            
            # æ‰§è¡Œä¼˜åŒ–
            evaluator.optimize_model(
                model_name=class_name,
                model_creator=create_model,
                param_space=param_space,
                optimization_method=args.method,
                n_iter=args.n_iter,
                train_periods=args.train_periods,
                val_periods=args.val_periods,
                metric=args.metric,
                verbose=True
            )
            
            # å¯è§†åŒ–ä¼˜åŒ–è¿‡ç¨‹
            if args.output_dir:
                evaluator.visualize_optimization(class_name, args.output_dir)
    
    # ç”ŸæˆæŠ¥å‘Š
    if args.report:
        print("\nğŸ“ ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
        report_path = args.report
        evaluator.generate_report(report_path)
        print(f"âœ… è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    
    # å¯è§†åŒ–æ¯”è¾ƒ
    if args.visualize_comparison:
        print("\nğŸ“ˆ å¯è§†åŒ–æ¨¡å‹æ¯”è¾ƒ...")
        metrics = args.metrics.split(',') if args.metrics else None
        evaluator.visualize_comparison(metrics=metrics, output_path=args.visualize_comparison)
        print(f"âœ… æ¯”è¾ƒå›¾è¡¨å·²ä¿å­˜åˆ°: {args.visualize_comparison}")
    
    # å¯è§†åŒ–æ¨¡å‹æ€§èƒ½
    if args.visualize_model:
        for model_name in args.visualize_model:
            print(f"\nğŸ“Š å¯è§†åŒ–æ¨¡å‹æ€§èƒ½: {model_name}...")
            output_path = f"output/{model_name}_performance.png"
            if args.output_dir:
                output_path = os.path.join(args.output_dir, f"{model_name}_performance.png")
            evaluator.visualize_model_performance(model_name, output_path=output_path)
            print(f"âœ… æ€§èƒ½å›¾è¡¨å·²ä¿å­˜åˆ°: {output_path}")
    
    # ä¿å­˜ç»“æœ
    if args.save_results:
        print("\nğŸ’¾ ä¿å­˜è¯„ä¼°ç»“æœ...")
        evaluator.save_results(args.save_results if args.save_results != "default" else args.output_dir)
        print(f"âœ… è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {args.save_results if args.save_results != 'default' else args.output_dir}")


def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºå‘½ä»¤è¡Œè§£æå™¨
    parser = argparse.ArgumentParser(description="æ¨¡å‹è¯„ä¼°å‘½ä»¤è¡Œå·¥å…·")
    subparsers = parser.add_subparsers(dest="command", help="å­å‘½ä»¤")
    
    # åŸºå‡†æµ‹è¯•å­å‘½ä»¤
    benchmark_parser = subparsers.add_parser("benchmark", help="è¿è¡Œæ¨¡å‹åŸºå‡†æµ‹è¯•")
    benchmark_parser.add_argument("--register-default", action="store_true", help="æ³¨å†Œé»˜è®¤é¢„æµ‹å™¨")
    benchmark_parser.add_argument("--custom-predictor", nargs="+", help="æ³¨å†Œè‡ªå®šä¹‰é¢„æµ‹å™¨ (æ ¼å¼: module_path:class_name:method_name:model_name:category)")
    benchmark_parser.add_argument("--predictor-config", help="é¢„æµ‹å™¨é…ç½®æ–‡ä»¶è·¯å¾„")
    benchmark_parser.add_argument("--evaluate-all", action="store_true", help="è¯„ä¼°æ‰€æœ‰æ³¨å†Œçš„æ¨¡å‹")
    benchmark_parser.add_argument("--evaluate", nargs="+", help="è¯„ä¼°æŒ‡å®šçš„æ¨¡å‹")
    benchmark_parser.add_argument("--test-periods", type=int, default=20, help="æµ‹è¯•æœŸæ•°")
    benchmark_parser.add_argument("--compare", action="store_true", help="æ¯”è¾ƒæ¨¡å‹æ€§èƒ½")
    benchmark_parser.add_argument("--categories", help="è¦æ¯”è¾ƒçš„æ¨¡å‹ç±»åˆ«ï¼Œé€—å·åˆ†éš”")
    benchmark_parser.add_argument("--metrics", help="è¦æ¯”è¾ƒçš„æŒ‡æ ‡ï¼Œé€—å·åˆ†éš”")
    benchmark_parser.add_argument("--report", help="ç”Ÿæˆè¯„ä¼°æŠ¥å‘Šçš„è¾“å‡ºè·¯å¾„")
    benchmark_parser.add_argument("--visualize-comparison", help="å¯è§†åŒ–æ¨¡å‹æ¯”è¾ƒçš„è¾“å‡ºè·¯å¾„")
    benchmark_parser.add_argument("--visualize-model", nargs="+", help="å¯è§†åŒ–æŒ‡å®šæ¨¡å‹çš„æ€§èƒ½")
    benchmark_parser.add_argument("--output-dir", default="output", help="è¾“å‡ºç›®å½•")
    benchmark_parser.add_argument("--save-results", help="ä¿å­˜è¯„ä¼°ç»“æœçš„è¾“å‡ºè·¯å¾„")
    benchmark_parser.add_argument("--load-results", help="åŠ è½½è¯„ä¼°ç»“æœçš„è¾“å…¥è·¯å¾„")
    
    # å‚æ•°ä¼˜åŒ–å­å‘½ä»¤
    optimize_parser = subparsers.add_parser("optimize", help="è¿è¡Œå‚æ•°ä¼˜åŒ–")
    optimize_parser.add_argument("--module-path", required=True, help="æ¨¡å‹æ¨¡å—è·¯å¾„")
    optimize_parser.add_argument("--class-name", required=True, help="æ¨¡å‹ç±»å")
    optimize_parser.add_argument("--param-space", required=True, help="å‚æ•°ç©ºé—´æ–‡ä»¶è·¯å¾„")
    optimize_parser.add_argument("--method", default="grid", choices=["grid", "random", "bayesian"], help="ä¼˜åŒ–æ–¹æ³•")
    optimize_parser.add_argument("--n-iter", type=int, default=50, help="éšæœºæœç´¢æˆ–è´å¶æ–¯ä¼˜åŒ–çš„è¿­ä»£æ¬¡æ•°")
    optimize_parser.add_argument("--train-periods", type=int, default=300, help="è®­ç»ƒæ•°æ®æœŸæ•°")
    optimize_parser.add_argument("--val-periods", type=int, default=50, help="éªŒè¯æ•°æ®æœŸæ•°")
    optimize_parser.add_argument("--test-periods", type=int, default=20, help="æµ‹è¯•æœŸæ•°")
    optimize_parser.add_argument("--metric", default="accuracy", choices=["accuracy", "hit_rate", "roi"], help="ä¼˜åŒ–æŒ‡æ ‡")
    optimize_parser.add_argument("--visualize-process", help="å¯è§†åŒ–ä¼˜åŒ–è¿‡ç¨‹çš„è¾“å‡ºè·¯å¾„")
    optimize_parser.add_argument("--visualize-importance", help="å¯è§†åŒ–å‚æ•°é‡è¦æ€§çš„è¾“å‡ºè·¯å¾„")
    optimize_parser.add_argument("--save-results", help="ä¿å­˜ä¼˜åŒ–ç»“æœçš„è¾“å‡ºè·¯å¾„")
    optimize_parser.add_argument("--benchmark", action="store_true", help="ä½¿ç”¨åŸºå‡†æµ‹è¯•æ¡†æ¶è¯„ä¼°æœ€ä½³æ¨¡å‹")
    optimize_parser.add_argument("--compare-baseline", action="store_true", help="æ¯”è¾ƒåŸºå‡†æ¨¡å‹å’Œä¼˜åŒ–æ¨¡å‹")
    
    # æ¨¡å‹è¯„ä¼°å­å‘½ä»¤
    evaluate_parser = subparsers.add_parser("evaluate", help="è¿è¡Œæ¨¡å‹è¯„ä¼°")
    evaluate_parser.add_argument("--register-default", action="store_true", help="æ³¨å†Œé»˜è®¤é¢„æµ‹å™¨")
    evaluate_parser.add_argument("--custom-predictor", nargs="+", help="æ³¨å†Œè‡ªå®šä¹‰é¢„æµ‹å™¨ (æ ¼å¼: module_path:class_name:method_name:model_name:category)")
    evaluate_parser.add_argument("--predictor-config", help="é¢„æµ‹å™¨é…ç½®æ–‡ä»¶è·¯å¾„")
    evaluate_parser.add_argument("--evaluate-all", action="store_true", help="è¯„ä¼°æ‰€æœ‰æ³¨å†Œçš„æ¨¡å‹")
    evaluate_parser.add_argument("--evaluate", nargs="+", help="è¯„ä¼°æŒ‡å®šçš„æ¨¡å‹")
    evaluate_parser.add_argument("--test-periods", type=int, default=20, help="æµ‹è¯•æœŸæ•°")
    evaluate_parser.add_argument("--compare", action="store_true", help="æ¯”è¾ƒæ¨¡å‹æ€§èƒ½")
    evaluate_parser.add_argument("--categories", help="è¦æ¯”è¾ƒçš„æ¨¡å‹ç±»åˆ«ï¼Œé€—å·åˆ†éš”")
    evaluate_parser.add_argument("--metrics", help="è¦æ¯”è¾ƒçš„æŒ‡æ ‡ï¼Œé€—å·åˆ†éš”")
    evaluate_parser.add_argument("--optimize", nargs="+", help="ä¼˜åŒ–æ¨¡å‹ (æ ¼å¼: module_path:class_name:param_space_file)")
    evaluate_parser.add_argument("--method", default="grid", choices=["grid", "random", "bayesian"], help="ä¼˜åŒ–æ–¹æ³•")
    evaluate_parser.add_argument("--n-iter", type=int, default=50, help="éšæœºæœç´¢æˆ–è´å¶æ–¯ä¼˜åŒ–çš„è¿­ä»£æ¬¡æ•°")
    evaluate_parser.add_argument("--train-periods", type=int, default=300, help="è®­ç»ƒæ•°æ®æœŸæ•°")
    evaluate_parser.add_argument("--val-periods", type=int, default=50, help="éªŒè¯æ•°æ®æœŸæ•°")
    evaluate_parser.add_argument("--metric", default="accuracy", choices=["accuracy", "hit_rate", "roi"], help="ä¼˜åŒ–æŒ‡æ ‡")
    evaluate_parser.add_argument("--report", help="ç”Ÿæˆè¯„ä¼°æŠ¥å‘Šçš„è¾“å‡ºè·¯å¾„")
    evaluate_parser.add_argument("--visualize-comparison", help="å¯è§†åŒ–æ¨¡å‹æ¯”è¾ƒçš„è¾“å‡ºè·¯å¾„")
    evaluate_parser.add_argument("--visualize-model", nargs="+", help="å¯è§†åŒ–æŒ‡å®šæ¨¡å‹çš„æ€§èƒ½")
    evaluate_parser.add_argument("--output-dir", default="output", help="è¾“å‡ºç›®å½•")
    evaluate_parser.add_argument("--save-results", default="default", help="ä¿å­˜è¯„ä¼°ç»“æœçš„è¾“å‡ºè·¯å¾„")
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    if hasattr(args, 'output_dir') and args.output_dir:
        os.makedirs(args.output_dir, exist_ok=True)
    
    # æ‰§è¡Œç›¸åº”çš„å‘½ä»¤
    if args.command == "benchmark":
        run_benchmark(args)
    elif args.command == "optimize":
        run_optimization(args)
    elif args.command == "evaluate":
        run_evaluation(args)
    else:
        parser.print_help()


if __name__ == "__main__":
    main()