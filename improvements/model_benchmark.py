#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ¨¡å‹åŸºå‡†æµ‹è¯•æ¡†æ¶
æä¾›å…¨é¢çš„æ¨¡å‹è¯„ä¼°ã€æ¯”è¾ƒå’Œå¯è§†åŒ–åŠŸèƒ½
"""

import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Tuple, Any, Callable, Union
from datetime import datetime
from collections import defaultdict
import json
import time
from tqdm import tqdm

# å°è¯•å¯¼å…¥æ ¸å¿ƒæ¨¡å—
try:
    from core_modules import logger_manager, data_manager, cache_manager
except ImportError:
    # å¦‚æœåœ¨ä¸åŒç›®å½•è¿è¡Œï¼Œæ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from core_modules import logger_manager, data_manager, cache_manager


class ModelBenchmark:
    """æ¨¡å‹åŸºå‡†æµ‹è¯•æ¡†æ¶"""
    
    def __init__(self):
        """åˆå§‹åŒ–åŸºå‡†æµ‹è¯•æ¡†æ¶"""
        self.df = data_manager.get_data()
        if self.df is None:
            logger_manager.error("æ•°æ®æœªåŠ è½½")
            
        # è¯„ä¼°æŒ‡æ ‡
        self.metrics = [
            'hit_rate',          # å‘½ä¸­ç‡ï¼ˆä»»æ„å·ç å‘½ä¸­ï¼‰
            'accuracy',          # å‡†ç¡®ç‡ï¼ˆåŠ æƒå¾—åˆ†ï¼‰
            'consistency',       # ä¸€è‡´æ€§ï¼ˆæ ‡å‡†å·®ï¼‰
            'roi',               # æŠ•èµ„å›æŠ¥ç‡
            'adaptability',      # é€‚åº”æ€§ï¼ˆæ€§èƒ½æ”¹è¿›è¶‹åŠ¿ï¼‰
            'time_efficiency'    # æ—¶é—´æ•ˆç‡ï¼ˆé¢„æµ‹é€Ÿåº¦ï¼‰
        ]
        
        # å¥–é‡‘è®¾ç½®
        self.prize_levels = {
            'ä¸€ç­‰å¥–': 10000000,  # 1000ä¸‡ï¼ˆç¤ºä¾‹å€¼ï¼‰
            'äºŒç­‰å¥–': 100000,    # 10ä¸‡ï¼ˆç¤ºä¾‹å€¼ï¼‰
            'ä¸‰ç­‰å¥–': 3000,      # 3000å…ƒ
            'å››ç­‰å¥–': 200,       # 200å…ƒ
            'äº”ç­‰å¥–': 10,        # 10å…ƒ
            'å…­ç­‰å¥–': 5,         # 5å…ƒ
            'æœªä¸­å¥–': 0
        }
        
        # è¯„ä¼°ç»“æœ
        self.results = {}
        
        # æ¯”è¾ƒç»“æœ
        self.comparison = None
    
    def register_model(self, model_name: str, predict_func: Callable, 
                      description: str = "", category: str = "traditional"):
        """æ³¨å†Œæ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            predict_func: é¢„æµ‹å‡½æ•°ï¼Œæ¥å—æœŸæ•°å‚æ•°ï¼Œè¿”å›é¢„æµ‹ç»“æœ
            description: æ¨¡å‹æè¿°
            category: æ¨¡å‹ç±»åˆ«
        """
        self.results[model_name] = {
            'name': model_name,
            'description': description,
            'category': category,
            'predict_func': predict_func,
            'evaluated': False
        }
        
        logger_manager.info(f"å·²æ³¨å†Œæ¨¡å‹: {model_name} ({category})")
    
    def evaluate_model(self, model_name: str, test_periods: int = 50, 
                      bet_cost: float = 2.0, verbose: bool = True) -> Dict:
        """è¯„ä¼°å•ä¸ªæ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            test_periods: æµ‹è¯•æœŸæ•°
            bet_cost: æ¯æ³¨æŠ•æ³¨æˆæœ¬
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        Returns:
            Dict: è¯„ä¼°ç»“æœ
        """
        if model_name not in self.results:
            logger_manager.error(f"æœªæ³¨å†Œçš„æ¨¡å‹: {model_name}")
            return {}
        
        if self.df is None or len(self.df) < test_periods:
            logger_manager.error(f"æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¯„ä¼°æ¨¡å‹ {model_name}")
            return {}
        
        if verbose:
            logger_manager.info(f"å¼€å§‹è¯„ä¼°æ¨¡å‹: {model_name}, æµ‹è¯•æœŸæ•°: {test_periods}")
        
        model_info = self.results[model_name]
        predict_func = model_info['predict_func']
        
        # åˆå§‹åŒ–ç»“æœ
        result = {
            'model_name': model_name,
            'description': model_info['description'],
            'category': model_info['category'],
            'test_periods': test_periods,
            'metrics': {},
            'detailed_results': [],
            'timestamp': datetime.now().isoformat()
        }
        
        # ç»Ÿè®¡å˜é‡
        total_hits = 0
        hit_counts = defaultdict(int)  # æŒ‰å‘½ä¸­æ•°é‡ç»Ÿè®¡
        prize_levels = defaultdict(int)  # æŒ‰å¥–çº§ç»Ÿè®¡
        total_cost = 0
        total_prize = 0
        accuracy_scores = []
        prediction_times = []
        
        # é€æœŸæµ‹è¯•
        for i in tqdm(range(test_periods), desc=f"è¯„ä¼° {model_name}", disable=not verbose):
            try:
                # è·å–æµ‹è¯•æœŸçš„å®é™…ç»“æœ
                test_row = self.df.iloc[i]
                actual_front, actual_back = data_manager.parse_balls(test_row)
                issue = test_row.get('issue', str(i))
                
                # ä½¿ç”¨å†å²æ•°æ®è¿›è¡Œé¢„æµ‹
                historical_data = self.df.iloc[i+1:i+501]  # ä½¿ç”¨åç»­500æœŸä½œä¸ºå†å²æ•°æ®
                
                # è®°å½•é¢„æµ‹æ—¶é—´
                start_time = time.time()
                predictions = predict_func(historical_data)
                end_time = time.time()
                prediction_time = end_time - start_time
                prediction_times.append(prediction_time)
                
                # å¦‚æœæ²¡æœ‰é¢„æµ‹ç»“æœï¼Œè·³è¿‡
                if not predictions:
                    continue
                
                # è¯„ä¼°æ¯ä¸€æ³¨é¢„æµ‹
                period_results = []
                period_cost = 0
                period_prize = 0
                
                for pred_idx, pred in enumerate(predictions):
                    # è§£æé¢„æµ‹ç»“æœ
                    if isinstance(pred, tuple) and len(pred) == 2:
                        pred_front, pred_back = pred
                    elif isinstance(pred, dict) and 'front_balls' in pred and 'back_balls' in pred:
                        pred_front = pred['front_balls']
                        pred_back = pred['back_balls']
                    else:
                        logger_manager.warning(f"æ— æ³•è§£æé¢„æµ‹ç»“æœ: {pred}")
                        continue
                    
                    # è®¡ç®—å‘½ä¸­æƒ…å†µ
                    front_hits = len(set(pred_front) & set(actual_front))
                    back_hits = len(set(pred_back) & set(actual_back))
                    
                    # åˆ¤æ–­ä¸­å¥–ç­‰çº§
                    prize_level, prize_amount = self._calculate_prize_level(front_hits, back_hits)
                    
                    # æ›´æ–°ç»Ÿè®¡
                    hit_key = f"{front_hits}+{back_hits}"
                    hit_counts[hit_key] += 1
                    prize_levels[prize_level] += 1
                    
                    if front_hits > 0 or back_hits > 0:
                        total_hits += 1
                    
                    # è®¡ç®—æˆæœ¬å’Œå¥–é‡‘
                    period_cost += bet_cost
                    period_prize += prize_amount
                    
                    # è®¡ç®—å‡†ç¡®ç‡å¾—åˆ†ï¼ˆåŠ æƒå¾—åˆ†ï¼‰
                    accuracy_score = self._calculate_accuracy_score(front_hits, back_hits)
                    accuracy_scores.append(accuracy_score)
                    
                    # è®°å½•è¯¦ç»†ç»“æœ
                    period_results.append({
                        'prediction_index': pred_idx + 1,
                        'predicted_front': pred_front,
                        'predicted_back': pred_back,
                        'actual_front': actual_front,
                        'actual_back': actual_back,
                        'front_hits': front_hits,
                        'back_hits': back_hits,
                        'prize_level': prize_level,
                        'prize_amount': prize_amount,
                        'accuracy_score': accuracy_score
                    })
                
                # æ›´æ–°æ€»æˆæœ¬å’Œå¥–é‡‘
                total_cost += period_cost
                total_prize += period_prize
                
                # è®°å½•æœ¬æœŸç»“æœ
                result['detailed_results'].append({
                    'issue': issue,
                    'period_results': period_results,
                    'period_cost': period_cost,
                    'period_prize': period_prize,
                    'period_profit': period_prize - period_cost,
                    'prediction_time': prediction_time
                })
                
            except Exception as e:
                logger_manager.error(f"è¯„ä¼°æœŸ {i} å¤±è´¥: {e}")
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        total_predictions = sum(hit_counts.values())
        
        if total_predictions > 0:
            # å‘½ä¸­ç‡
            result['metrics']['hit_rate'] = total_hits / total_predictions
            
            # å‡†ç¡®ç‡ï¼ˆåŠ æƒå¾—åˆ†ï¼‰
            result['metrics']['accuracy'] = np.mean(accuracy_scores) if accuracy_scores else 0
            
            # ä¸€è‡´æ€§ï¼ˆæ ‡å‡†å·®ï¼‰
            result['metrics']['consistency'] = 1.0 / (1.0 + np.std(accuracy_scores)) if len(accuracy_scores) > 1 else 0
            
            # æŠ•èµ„å›æŠ¥ç‡
            result['metrics']['roi'] = (total_prize - total_cost) / total_cost if total_cost > 0 else 0
            
            # é€‚åº”æ€§ï¼ˆååŠæ®µvså‰åŠæ®µï¼‰
            if len(accuracy_scores) >= 10:
                mid_point = len(accuracy_scores) // 2
                first_half = np.mean(accuracy_scores[:mid_point])
                second_half = np.mean(accuracy_scores[mid_point:])
                result['metrics']['adaptability'] = second_half - first_half
            else:
                result['metrics']['adaptability'] = 0
            
            # æ—¶é—´æ•ˆç‡ï¼ˆé¢„æµ‹é€Ÿåº¦ï¼‰
            avg_time = np.mean(prediction_times) if prediction_times else 0
            result['metrics']['time_efficiency'] = 1.0 / (1.0 + avg_time)  # è½¬æ¢ä¸º0-1èŒƒå›´ï¼Œè¶Šå¿«è¶Šé«˜
        
        # è®°å½•å‘½ä¸­ç»Ÿè®¡
        result['hit_statistics'] = dict(hit_counts)
        result['prize_statistics'] = dict(prize_levels)
        result['total_cost'] = total_cost
        result['total_prize'] = total_prize
        result['net_profit'] = total_prize - total_cost
        result['avg_prediction_time'] = np.mean(prediction_times) if prediction_times else 0
        
        # æ›´æ–°æ¨¡å‹è¯„ä¼°çŠ¶æ€
        self.results[model_name].update(result)
        self.results[model_name]['evaluated'] = True
        
        if verbose:
            logger_manager.info(f"æ¨¡å‹ {model_name} è¯„ä¼°å®Œæˆ")
            
            # æ‰“å°å…³é”®æŒ‡æ ‡
            print(f"\nğŸ“Š {model_name} è¯„ä¼°ç»“æœ:")
            print(f"  å‘½ä¸­ç‡: {result['metrics'].get('hit_rate', 0):.4f}")
            print(f"  å‡†ç¡®ç‡: {result['metrics'].get('accuracy', 0):.4f}")
            print(f"  ROI: {result['metrics'].get('roi', 0):.4f}")
            print(f"  æ€»æŠ•æ³¨: {total_cost:.2f} å…ƒ")
            print(f"  æ€»å¥–é‡‘: {total_prize:.2f} å…ƒ")
            print(f"  å‡€åˆ©æ¶¦: {total_prize - total_cost:.2f} å…ƒ")
            print(f"  å¹³å‡é¢„æµ‹æ—¶é—´: {np.mean(prediction_times) if prediction_times else 0:.4f} ç§’")
        
        return result
    
    def evaluate_all_models(self, test_periods: int = 50, bet_cost: float = 2.0, verbose: bool = True):
        """è¯„ä¼°æ‰€æœ‰æ³¨å†Œçš„æ¨¡å‹
        
        Args:
            test_periods: æµ‹è¯•æœŸæ•°
            bet_cost: æ¯æ³¨æŠ•æ³¨æˆæœ¬
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        """
        if verbose:
            logger_manager.info(f"å¼€å§‹è¯„ä¼°æ‰€æœ‰æ¨¡å‹ï¼Œæµ‹è¯•æœŸæ•°: {test_periods}")
        
        for model_name in self.results.keys():
            self.evaluate_model(model_name, test_periods, bet_cost, verbose)
        
        if verbose:
            logger_manager.info("æ‰€æœ‰æ¨¡å‹è¯„ä¼°å®Œæˆ")
    
    def compare_models(self, metrics: List[str] = None, categories: List[str] = None, 
                      verbose: bool = True) -> Dict:
        """æ¯”è¾ƒæ¨¡å‹æ€§èƒ½
        
        Args:
            metrics: è¦æ¯”è¾ƒçš„æŒ‡æ ‡åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºæ‰€æœ‰æŒ‡æ ‡
            categories: è¦æ¯”è¾ƒçš„æ¨¡å‹ç±»åˆ«ï¼Œé»˜è®¤ä¸ºæ‰€æœ‰ç±»åˆ«
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        Returns:
            Dict: æ¯”è¾ƒç»“æœ
        """
        # ä½¿ç”¨é»˜è®¤æŒ‡æ ‡æˆ–æŒ‡å®šæŒ‡æ ‡
        metrics = metrics or self.metrics
        
        # ç­›é€‰å·²è¯„ä¼°çš„æ¨¡å‹
        evaluated_models = {name: info for name, info in self.results.items() 
                           if info.get('evaluated', False)}
        
        # ç­›é€‰æŒ‡å®šç±»åˆ«çš„æ¨¡å‹
        if categories:
            evaluated_models = {name: info for name, info in evaluated_models.items() 
                              if info.get('category') in categories}
        
        if not evaluated_models:
            logger_manager.warning("æ²¡æœ‰å·²è¯„ä¼°çš„æ¨¡å‹å¯ä¾›æ¯”è¾ƒ")
            return {}
        
        comparison_result = {
            'models': list(evaluated_models.keys()),
            'metrics': {},
            'rankings': {},
            'overall_ranking': {},
            'categories': {},
            'timestamp': datetime.now().isoformat()
        }
        
        # æŒ‰ç±»åˆ«åˆ†ç»„
        for model_name, model_info in evaluated_models.items():
            category = model_info.get('category', 'unknown')
            if category not in comparison_result['categories']:
                comparison_result['categories'][category] = []
            comparison_result['categories'][category].append(model_name)
        
        # æ¯”è¾ƒå„æŒ‡æ ‡
        for metric in metrics:
            comparison_result['metrics'][metric] = {}
            metric_values = {}
            
            for model_name, model_info in evaluated_models.items():
                if 'metrics' in model_info and metric in model_info['metrics']:
                    metric_values[model_name] = model_info['metrics'][metric]
            
            # æ’åº
            sorted_models = sorted(metric_values.items(), key=lambda x: x[1], reverse=True)
            
            # è®°å½•æ’å
            for rank, (model_name, value) in enumerate(sorted_models, 1):
                comparison_result['metrics'][metric][model_name] = {
                    'value': value,
                    'rank': rank
                }
        
        # è®¡ç®—æ€»ä½“æ’å
        model_ranks = defaultdict(int)
        for metric in metrics:
            if metric in comparison_result['metrics']:
                for model_name, info in comparison_result['metrics'][metric].items():
                    model_ranks[model_name] += info['rank']
        
        # æ’åºï¼ˆæ€»æ’åæœ€ä½çš„æœ€å¥½ï¼‰
        sorted_models = sorted(model_ranks.items(), key=lambda x: x[1])
        
        # è®°å½•æ€»ä½“æ’å
        for rank, (model_name, total_rank) in enumerate(sorted_models, 1):
            comparison_result['overall_ranking'][model_name] = {
                'rank': rank,
                'total_rank_score': total_rank,
                'category': evaluated_models[model_name].get('category', 'unknown')
            }
        
        # è®°å½•å„æŒ‡æ ‡çš„æ’å
        for metric in metrics:
            if metric in comparison_result['metrics']:
                comparison_result['rankings'][metric] = [
                    model_name for model_name, _ in sorted(
                        comparison_result['metrics'][metric].items(),
                        key=lambda x: x[1]['rank']
                    )
                ]
        
        # ä¿å­˜æ¯”è¾ƒç»“æœ
        self.comparison = comparison_result
        
        if verbose:
            # æ‰“å°æ€»ä½“æ’å
            print("\nğŸ† æ¨¡å‹æ€»ä½“æ’å:")
            for model_name, info in comparison_result['overall_ranking'].items():
                print(f"  {info['rank']}. {model_name} (ç±»åˆ«: {info['category']})")
            
            # æ‰“å°å„æŒ‡æ ‡æœ€ä½³æ¨¡å‹
            print("\nğŸ¥‡ å„æŒ‡æ ‡æœ€ä½³æ¨¡å‹:")
            for metric in metrics:
                if metric in comparison_result['rankings'] and comparison_result['rankings'][metric]:
                    best_model = comparison_result['rankings'][metric][0]
                    best_value = comparison_result['metrics'][metric][best_model]['value']
                    print(f"  {metric}: {best_model} ({best_value:.4f})")
        
        return comparison_result
    
    def generate_report(self, output_path=None) -> str:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        
        Args:
            output_path: æŠ¥å‘Šè¾“å‡ºè·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è¿”å›æŠ¥å‘Šå†…å®¹
            
        Returns:
            str: æŠ¥å‘Šå†…å®¹
        """
        # ç­›é€‰å·²è¯„ä¼°çš„æ¨¡å‹
        evaluated_models = {name: info for name, info in self.results.items() 
                           if info.get('evaluated', False)}
        
        if not evaluated_models:
            return "æ²¡æœ‰è¯„ä¼°ç»“æœå¯ä¾›ç”ŸæˆæŠ¥å‘Š"
        
        report = ["# æ¨¡å‹è¯„ä¼°æŠ¥å‘Š"]
        report.append(f"\n## ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # æ¨¡å‹æ¦‚è§ˆ
        report.append("\n## æ¨¡å‹æ¦‚è§ˆ")
        report.append("\n| æ¨¡å‹åç§° | ç±»åˆ« | å‘½ä¸­ç‡ | å‡†ç¡®ç‡ | ROI | å¹³å‡é¢„æµ‹æ—¶é—´(ç§’) |")
        report.append("| --- | --- | --- | --- | --- | --- |")
        
        for model_name, info in evaluated_models.items():
            metrics = info.get('metrics', {})
            hit_rate = metrics.get('hit_rate', 0)
            accuracy = metrics.get('accuracy', 0)
            roi = metrics.get('roi', 0)
            avg_time = info.get('avg_prediction_time', 0)
            category = info.get('category', 'unknown')
            
            report.append(f"| {model_name} | {category} | {hit_rate:.4f} | {accuracy:.4f} | {roi:.4f} | {avg_time:.4f} |")
        
        # æ€»ä½“æ’å
        if self.comparison and 'overall_ranking' in self.comparison:
            report.append("\n## æ€»ä½“æ’å")
            report.append("\n| æ’å | æ¨¡å‹åç§° | ç±»åˆ« | æ€»åˆ† |")
            report.append("| --- | --- | --- | --- |")
            
            for model_name, info in self.comparison['overall_ranking'].items():
                rank = info['rank']
                category = info['category']
                score = info['total_rank_score']
                report.append(f"| {rank} | {model_name} | {category} | {score} |")
        
        # å„æŒ‡æ ‡æ’å
        if self.comparison and 'rankings' in self.comparison:
            report.append("\n## å„æŒ‡æ ‡æ’å")
            
            for metric, models in self.comparison['rankings'].items():
                report.append(f"\n### {metric}")
                report.append("\n| æ’å | æ¨¡å‹åç§° | å¾—åˆ† |")
                report.append("| --- | --- | --- |")
                
                for i, model_name in enumerate(models, 1):
                    value = self.comparison['metrics'][metric][model_name]['value']
                    report.append(f"| {i} | {model_name} | {value:.4f} |")
        
        # è¯¦ç»†è¯„ä¼°ç»“æœ
        report.append("\n## è¯¦ç»†è¯„ä¼°ç»“æœ")
        
        for model_name, info in evaluated_models.items():
            report.append(f"\n### {model_name}")
            
            # åŸºæœ¬ä¿¡æ¯
            report.append(f"\n#### åŸºæœ¬ä¿¡æ¯")
            report.append(f"- ç±»åˆ«: {info.get('category', 'unknown')}")
            report.append(f"- æè¿°: {info.get('description', '')}")
            report.append(f"- æµ‹è¯•æœŸæ•°: {info.get('test_periods', 0)}")
            report.append(f"- æ€»æŠ•æ³¨æˆæœ¬: {info.get('total_cost', 0):.2f}")
            report.append(f"- æ€»å¥–é‡‘: {info.get('total_prize', 0):.2f}")
            report.append(f"- å‡€åˆ©æ¶¦: {info.get('net_profit', 0):.2f}")
            report.append(f"- å¹³å‡é¢„æµ‹æ—¶é—´: {info.get('avg_prediction_time', 0):.4f} ç§’")
            
            # è¯„ä¼°æŒ‡æ ‡
            report.append(f"\n#### è¯„ä¼°æŒ‡æ ‡")
            metrics = info.get('metrics', {})
            for metric, value in metrics.items():
                report.append(f"- {metric}: {value:.4f}")
            
            # å‘½ä¸­ç»Ÿè®¡
            report.append(f"\n#### å‘½ä¸­ç»Ÿè®¡")
            hit_stats = info.get('hit_statistics', {})
            for hit_key, count in sorted(hit_stats.items()):
                report.append(f"- {hit_key}: {count}æ¬¡")
            
            # å¥–çº§ç»Ÿè®¡
            report.append(f"\n#### å¥–çº§ç»Ÿè®¡")
            prize_stats = info.get('prize_statistics', {})
            for level, count in sorted(prize_stats.items()):
                report.append(f"- {level}: {count}æ¬¡")
        
        # ä¿å­˜æŠ¥å‘Š
        report_content = "\n".join(report)
        
        if output_path:
            try:
                os.makedirs(os.path.dirname(output_path), exist_ok=True)
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(report_content)
                logger_manager.info(f"è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ° {output_path}")
            except Exception as e:
                logger_manager.error(f"ä¿å­˜è¯„ä¼°æŠ¥å‘Šå¤±è´¥: {e}")
        
        return report_content
    
    def visualize_comparison(self, metrics=None, output_path=None, figsize=(12, 10)):
        """å¯è§†åŒ–æ¨¡å‹æ¯”è¾ƒç»“æœ
        
        Args:
            metrics: è¦å¯è§†åŒ–çš„æŒ‡æ ‡åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºæ‰€æœ‰æŒ‡æ ‡
            output_path: å›¾è¡¨è¾“å‡ºè·¯å¾„
            figsize: å›¾è¡¨å¤§å°
        """
        if not self.comparison:
            logger_manager.warning("æ²¡æœ‰æ¯”è¾ƒç»“æœå¯ä¾›å¯è§†åŒ–")
            return
        
        # ä½¿ç”¨é»˜è®¤æŒ‡æ ‡æˆ–æŒ‡å®šæŒ‡æ ‡
        metrics = metrics or self.metrics
        
        # ç­›é€‰æœ‰æ•ˆæŒ‡æ ‡
        valid_metrics = [m for m in metrics if m in self.comparison.get('metrics', {})]
        
        if not valid_metrics:
            logger_manager.warning("æ²¡æœ‰æœ‰æ•ˆæŒ‡æ ‡å¯ä¾›å¯è§†åŒ–")
            return
        
        try:
            # åˆ›å»ºå›¾è¡¨
            fig, axes = plt.subplots(len(valid_metrics), 1, figsize=figsize)
            if len(valid_metrics) == 1:
                axes = [axes]
            
            # è®¾ç½®é¢œè‰²æ˜ å°„
            categories = self.comparison.get('categories', {})
            category_colors = {}
            cmap = plt.cm.tab10
            for i, category in enumerate(categories.keys()):
                category_colors[category] = cmap(i % 10)
            
            # ç»˜åˆ¶å„æŒ‡æ ‡å¯¹æ¯”å›¾
            for i, metric in enumerate(valid_metrics):
                ax = axes[i]
                
                # å‡†å¤‡æ•°æ®
                models = []
                values = []
                colors = []
                
                metric_data = self.comparison['metrics'][metric]
                for model_name, data in sorted(metric_data.items(), key=lambda x: x[1]['value'], reverse=True):
                    models.append(model_name)
                    values.append(data['value'])
                    
                    # è·å–æ¨¡å‹ç±»åˆ«
                    category = 'unknown'
                    for cat, model_list in categories.items():
                        if model_name in model_list:
                            category = cat
                            break
                    
                    colors.append(category_colors.get(category, 'gray'))
                
                # ç»˜åˆ¶æ¡å½¢å›¾
                bars = ax.bar(models, values, color=colors, alpha=0.7)
                
                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                for bar in bars:
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                           f'{height:.4f}', ha='center', va='bottom', fontsize=8)
                
                # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
                ax.set_title(f'{metric.capitalize()} æ¯”è¾ƒ')
                ax.set_ylabel(metric)
                ax.set_ylim(bottom=0)
                
                # æ—‹è½¬xè½´æ ‡ç­¾
                plt.setp(ax.get_xticklabels(), rotation=45, ha='right')
            
            # æ·»åŠ å›¾ä¾‹
            handles = [plt.Rectangle((0,0),1,1, color=color) for color in category_colors.values()]
            labels = list(category_colors.keys())
            fig.legend(handles, labels, loc='upper right', title='æ¨¡å‹ç±»åˆ«')
            
            plt.tight_layout()
            
            # ä¿å­˜æˆ–æ˜¾ç¤ºå›¾è¡¨
            if output_path:
                os.makedirs(os.path.dirname(output_path), exist_ok=True)
                plt.savefig(output_path, dpi=300, bbox_inches='tight')
                logger_manager.info(f"æ¯”è¾ƒå›¾è¡¨å·²ä¿å­˜åˆ° {output_path}")
            else:
                plt.show()
                
        except Exception as e:
            logger_manager.error(f"å¯è§†åŒ–æ¯”è¾ƒå¤±è´¥: {e}")
    
    def visualize_model_performance(self, model_name, output_path=None):
        """å¯è§†åŒ–å•ä¸ªæ¨¡å‹çš„æ€§èƒ½
        
        Args:
            model_name: æ¨¡å‹åç§°
            output_path: å›¾è¡¨è¾“å‡ºè·¯å¾„
        """
        if model_name not in self.results or not self.results[model_name].get('evaluated', False):
            logger_manager.warning(f"æ¨¡å‹ {model_name} æœªè¯„ä¼°ï¼Œæ— æ³•å¯è§†åŒ–")
            return
        
        model_info = self.results[model_name]
        detailed_results = model_info.get('detailed_results', [])
        
        if not detailed_results:
            logger_manager.warning(f"æ¨¡å‹ {model_name} æ²¡æœ‰è¯¦ç»†ç»“æœï¼Œæ— æ³•å¯è§†åŒ–")
            return
        
        try:
            # åˆ›å»ºå›¾è¡¨
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            
            # 1. å‡†ç¡®ç‡éšæ—¶é—´å˜åŒ–
            accuracy_scores = []
            issues = []
            
            for period in detailed_results:
                for pred in period['period_results']:
                    accuracy_scores.append(pred['accuracy_score'])
                    issues.append(period['issue'])
            
            axes[0, 0].plot(accuracy_scores, marker='o', linestyle='-', alpha=0.7)
            axes[0, 0].set_title(f'{model_name} - å‡†ç¡®ç‡éšæ—¶é—´å˜åŒ–')
            axes[0, 0].set_xlabel('é¢„æµ‹åºå·')
            axes[0, 0].set_ylabel('å‡†ç¡®ç‡å¾—åˆ†')
            axes[0, 0].grid(True, alpha=0.3)
            
            # æ·»åŠ ç§»åŠ¨å¹³å‡çº¿
            window = min(10, len(accuracy_scores))
            if window > 1:
                moving_avg = np.convolve(accuracy_scores, np.ones(window)/window, mode='valid')
                axes[0, 0].plot(range(window-1, len(accuracy_scores)), moving_avg, 'r-', 
                               linewidth=2, alpha=0.8, label=f'{window}æœŸç§»åŠ¨å¹³å‡')
                axes[0, 0].legend()
            
            # 2. å‘½ä¸­ç»Ÿè®¡
            hit_stats = model_info.get('hit_statistics', {})
            hit_keys = []
            hit_counts = []
            
            for key, count in sorted(hit_stats.items()):
                hit_keys.append(key)
                hit_counts.append(count)
            
            axes[0, 1].bar(hit_keys, hit_counts, color='skyblue', alpha=0.7)
            axes[0, 1].set_title(f'{model_name} - å‘½ä¸­ç»Ÿè®¡')
            axes[0, 1].set_xlabel('å‘½ä¸­æƒ…å†µ (å‰åŒº+ååŒº)')
            axes[0, 1].set_ylabel('æ¬¡æ•°')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, v in enumerate(hit_counts):
                axes[0, 1].text(i, v + 0.1, str(v), ha='center')
            
            # 3. å¥–çº§ç»Ÿè®¡
            prize_stats = model_info.get('prize_statistics', {})
            prize_levels = []
            prize_counts = []
            
            # æŒ‰å¥–çº§æ’åº
            level_order = ['ä¸€ç­‰å¥–', 'äºŒç­‰å¥–', 'ä¸‰ç­‰å¥–', 'å››ç­‰å¥–', 'äº”ç­‰å¥–', 'å…­ç­‰å¥–', 'æœªä¸­å¥–']
            for level in level_order:
                if level in prize_stats:
                    prize_levels.append(level)
                    prize_counts.append(prize_stats[level])
            
            axes[1, 0].bar(prize_levels, prize_counts, color='lightgreen', alpha=0.7)
            axes[1, 0].set_title(f'{model_name} - å¥–çº§ç»Ÿè®¡')
            axes[1, 0].set_xlabel('å¥–çº§')
            axes[1, 0].set_ylabel('æ¬¡æ•°')
            plt.setp(axes[1, 0].get_xticklabels(), rotation=45, ha='right')
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, v in enumerate(prize_counts):
                axes[1, 0].text(i, v + 0.1, str(v), ha='center')
            
            # 4. ç´¯è®¡æ”¶ç›Šæ›²çº¿
            cumulative_cost = 0
            cumulative_prize = 0
            cumulative_profit = []
            
            for period in detailed_results:
                cumulative_cost += period['period_cost']
                cumulative_prize += period['period_prize']
                cumulative_profit.append(cumulative_prize - cumulative_cost)
            
            axes[1, 1].plot(cumulative_profit, marker='o', linestyle='-', color='orange', alpha=0.7)
            axes[1, 1].axhline(y=0, color='r', linestyle='--', alpha=0.5)
            axes[1, 1].set_title(f'{model_name} - ç´¯è®¡æ”¶ç›Šæ›²çº¿')
            axes[1, 1].set_xlabel('æœŸæ•°')
            axes[1, 1].set_ylabel('ç´¯è®¡æ”¶ç›Š (å…ƒ)')
            axes[1, 1].grid(True, alpha=0.3)
            
            # æ·»åŠ æœ€ç»ˆæ”¶ç›Šæ ‡æ³¨
            final_profit = cumulative_profit[-1] if cumulative_profit else 0
            axes[1, 1].text(len(cumulative_profit) - 1, final_profit, 
                           f'{final_profit:.2f}å…ƒ', ha='right', va='bottom')
            
            plt.tight_layout()
            
            # ä¿å­˜æˆ–æ˜¾ç¤ºå›¾è¡¨
            if output_path:
                os.makedirs(os.path.dirname(output_path), exist_ok=True)
                plt.savefig(output_path, dpi=300, bbox_inches='tight')
                logger_manager.info(f"æ¨¡å‹æ€§èƒ½å›¾è¡¨å·²ä¿å­˜åˆ° {output_path}")
            else:
                plt.show()
                
        except Exception as e:
            logger_manager.error(f"å¯è§†åŒ–æ¨¡å‹æ€§èƒ½å¤±è´¥: {e}")
    
    def save_results(self, output_path):
        """ä¿å­˜è¯„ä¼°ç»“æœ
        
        Args:
            output_path: ç»“æœè¾“å‡ºè·¯å¾„
        """
        try:
            # å‡†å¤‡ç»“æœæ•°æ®
            save_data = {
                'results': {},
                'comparison': self.comparison,
                'timestamp': datetime.now().isoformat()
            }
            
            # ç§»é™¤ä¸å¯åºåˆ—åŒ–çš„é¢„æµ‹å‡½æ•°
            for model_name, info in self.results.items():
                if info.get('evaluated', False):
                    model_data = info.copy()
                    if 'predict_func' in model_data:
                        del model_data['predict_func']
                    save_data['results'][model_name] = model_data
            
            # ä¿å­˜ä¸ºJSON
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            logger_manager.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ° {output_path}")
            
        except Exception as e:
            logger_manager.error(f"ä¿å­˜è¯„ä¼°ç»“æœå¤±è´¥: {e}")
    
    def load_results(self, input_path):
        """åŠ è½½è¯„ä¼°ç»“æœ
        
        Args:
            input_path: ç»“æœè¾“å…¥è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(input_path):
                logger_manager.error(f"ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
                return False
            
            # åŠ è½½JSON
            with open(input_path, 'r', encoding='utf-8') as f:
                load_data = json.load(f)
            
            # æ›´æ–°æ¯”è¾ƒç»“æœ
            if 'comparison' in load_data:
                self.comparison = load_data['comparison']
            
            # æ›´æ–°æ¨¡å‹ç»“æœï¼ˆä¿ç•™é¢„æµ‹å‡½æ•°ï¼‰
            if 'results' in load_data:
                for model_name, info in load_data['results'].items():
                    if model_name in self.results:
                        # ä¿ç•™é¢„æµ‹å‡½æ•°
                        predict_func = self.results[model_name].get('predict_func')
                        self.results[model_name] = info
                        self.results[model_name]['predict_func'] = predict_func
                    else:
                        # æ–°æ¨¡å‹ï¼Œæ²¡æœ‰é¢„æµ‹å‡½æ•°
                        info['predict_func'] = None
                        self.results[model_name] = info
            
            logger_manager.info(f"è¯„ä¼°ç»“æœå·²ä» {input_path} åŠ è½½")
            return True
            
        except Exception as e:
            logger_manager.error(f"åŠ è½½è¯„ä¼°ç»“æœå¤±è´¥: {e}")
            return False
    
    def _calculate_prize_level(self, front_hits: int, back_hits: int) -> Tuple[str, float]:
        """è®¡ç®—ä¸­å¥–ç­‰çº§å’Œå¥–é‡‘
        
        Args:
            front_hits: å‰åŒºå‘½ä¸­æ•°
            back_hits: ååŒºå‘½ä¸­æ•°
            
        Returns:
            Tuple[str, float]: å¥–çº§å’Œå¥–é‡‘
        """
        if front_hits == 5 and back_hits == 2:
            return "ä¸€ç­‰å¥–", self.prize_levels["ä¸€ç­‰å¥–"]
        elif front_hits == 5 and back_hits == 1:
            return "äºŒç­‰å¥–", self.prize_levels["äºŒç­‰å¥–"]
        elif front_hits == 5 and back_hits == 0:
            return "ä¸‰ç­‰å¥–", self.prize_levels["ä¸‰ç­‰å¥–"]
        elif front_hits == 4 and back_hits == 2:
            return "å››ç­‰å¥–", self.prize_levels["å››ç­‰å¥–"]
        elif front_hits == 4 and back_hits == 1:
            return "äº”ç­‰å¥–", self.prize_levels["äº”ç­‰å¥–"]
        elif front_hits == 3 and back_hits == 2:
            return "äº”ç­‰å¥–", self.prize_levels["äº”ç­‰å¥–"]
        elif front_hits == 4 and back_hits == 0:
            return "å…­ç­‰å¥–", self.prize_levels["å…­ç­‰å¥–"]
        elif front_hits == 3 and back_hits == 1:
            return "å…­ç­‰å¥–", self.prize_levels["å…­ç­‰å¥–"]
        elif front_hits == 2 and back_hits == 2:
            return "å…­ç­‰å¥–", self.prize_levels["å…­ç­‰å¥–"]
        elif front_hits == 0 and back_hits == 2:
            return "å…­ç­‰å¥–", self.prize_levels["å…­ç­‰å¥–"]
        else:
            return "æœªä¸­å¥–", self.prize_levels["æœªä¸­å¥–"]
    
    def _calculate_accuracy_score(self, front_hits: int, back_hits: int) -> float:
        """è®¡ç®—å‡†ç¡®ç‡å¾—åˆ†ï¼ˆåŠ æƒå¾—åˆ†ï¼‰
        
        Args:
            front_hits: å‰åŒºå‘½ä¸­æ•°
            back_hits: ååŒºå‘½ä¸­æ•°
            
        Returns:
            float: å‡†ç¡®ç‡å¾—åˆ†
        """
        # å‰åŒºæƒé‡
        front_weight = 0.7
        # ååŒºæƒé‡
        back_weight = 0.3
        
        # å‰åŒºå¾—åˆ†ï¼ˆæ»¡åˆ†ä¸º5ï¼‰
        front_score = front_hits / 5
        # ååŒºå¾—åˆ†ï¼ˆæ»¡åˆ†ä¸º2ï¼‰
        back_score = back_hits / 2
        
        # åŠ æƒæ€»åˆ†
        return front_weight * front_score + back_weight * back_score


# å…¨å±€å®ä¾‹
_model_benchmark = None

def get_model_benchmark() -> ModelBenchmark:
    """è·å–æ¨¡å‹åŸºå‡†æµ‹è¯•å®ä¾‹"""
    global _model_benchmark
    if _model_benchmark is None:
        _model_benchmark = ModelBenchmark()
    return _model_benchmark


if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å‹åŸºå‡†æµ‹è¯•æ¡†æ¶
    print("ğŸ” æµ‹è¯•æ¨¡å‹åŸºå‡†æµ‹è¯•æ¡†æ¶...")
    benchmark = get_model_benchmark()
    
    # æ¨¡æ‹Ÿé¢„æµ‹å‡½æ•°
    def mock_random_predict(historical_data):
        """éšæœºé¢„æµ‹"""
        return [(sorted(np.random.choice(range(1, 36), 5, replace=False)), 
                sorted(np.random.choice(range(1, 13), 2, replace=False))) for _ in range(3)]
    
    def mock_frequency_predict(historical_data):
        """é¢‘ç‡é¢„æµ‹"""
        # ç®€åŒ–ç‰ˆé¢‘ç‡é¢„æµ‹
        from collections import Counter
        
        front_counter = Counter()
        back_counter = Counter()
        
        for _, row in historical_data.iterrows():
            front_balls, back_balls = data_manager.parse_balls(row)
            front_counter.update(front_balls)
            back_counter.update(back_balls)
        
        front_most_common = [ball for ball, _ in front_counter.most_common(5)]
        back_most_common = [ball for ball, _ in back_counter.most_common(2)]
        
        return [(sorted(front_most_common), sorted(back_most_common)) for _ in range(3)]
    
    def mock_markov_predict(historical_data):
        """é©¬å°”å¯å¤«é¢„æµ‹"""
        # ç®€åŒ–ç‰ˆé©¬å°”å¯å¤«é¢„æµ‹
        return [(sorted(np.random.choice(range(1, 36), 5, replace=False)), 
                sorted(np.random.choice(range(1, 13), 2, replace=False))) for _ in range(3)]
    
    # æ³¨å†Œæ¨¡å‹
    print("\nğŸ“ æ³¨å†Œæµ‹è¯•æ¨¡å‹...")
    benchmark.register_model("éšæœºé¢„æµ‹", mock_random_predict, "å®Œå…¨éšæœºçš„é¢„æµ‹æ–¹æ³•", "baseline")
    benchmark.register_model("é¢‘ç‡é¢„æµ‹", mock_frequency_predict, "åŸºäºå†å²é¢‘ç‡çš„é¢„æµ‹æ–¹æ³•", "traditional")
    benchmark.register_model("é©¬å°”å¯å¤«é¢„æµ‹", mock_markov_predict, "åŸºäºé©¬å°”å¯å¤«é“¾çš„é¢„æµ‹æ–¹æ³•", "advanced")
    
    # è¯„ä¼°æ¨¡å‹
    print("\nğŸ“Š è¯„ä¼°æ¨¡å‹...")
    benchmark.evaluate_all_models(test_periods=20, verbose=True)
    
    # æ¯”è¾ƒæ¨¡å‹
    print("\nğŸ”„ æ¯”è¾ƒæ¨¡å‹...")
    benchmark.compare_models(verbose=True)
    
    # ç”ŸæˆæŠ¥å‘Š
    print("\nğŸ“ ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
    report = benchmark.generate_report("output/model_benchmark_report.md")
    
    # å¯è§†åŒ–æ¯”è¾ƒ
    print("\nğŸ“ˆ å¯è§†åŒ–æ¨¡å‹æ¯”è¾ƒ...")
    benchmark.visualize_comparison(output_path="output/model_comparison.png")
    
    # å¯è§†åŒ–å•ä¸ªæ¨¡å‹æ€§èƒ½
    print("\nğŸ“Š å¯è§†åŒ–æ¨¡å‹æ€§èƒ½...")
    benchmark.visualize_model_performance("é¢‘ç‡é¢„æµ‹", output_path="output/frequency_model_performance.png")
    
    # ä¿å­˜ç»“æœ
    print("\nğŸ’¾ ä¿å­˜è¯„ä¼°ç»“æœ...")
    benchmark.save_results("output/model_benchmark_results.json")
    
    print("\nâœ… æ¨¡å‹åŸºå‡†æµ‹è¯•æ¡†æ¶æµ‹è¯•å®Œæˆ")