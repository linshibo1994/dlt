#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¢å¼ºç‰¹æ€§æ¨¡å—
æä¾›é«˜çº§é¢„æµ‹å’Œåˆ†æåŠŸèƒ½
"""

import os
import sys
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Any, Callable, Union
from datetime import datetime
from collections import defaultdict, Counter
import matplotlib.pyplot as plt
import seaborn as sns

# å°è¯•å¯¼å…¥æ ¸å¿ƒæ¨¡å—
try:
    from core_modules import logger_manager, data_manager, cache_manager
except ImportError:
    # å¦‚æœåœ¨ä¸åŒç›®å½•è¿è¡Œï¼Œæ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from core_modules import logger_manager, data_manager, cache_manager

# å°è¯•å¯¼å…¥scikit-learn
try:
    from sklearn.cluster import KMeans, DBSCAN
    from sklearn.mixture import GaussianMixture
    from sklearn.decomposition import PCA
    from sklearn.preprocessing import StandardScaler
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    logger_manager.warning("scikit-learnæœªå®‰è£…ï¼Œéƒ¨åˆ†åŠŸèƒ½å°†ä¸å¯ç”¨")


class EnhancedFeatureAnalyzer:
    """å¢å¼ºç‰¹æ€§åˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¢å¼ºç‰¹æ€§åˆ†æå™¨"""
        self.df = data_manager.get_data()
        if self.df is None:
            logger_manager.error("æ•°æ®æœªåŠ è½½")
    
    def analyze_number_patterns(self, periods: int = 500) -> Dict:
        """åˆ†æå·ç æ¨¡å¼
        
        Args:
            periods: åˆ†ææœŸæ•°
            
        Returns:
            Dict: åˆ†æç»“æœ
        """
        if self.df is None or len(self.df) < periods:
            logger_manager.error(f"æ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ†æå·ç æ¨¡å¼")
            return {}
        
        cache_key = f"number_patterns_{periods}"
        cached_result = cache_manager.load_cache("analysis", cache_key)
        if cached_result:
            return cached_result
        
        df_subset = self.df.head(periods)
        
        # åˆå§‹åŒ–ç»“æœ
        result = {
            'consecutive_patterns': {},
            'sum_patterns': {},
            'odd_even_patterns': {},
            'big_small_patterns': {},
            'timestamp': datetime.now().isoformat()
        }
        
        # åˆ†æè¿å·æ¨¡å¼
        consecutive_counts = []
        for _, row in df_subset.iterrows():
            front_balls, _ = data_manager.parse_balls(row)
            front_balls = sorted(front_balls)
            consecutive_count = 0
            for i in range(len(front_balls) - 1):
                if front_balls[i + 1] - front_balls[i] == 1:
                    consecutive_count += 1
            consecutive_counts.append(consecutive_count)
        
        result['consecutive_patterns'] = {
            'counts': dict(Counter(consecutive_counts)),
            'avg': np.mean(consecutive_counts),
            'max': max(consecutive_counts),
            'min': min(consecutive_counts)
        }
        
        # åˆ†æå’Œå€¼æ¨¡å¼
        front_sums = []
        back_sums = []
        total_sums = []
        for _, row in df_subset.iterrows():
            front_balls, back_balls = data_manager.parse_balls(row)
            front_sum = sum(front_balls)
            back_sum = sum(back_balls)
            front_sums.append(front_sum)
            back_sums.append(back_sum)
            total_sums.append(front_sum + back_sum)
        
        result['sum_patterns'] = {
            'front_sum': {
                'avg': np.mean(front_sums),
                'std': np.std(front_sums),
                'max': max(front_sums),
                'min': min(front_sums),
                'distribution': dict(Counter(front_sums))
            },
            'back_sum': {
                'avg': np.mean(back_sums),
                'std': np.std(back_sums),
                'max': max(back_sums),
                'min': min(back_sums),
                'distribution': dict(Counter(back_sums))
            },
            'total_sum': {
                'avg': np.mean(total_sums),
                'std': np.std(total_sums),
                'max': max(total_sums),
                'min': min(total_sums),
                'distribution': dict(Counter(total_sums))
            }
        }
        
        # ç¼“å­˜ç»“æœ
        cache_manager.save_cache("analysis", cache_key, result)
        
        return result

    def cluster_analysis(self, periods: int = 500, n_clusters: int = 5) -> Dict:
        """èšç±»åˆ†æ
        
        Args:
            periods: åˆ†ææœŸæ•°
            n_clusters: èšç±»æ•°é‡
            
        Returns:
            Dict: åˆ†æç»“æœ
        """
        if not SKLEARN_AVAILABLE:
            logger_manager.error("scikit-learnæœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œèšç±»åˆ†æ")
            return {}
        
        if self.df is None or len(self.df) < periods:
            logger_manager.error(f"æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œèšç±»åˆ†æ")
            return {}
        
        cache_key = f"cluster_analysis_{periods}_{n_clusters}"
        cached_result = cache_manager.load_cache("analysis", cache_key)
        if cached_result:
            return cached_result
        
        df_subset = self.df.head(periods)
        
        # æå–ç‰¹å¾
        features = []
        for _, row in df_subset.iterrows():
            front_balls, back_balls = data_manager.parse_balls(row)
            
            # åŸºç¡€ç‰¹å¾
            front_sum = sum(front_balls)
            back_sum = sum(back_balls)
            front_mean = np.mean(front_balls)
            back_mean = np.mean(back_balls)
            front_std = np.std(front_balls)
            back_std = np.std(back_balls)
            
            # æ¨¡å¼ç‰¹å¾
            front_balls = sorted(front_balls)
            consecutive_count = sum(1 for i in range(len(front_balls)-1) if front_balls[i+1] - front_balls[i] == 1)
            front_odd = sum(1 for ball in front_balls if ball % 2 == 1)
            back_odd = sum(1 for ball in back_balls if ball % 2 == 1)
            front_big = sum(1 for ball in front_balls if ball >= 18)
            back_big = sum(1 for ball in back_balls if ball >= 7)
            
            # ç»„åˆç‰¹å¾
            feature_vector = [
                front_sum, back_sum, front_mean, back_mean, front_std, back_std,
                consecutive_count, front_odd, back_odd, front_big, back_big
            ]
            features.append(feature_vector)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        features = np.array(features)
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features)
        
        # é™ç»´ï¼ˆå¯é€‰ï¼‰
        pca = PCA(n_components=min(5, features.shape[1]))
        features_pca = pca.fit_transform(features_scaled)
        
        # K-Meansèšç±»
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        clusters = kmeans.fit_predict(features_scaled)
        
        # é«˜æ–¯æ··åˆæ¨¡å‹
        gmm = GaussianMixture(n_components=n_clusters, random_state=42)
        gmm_clusters = gmm.fit_predict(features_scaled)
        
        # DBSCANèšç±»
        dbscan = DBSCAN(eps=1.0, min_samples=5)
        dbscan_clusters = dbscan.fit_predict(features_scaled)
        
        # åˆ†æèšç±»ç»“æœ
        cluster_stats = {}
        for i in range(n_clusters):
            cluster_indices = np.where(clusters == i)[0]
            if len(cluster_indices) > 0:
                cluster_features = features[cluster_indices]
                cluster_stats[i] = {
                    'count': len(cluster_indices),
                    'front_sum_avg': np.mean(cluster_features[:, 0]),
                    'back_sum_avg': np.mean(cluster_features[:, 1]),
                    'consecutive_avg': np.mean(cluster_features[:, 6]),
                    'front_odd_avg': np.mean(cluster_features[:, 7]),
                    'back_odd_avg': np.mean(cluster_features[:, 8]),
                    'front_big_avg': np.mean(cluster_features[:, 9]),
                    'back_big_avg': np.mean(cluster_features[:, 10])
                }
        
        # å‡†å¤‡ç»“æœ
        result = {
            'kmeans': {
                'clusters': clusters.tolist(),
                'centers': kmeans.cluster_centers_.tolist(),
                'inertia': kmeans.inertia_,
                'stats': cluster_stats
            },
            'gmm': {
                'clusters': gmm_clusters.tolist(),
                'means': gmm.means_.tolist(),
                'weights': gmm.weights_.tolist()
            },
            'dbscan': {
                'clusters': dbscan_clusters.tolist(),
                'n_clusters': len(set(dbscan_clusters)) - (1 if -1 in dbscan_clusters else 0),
                'noise': np.sum(dbscan_clusters == -1)
            },
            'pca': {
                'components': pca.components_.tolist(),
                'explained_variance_ratio': pca.explained_variance_ratio_.tolist(),
                'features_pca': features_pca.tolist()
            },
            'feature_names': [
                'front_sum', 'back_sum', 'front_mean', 'back_mean', 'front_std', 'back_std',
                'consecutive_count', 'front_odd', 'back_odd', 'front_big', 'back_big'
            ],
            'timestamp': datetime.now().isoformat()
        }
        
        # ç¼“å­˜ç»“æœ
        cache_manager.save_cache("analysis", cache_key, result)
        
        return result
    
    def predict_with_patterns(self, count: int = 5) -> List[Tuple[List[int], List[int]]]:
        """åŸºäºæ¨¡å¼åˆ†æè¿›è¡Œé¢„æµ‹
        
        Args:
            count: é¢„æµ‹æ³¨æ•°
            
        Returns:
            List[Tuple[List[int], List[int]]]: é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        # åˆ†æå·ç æ¨¡å¼
        patterns = self.analyze_number_patterns(500)
        
        # è·å–å’Œå€¼åˆ†å¸ƒ
        front_sum_dist = patterns.get('sum_patterns', {}).get('front_sum', {}).get('distribution', {})
        back_sum_dist = patterns.get('sum_patterns', {}).get('back_sum', {}).get('distribution', {})
        
        # è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
        front_sum_probs = {}
        total_front = sum(front_sum_dist.values())
        for sum_val, count in front_sum_dist.items():
            front_sum_probs[sum_val] = count / total_front
        
        back_sum_probs = {}
        total_back = sum(back_sum_dist.values())
        for sum_val, count in back_sum_dist.items():
            back_sum_probs[sum_val] = count / total_back
        
        # è·å–è¿å·åˆ†å¸ƒ
        consecutive_dist = patterns.get('consecutive_patterns', {}).get('counts', {})
        consecutive_probs = {}
        total_consecutive = sum(consecutive_dist.values())
        for cons_val, count in consecutive_dist.items():
            consecutive_probs[cons_val] = count / total_consecutive
        
        # ç”Ÿæˆé¢„æµ‹
        predictions = []
        for _ in range(count):
            # é€‰æ‹©ç›®æ ‡å’Œå€¼
            front_sum_target = self._weighted_choice(front_sum_probs)
            back_sum_target = self._weighted_choice(back_sum_probs)
            consecutive_target = self._weighted_choice(consecutive_probs)
            
            # ç”Ÿæˆç¬¦åˆç›®æ ‡å’Œå€¼å’Œè¿å·æ•°çš„å·ç ç»„åˆ
            front_balls = self._generate_with_constraints(
                5, 1, 35, front_sum_target, consecutive_target
            )
            back_balls = self._generate_with_constraints(
                2, 1, 12, back_sum_target, 0
            )
            
            predictions.append((sorted(front_balls), sorted(back_balls)))
        
        return predictions
    
    def _weighted_choice(self, probs: Dict[int, float]) -> int:
        """åŠ æƒéšæœºé€‰æ‹©
        
        Args:
            probs: æ¦‚ç‡å­—å…¸ï¼Œé”®ä¸ºå€¼ï¼Œå€¼ä¸ºæ¦‚ç‡
            
        Returns:
            int: é€‰æ‹©çš„å€¼
        """
        items = list(probs.items())
        values = [item[0] for item in items]
        weights = [item[1] for item in items]
        return np.random.choice(values, p=weights)
    
    def _generate_with_constraints(self, count: int, min_val: int, max_val: int, 
                                  sum_target: int, consecutive_target: int) -> List[int]:
        """ç”Ÿæˆç¬¦åˆçº¦æŸçš„å·ç ç»„åˆ
        
        Args:
            count: å·ç æ•°é‡
            min_val: æœ€å°å€¼
            max_val: æœ€å¤§å€¼
            sum_target: ç›®æ ‡å’Œå€¼
            consecutive_target: ç›®æ ‡è¿å·æ•°
            
        Returns:
            List[int]: ç”Ÿæˆçš„å·ç åˆ—è¡¨
        """
        # æœ€å¤§å°è¯•æ¬¡æ•°
        max_attempts = 1000
        
        for _ in range(max_attempts):
            # éšæœºç”Ÿæˆå·ç 
            balls = sorted(np.random.choice(range(min_val, max_val + 1), count, replace=False))
            
            # è®¡ç®—å’Œå€¼
            balls_sum = sum(balls)
            
            # è®¡ç®—è¿å·æ•°
            consecutive_count = 0
            for i in range(len(balls) - 1):
                if balls[i + 1] - balls[i] == 1:
                    consecutive_count += 1
            
            # æ£€æŸ¥çº¦æŸ
            sum_diff = abs(balls_sum - sum_target)
            consecutive_diff = abs(consecutive_count - consecutive_target)
            
            # å¦‚æœå’Œå€¼å’Œè¿å·æ•°éƒ½æ¥è¿‘ç›®æ ‡ï¼Œè¿”å›ç»“æœ
            if sum_diff <= 5 and consecutive_diff <= 1:
                return balls
        
        # å¦‚æœè¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°ä»æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç»„åˆï¼Œä½¿ç”¨åŸºäºå†å²æ•°æ®çš„æ™ºèƒ½é€‰æ‹©
        # è·å–å†å²æ•°æ®è¿›è¡Œåˆ†æ
        from core_modules import data_manager
        historical_data = data_manager.get_data()

        if historical_data is not None and len(historical_data) > 0:
            # åŸºäºå†å²æ•°æ®çš„é¢‘ç‡åˆ†æé€‰æ‹©
            from collections import Counter
            counter = Counter()

            for _, row in historical_data.head(100).iterrows():  # ä½¿ç”¨æœ€è¿‘100æœŸæ•°æ®
                if min_val == 1 and max_val == 35:  # å‰åŒº
                    balls_str = str(row.get('front_balls', ''))
                else:  # ååŒº
                    balls_str = str(row.get('back_balls', ''))

                balls = [int(x) for x in balls_str.split(',') if x.strip().isdigit()]
                for ball in balls:
                    if min_val <= ball <= max_val:
                        counter[ball] += 1

            # é€‰æ‹©é¢‘ç‡æœ€é«˜çš„å·ç 
            if counter:
                most_common = [ball for ball, freq in counter.most_common()]
                selected = []
                for ball in most_common:
                    if len(selected) >= count:
                        break
                    if ball not in selected:
                        selected.append(ball)

                # å¦‚æœä¸å¤Ÿï¼Œè¡¥å……å‰©ä½™å·ç 
                while len(selected) < count:
                    for i in range(min_val, max_val + 1):
                        if i not in selected:
                            selected.append(i)
                            if len(selected) >= count:
                                break

                return sorted(selected[:count])

        # å¦‚æœæ²¡æœ‰å†å²æ•°æ®ï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒé€‰æ‹©
        selected = list(range(min_val, min(min_val + count, max_val + 1)))
        while len(selected) < count and len(selected) < (max_val - min_val + 1):
            for i in range(min_val, max_val + 1):
                if i not in selected:
                    selected.append(i)
                    if len(selected) >= count:
                        break

        return sorted(selected[:count])


class EnhancedFeaturePredictor:
    """å¢å¼ºç‰¹æ€§é¢„æµ‹å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¢å¼ºç‰¹æ€§é¢„æµ‹å™¨"""
        self.analyzer = EnhancedFeatureAnalyzer()
        self.df = data_manager.get_data()
        if self.df is None:
            logger_manager.error("æ•°æ®æœªåŠ è½½")
    
    def pattern_based_predict(self, count: int = 5) -> List[Tuple[List[int], List[int]]]:
        """åŸºäºæ¨¡å¼çš„é¢„æµ‹
        
        Args:
            count: é¢„æµ‹æ³¨æ•°
            
        Returns:
            List[Tuple[List[int], List[int]]]: é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        return self.analyzer.predict_with_patterns(count)
    
    def cluster_based_predict(self, count: int = 5) -> List[Tuple[List[int], List[int]]]:
        """åŸºäºèšç±»çš„é¢„æµ‹
        
        Args:
            count: é¢„æµ‹æ³¨æ•°
            
        Returns:
            List[Tuple[List[int], List[int]]]: é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        if not SKLEARN_AVAILABLE:
            logger_manager.error("scikit-learnæœªå®‰è£…ï¼Œæ— æ³•è¿›è¡ŒåŸºäºèšç±»çš„é¢„æµ‹")
            return []
        
        # è¿›è¡Œèšç±»åˆ†æ
        cluster_result = self.analyzer.cluster_analysis(500, 5)
        
        # è·å–æœ€å¤§çš„èšç±»
        kmeans_stats = cluster_result.get('kmeans', {}).get('stats', {})
        largest_cluster = max(kmeans_stats.items(), key=lambda x: x[1]['count'])[0]
        
        # è·å–è¯¥èšç±»çš„ç‰¹å¾å‡å€¼
        cluster_features = kmeans_stats[largest_cluster]
        
        # ç”Ÿæˆé¢„æµ‹
        predictions = []
        for _ in range(count):
            # åŸºäºèšç±»ç‰¹å¾ç”Ÿæˆå·ç 
            front_balls = self._generate_with_features(
                5, 1, 35,
                cluster_features['front_sum_avg'],
                cluster_features['consecutive_avg'],
                cluster_features['front_odd_avg'],
                cluster_features['front_big_avg']
            )
            
            back_balls = self._generate_with_features(
                2, 1, 12,
                cluster_features['back_sum_avg'],
                0,
                cluster_features['back_odd_avg'],
                cluster_features['back_big_avg']
            )
            
            predictions.append((sorted(front_balls), sorted(back_balls)))
        
        return predictions
    
    def _generate_with_features(self, count: int, min_val: int, max_val: int,
                               sum_avg: float, consecutive_avg: float,
                               odd_avg: float, big_avg: float) -> List[int]:
        """åŸºäºç‰¹å¾ç”Ÿæˆå·ç 
        
        Args:
            count: å·ç æ•°é‡
            min_val: æœ€å°å€¼
            max_val: æœ€å¤§å€¼
            sum_avg: å’Œå€¼å‡å€¼
            consecutive_avg: è¿å·å‡å€¼
            odd_avg: å¥‡æ•°å‡å€¼
            big_avg: å¤§å·å‡å€¼
            
        Returns:
            List[int]: ç”Ÿæˆçš„å·ç åˆ—è¡¨
        """
        # æœ€å¤§å°è¯•æ¬¡æ•°
        max_attempts = 1000
        
        # ç¡®å®šç›®æ ‡å€¼
        sum_target = int(round(sum_avg))
        consecutive_target = int(round(consecutive_avg))
        odd_target = int(round(odd_avg))
        big_target = int(round(big_avg))
        
        for _ in range(max_attempts):
            # éšæœºç”Ÿæˆå·ç 
            balls = sorted(np.random.choice(range(min_val, max_val + 1), count, replace=False))
            
            # è®¡ç®—ç‰¹å¾
            balls_sum = sum(balls)
            
            consecutive_count = 0
            for i in range(len(balls) - 1):
                if balls[i + 1] - balls[i] == 1:
                    consecutive_count += 1
            
            odd_count = sum(1 for ball in balls if ball % 2 == 1)
            
            big_threshold = 18 if max_val > 20 else max_val // 2
            big_count = sum(1 for ball in balls if ball >= big_threshold)
            
            # è®¡ç®—ç‰¹å¾å·®å¼‚
            sum_diff = abs(balls_sum - sum_target)
            consecutive_diff = abs(consecutive_count - consecutive_target)
            odd_diff = abs(odd_count - odd_target)
            big_diff = abs(big_count - big_target)
            
            # è®¡ç®—æ€»å·®å¼‚
            total_diff = sum_diff / count + consecutive_diff + odd_diff + big_diff
            
            # å¦‚æœæ€»å·®å¼‚è¾ƒå°ï¼Œè¿”å›ç»“æœ
            if total_diff <= 3:
                return balls
        
        # å¦‚æœè¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°ä»æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç»„åˆï¼Œæ”¾å®½æ¡ä»¶
        return sorted(np.random.choice(range(min_val, max_val + 1), count, replace=False))


# å…¨å±€å®ä¾‹
_enhanced_feature_analyzer = None
_enhanced_feature_predictor = None

def get_enhanced_feature_analyzer() -> EnhancedFeatureAnalyzer:
    """è·å–å¢å¼ºç‰¹æ€§åˆ†æå™¨å®ä¾‹"""
    global _enhanced_feature_analyzer
    if _enhanced_feature_analyzer is None:
        _enhanced_feature_analyzer = EnhancedFeatureAnalyzer()
    return _enhanced_feature_analyzer

def get_enhanced_feature_predictor() -> EnhancedFeaturePredictor:
    """è·å–å¢å¼ºç‰¹æ€§é¢„æµ‹å™¨å®ä¾‹"""
    global _enhanced_feature_predictor
    if _enhanced_feature_predictor is None:
        _enhanced_feature_predictor = EnhancedFeaturePredictor()
    return _enhanced_feature_predictor


if __name__ == "__main__":
    # æµ‹è¯•å¢å¼ºç‰¹æ€§æ¨¡å—
    print("ğŸ” æµ‹è¯•å¢å¼ºç‰¹æ€§æ¨¡å—...")
    
    # æµ‹è¯•å·ç æ¨¡å¼åˆ†æ
    analyzer = get_enhanced_feature_analyzer()
    print("\nğŸ“Š åˆ†æå·ç æ¨¡å¼...")
    patterns = analyzer.analyze_number_patterns(300)
    
    print(f"è¿å·æ¨¡å¼: å¹³å‡è¿å·æ•° {patterns['consecutive_patterns']['avg']:.2f}")
    print(f"å’Œå€¼æ¨¡å¼: å‰åŒºå¹³å‡å’Œå€¼ {patterns['sum_patterns']['front_sum']['avg']:.2f}, ååŒºå¹³å‡å’Œå€¼ {patterns['sum_patterns']['back_sum']['avg']:.2f}")
    
    # æµ‹è¯•èšç±»åˆ†æ
    if SKLEARN_AVAILABLE:
        print("\nğŸ“Š èšç±»åˆ†æ...")
        clusters = analyzer.cluster_analysis(300, 5)
        print(f"K-Meansèšç±»: {len(clusters['kmeans']['stats'])} ä¸ªèšç±»")
        print(f"GMMèšç±»: {len(clusters['gmm']['weights'])} ä¸ªèšç±»")
        print(f"DBSCANèšç±»: {clusters['dbscan']['n_clusters']} ä¸ªèšç±», {clusters['dbscan']['noise']} ä¸ªå™ªå£°ç‚¹")
    
    # æµ‹è¯•åŸºäºæ¨¡å¼çš„é¢„æµ‹
    predictor = get_enhanced_feature_predictor()
    print("\nğŸ¯ åŸºäºæ¨¡å¼çš„é¢„æµ‹...")
    pattern_predictions = predictor.pattern_based_predict(3)
    for i, (front, back) in enumerate(pattern_predictions):
        print(f"  ç¬¬ {i+1} æ³¨: {front} + {back}")
    
    # æµ‹è¯•åŸºäºèšç±»çš„é¢„æµ‹
    if SKLEARN_AVAILABLE:
        print("\nğŸ¯ åŸºäºèšç±»çš„é¢„æµ‹...")
        cluster_predictions = predictor.cluster_based_predict(3)
        for i, (front, back) in enumerate(cluster_predictions):
            print(f"  ç¬¬ {i+1} æ³¨: {front} + {back}")
    
    print("\nâœ… å¢å¼ºç‰¹æ€§æ¨¡å—æµ‹è¯•å®Œæˆ")