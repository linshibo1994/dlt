#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
é›†æˆæ¨¡å—
æ•´åˆå„ç§é¢„æµ‹ç®—æ³•ï¼Œæä¾›ç»Ÿä¸€çš„æ¥å£
"""

import os
import sys
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Any, Callable, Union
from datetime import datetime
from collections import defaultdict, Counter

# å°è¯•å¯¼å…¥æ ¸å¿ƒæ¨¡å—
try:
    from core_modules import logger_manager, data_manager, cache_manager
except ImportError:
    # å¦‚æœåœ¨ä¸åŒç›®å½•è¿è¡Œï¼Œæ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from core_modules import logger_manager, data_manager, cache_manager

# å°è¯•å¯¼å…¥é¢„æµ‹å™¨æ¨¡å—
try:
    from predictor_modules import get_traditional_predictor, get_advanced_predictor, get_super_predictor
    PREDICTORS_AVAILABLE = True
except ImportError:
    PREDICTORS_AVAILABLE = False
    logger_manager.warning("é¢„æµ‹å™¨æ¨¡å—æœªæ‰¾åˆ°ï¼Œéƒ¨åˆ†åŠŸèƒ½å°†ä¸å¯ç”¨")

# å°è¯•å¯¼å…¥å¢å¼ºé©¬å°”å¯å¤«é¢„æµ‹å™¨
try:
    from improvements.enhanced_markov import get_markov_predictor
    ENHANCED_MARKOV_AVAILABLE = True
except ImportError:
    ENHANCED_MARKOV_AVAILABLE = False
    logger_manager.warning("å¢å¼ºé©¬å°”å¯å¤«æ¨¡å—æœªæ‰¾åˆ°ï¼Œéƒ¨åˆ†åŠŸèƒ½å°†ä¸å¯ç”¨")

# å°è¯•å¯¼å…¥å¢å¼ºç‰¹æ€§é¢„æµ‹å™¨
try:
    from improvements.enhanced_features import get_enhanced_feature_predictor
    ENHANCED_FEATURES_AVAILABLE = True
except ImportError:
    ENHANCED_FEATURES_AVAILABLE = False
    logger_manager.warning("å¢å¼ºç‰¹æ€§æ¨¡å—æœªæ‰¾åˆ°ï¼Œéƒ¨åˆ†åŠŸèƒ½å°†ä¸å¯ç”¨")

# å°è¯•å¯¼å…¥LSTMé¢„æµ‹å™¨
try:
    from advanced_lstm_predictor import AdvancedLSTMPredictor, TENSORFLOW_AVAILABLE
except ImportError:
    TENSORFLOW_AVAILABLE = False
    logger_manager.warning("LSTMé¢„æµ‹å™¨æ¨¡å—æœªæ‰¾åˆ°ï¼Œéƒ¨åˆ†åŠŸèƒ½å°†ä¸å¯ç”¨")


class IntegratedPredictor:
    """é›†æˆé¢„æµ‹å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–é›†æˆé¢„æµ‹å™¨"""
        self.df = data_manager.get_data()
        if self.df is None:
            logger_manager.error("æ•°æ®æœªåŠ è½½")
        
        # é¢„æµ‹å™¨å­—å…¸
        self.predictors = {}
        
        # åŠ è½½é¢„æµ‹å™¨
        self._load_predictors()
    
    def _load_predictors(self):
        """åŠ è½½é¢„æµ‹å™¨"""
        # åŠ è½½ä¼ ç»Ÿé¢„æµ‹å™¨
        if PREDICTORS_AVAILABLE:
            try:
                traditional = get_traditional_predictor()
                advanced = get_advanced_predictor()
                super_predictor = get_super_predictor()
                
                self.predictors.update({
                    'frequency': lambda data, count=3: traditional.frequency_predict(count),
                    'hot_cold': lambda data, count=3: traditional.hot_cold_predict(count),
                    'missing': lambda data, count=3: traditional.missing_predict(count),
                    'markov': lambda data, count=3: advanced.markov_predict(count),
                    'bayesian': lambda data, count=3: advanced.bayesian_predict(count),
                    'ensemble': lambda data, count=3: advanced.ensemble_predict(count),
                    'super': lambda data, count=3: super_predictor.predict_super(count)
                })
                
                logger_manager.info("å·²åŠ è½½ä¼ ç»Ÿé¢„æµ‹å™¨")
            except Exception as e:
                logger_manager.error(f"åŠ è½½ä¼ ç»Ÿé¢„æµ‹å™¨å¤±è´¥: {e}")
        
        # åŠ è½½å¢å¼ºé©¬å°”å¯å¤«é¢„æµ‹å™¨
        if ENHANCED_MARKOV_AVAILABLE:
            try:
                markov_predictor = get_markov_predictor()
                
                self.predictors.update({
                    'markov_2nd': lambda data, count=3: markov_predictor.multi_order_markov_predict(count, 300, 2),
                    'markov_3rd': lambda data, count=3: markov_predictor.multi_order_markov_predict(count, 300, 3),
                    'adaptive_markov': lambda data, count=3: [
                        (pred['front_balls'], pred['back_balls']) 
                        for pred in markov_predictor.adaptive_order_markov_predict(count, 300)
                    ]
                })
                
                logger_manager.info("å·²åŠ è½½å¢å¼ºé©¬å°”å¯å¤«é¢„æµ‹å™¨")
            except Exception as e:
                logger_manager.error(f"åŠ è½½å¢å¼ºé©¬å°”å¯å¤«é¢„æµ‹å™¨å¤±è´¥: {e}")
        
        # åŠ è½½å¢å¼ºç‰¹æ€§é¢„æµ‹å™¨
        if ENHANCED_FEATURES_AVAILABLE:
            try:
                feature_predictor = get_enhanced_feature_predictor()
                
                self.predictors.update({
                    'pattern_based': lambda data, count=3: feature_predictor.pattern_based_predict(count),
                    'cluster_based': lambda data, count=3: feature_predictor.cluster_based_predict(count)
                })
                
                logger_manager.info("å·²åŠ è½½å¢å¼ºç‰¹æ€§é¢„æµ‹å™¨")
            except Exception as e:
                logger_manager.error(f"åŠ è½½å¢å¼ºç‰¹æ€§é¢„æµ‹å™¨å¤±è´¥: {e}")
        
        # åŠ è½½LSTMé¢„æµ‹å™¨
        if TENSORFLOW_AVAILABLE:
            try:
                lstm_predictor = AdvancedLSTMPredictor()
                
                self.predictors.update({
                    'lstm': lambda data, count=3: lstm_predictor.lstm_predict(count)
                })
                
                logger_manager.info("å·²åŠ è½½LSTMé¢„æµ‹å™¨")
            except Exception as e:
                logger_manager.error(f"åŠ è½½LSTMé¢„æµ‹å™¨å¤±è´¥: {e}")
    
    def stacking_predict(self, count: int = 3) -> List[Dict]:
        """Stackingé›†æˆé¢„æµ‹
        
        Args:
            count: é¢„æµ‹æ³¨æ•°
            
        Returns:
            List[Dict]: é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        if not self.predictors:
            logger_manager.error("æ²¡æœ‰å¯ç”¨çš„é¢„æµ‹å™¨")
            return []
        
        # æ”¶é›†å„é¢„æµ‹å™¨çš„é¢„æµ‹ç»“æœ
        all_predictions = {}
        for name, predictor in self.predictors.items():
            try:
                predictions = predictor(self.df, count)
                all_predictions[name] = predictions
            except Exception as e:
                logger_manager.error(f"é¢„æµ‹å™¨ {name} é¢„æµ‹å¤±è´¥: {e}")
        
        # Stackingèåˆ
        results = []
        for i in range(count):
            # æ”¶é›†ç¬¬iæ³¨çš„æ‰€æœ‰é¢„æµ‹
            front_candidates = []
            back_candidates = []
            
            for name, predictions in all_predictions.items():
                if i < len(predictions):
                    if isinstance(predictions[i], tuple) and len(predictions[i]) == 2:
                        front, back = predictions[i]
                    elif isinstance(predictions[i], dict) and 'front_balls' in predictions[i] and 'back_balls' in predictions[i]:
                        front = predictions[i]['front_balls']
                        back = predictions[i]['back_balls']
                    else:
                        logger_manager.warning(f"æ— æ³•è§£æé¢„æµ‹ç»“æœ: {predictions[i]}")
                        continue
                    
                    front_candidates.extend(front)
                    back_candidates.extend(back)
            
            # ä½¿ç”¨æŠ•ç¥¨æœºåˆ¶é€‰æ‹©æœ€ç»ˆå·ç 
            front_counter = Counter(front_candidates)
            back_counter = Counter(back_candidates)
            
            front_balls = [ball for ball, _ in front_counter.most_common(5)]
            back_balls = [ball for ball, _ in back_counter.most_common(2)]
            
            # å¦‚æœå·ç ä¸è¶³ï¼Œä½¿ç”¨é¢‘ç‡åˆ†æè¡¥å……
            if len(front_balls) < 5:
                from analyzer_modules import basic_analyzer
                freq_analysis = basic_analyzer.frequency_analysis()
                front_freq = freq_analysis.get('front_frequency', {})
                sorted_freq = sorted(front_freq.items(), key=lambda x: x[1], reverse=True)

                for ball, _ in sorted_freq:
                    if len(front_balls) >= 5:
                        break
                    ball_int = int(ball) if isinstance(ball, str) else ball
                    if ball_int not in front_balls:
                        front_balls.append(ball_int)

            if len(back_balls) < 2:
                from analyzer_modules import basic_analyzer
                freq_analysis = basic_analyzer.frequency_analysis()
                back_freq = freq_analysis.get('back_frequency', {})
                sorted_freq = sorted(back_freq.items(), key=lambda x: x[1], reverse=True)

                for ball, _ in sorted_freq:
                    if len(back_balls) >= 2:
                        break
                    ball_int = int(ball) if isinstance(ball, str) else ball
                    if ball_int not in back_balls:
                        back_balls.append(ball_int)
            
            # æ„å»ºç»“æœ
            result = {
                'front_balls': sorted(front_balls),
                'back_balls': sorted(back_balls),
                'method': 'stacking',
                'confidence': 0.85,
                'predictors_used': list(all_predictions.keys())
            }
            
            results.append(result)
        
        return results
    
    def weighted_ensemble_predict(self, count: int = 3) -> List[Dict]:
        """åŠ æƒé›†æˆé¢„æµ‹
        
        Args:
            count: é¢„æµ‹æ³¨æ•°
            
        Returns:
            List[Dict]: é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        if not self.predictors:
            logger_manager.error("æ²¡æœ‰å¯ç”¨çš„é¢„æµ‹å™¨")
            return []
        
        # é¢„æµ‹å™¨æƒé‡
        weights = {
            'frequency': 0.1,
            'hot_cold': 0.1,
            'missing': 0.1,
            'markov': 0.15,
            'bayesian': 0.15,
            'ensemble': 0.15,
            'super': 0.2,
            'markov_2nd': 0.2,
            'markov_3rd': 0.2,
            'adaptive_markov': 0.25,
            'pattern_based': 0.15,
            'cluster_based': 0.15,
            'lstm': 0.25
        }
        
        # æ”¶é›†å„é¢„æµ‹å™¨çš„é¢„æµ‹ç»“æœ
        all_predictions = {}
        for name, predictor in self.predictors.items():
            try:
                predictions = predictor(self.df, count)
                all_predictions[name] = predictions
            except Exception as e:
                logger_manager.error(f"é¢„æµ‹å™¨ {name} é¢„æµ‹å¤±è´¥: {e}")
        
        # åŠ æƒèåˆ
        results = []
        for i in range(count):
            # å‰åŒºå’ŒååŒºå·ç çš„åŠ æƒå¾—åˆ†
            front_scores = defaultdict(float)
            back_scores = defaultdict(float)
            
            # è®¡ç®—æ¯ä¸ªå·ç çš„åŠ æƒå¾—åˆ†
            for name, predictions in all_predictions.items():
                if i < len(predictions):
                    weight = weights.get(name, 0.1)
                    
                    if isinstance(predictions[i], tuple) and len(predictions[i]) == 2:
                        front, back = predictions[i]
                    elif isinstance(predictions[i], dict) and 'front_balls' in predictions[i] and 'back_balls' in predictions[i]:
                        front = predictions[i]['front_balls']
                        back = predictions[i]['back_balls']
                    else:
                        logger_manager.warning(f"æ— æ³•è§£æé¢„æµ‹ç»“æœ: {predictions[i]}")
                        continue
                    
                    for ball in front:
                        front_scores[ball] += weight
                    
                    for ball in back:
                        back_scores[ball] += weight
            
            # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„å·ç 
            front_balls = [ball for ball, _ in sorted(front_scores.items(), key=lambda x: x[1], reverse=True)[:5]]
            back_balls = [ball for ball, _ in sorted(back_scores.items(), key=lambda x: x[1], reverse=True)[:2]]
            
            # å¦‚æœå·ç ä¸è¶³ï¼Œéšæœºè¡¥å……
            while len(front_balls) < 5:
                ball = np.random.randint(1, 36)
                if ball not in front_balls:
                    front_balls.append(ball)
            
            while len(back_balls) < 2:
                ball = np.random.randint(1, 13)
                if ball not in back_balls:
                    back_balls.append(ball)
            
            # æ„å»ºç»“æœ
            result = {
                'front_balls': sorted(front_balls),
                'back_balls': sorted(back_balls),
                'method': 'weighted_ensemble',
                'confidence': 0.9,
                'predictors_used': list(all_predictions.keys()),
                'weights': {name: weights.get(name, 0.1) for name in all_predictions.keys()}
            }
            
            results.append(result)
        
        return results
    
    def adaptive_ensemble_predict(self, count: int = 3) -> List[Dict]:
        """è‡ªé€‚åº”é›†æˆé¢„æµ‹
        
        Args:
            count: é¢„æµ‹æ³¨æ•°
            
        Returns:
            List[Dict]: é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        if not self.predictors:
            logger_manager.error("æ²¡æœ‰å¯ç”¨çš„é¢„æµ‹å™¨")
            return []
        
        # åŠ è½½å†å²æ€§èƒ½æ•°æ®
        performance_data = self._load_performance_data()
        
        # æ ¹æ®å†å²æ€§èƒ½è®¡ç®—æƒé‡
        weights = self._calculate_adaptive_weights(performance_data)
        
        # æ”¶é›†å„é¢„æµ‹å™¨çš„é¢„æµ‹ç»“æœ
        all_predictions = {}
        for name, predictor in self.predictors.items():
            try:
                predictions = predictor(self.df, count)
                all_predictions[name] = predictions
            except Exception as e:
                logger_manager.error(f"é¢„æµ‹å™¨ {name} é¢„æµ‹å¤±è´¥: {e}")
        
        # è‡ªé€‚åº”èåˆ
        results = []
        for i in range(count):
            # å‰åŒºå’ŒååŒºå·ç çš„åŠ æƒå¾—åˆ†
            front_scores = defaultdict(float)
            back_scores = defaultdict(float)
            
            # è®¡ç®—æ¯ä¸ªå·ç çš„åŠ æƒå¾—åˆ†
            for name, predictions in all_predictions.items():
                if i < len(predictions):
                    weight = weights.get(name, 0.1)
                    
                    if isinstance(predictions[i], tuple) and len(predictions[i]) == 2:
                        front, back = predictions[i]
                    elif isinstance(predictions[i], dict) and 'front_balls' in predictions[i] and 'back_balls' in predictions[i]:
                        front = predictions[i]['front_balls']
                        back = predictions[i]['back_balls']
                    else:
                        logger_manager.warning(f"æ— æ³•è§£æé¢„æµ‹ç»“æœ: {predictions[i]}")
                        continue
                    
                    for ball in front:
                        front_scores[ball] += weight
                    
                    for ball in back:
                        back_scores[ball] += weight
            
            # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„å·ç 
            front_balls = [ball for ball, _ in sorted(front_scores.items(), key=lambda x: x[1], reverse=True)[:5]]
            back_balls = [ball for ball, _ in sorted(back_scores.items(), key=lambda x: x[1], reverse=True)[:2]]
            
            # å¦‚æœå·ç ä¸è¶³ï¼Œéšæœºè¡¥å……
            while len(front_balls) < 5:
                ball = np.random.randint(1, 36)
                if ball not in front_balls:
                    front_balls.append(ball)
            
            while len(back_balls) < 2:
                ball = np.random.randint(1, 13)
                if ball not in back_balls:
                    back_balls.append(ball)
            
            # æ„å»ºç»“æœ
            result = {
                'front_balls': sorted(front_balls),
                'back_balls': sorted(back_balls),
                'method': 'adaptive_ensemble',
                'confidence': 0.95,
                'predictors_used': list(all_predictions.keys()),
                'weights': weights
            }
            
            results.append(result)
        
        return results
    
    def _load_performance_data(self) -> Dict:
        """åŠ è½½å†å²æ€§èƒ½æ•°æ®"""
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        cache_key = "predictor_performance_data"
        cached_data = cache_manager.load_cache("analysis", cache_key)
        if cached_data:
            return cached_data
        
        # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œä½¿ç”¨é»˜è®¤å€¼
        default_data = {
            'frequency': 0.5,
            'hot_cold': 0.5,
            'missing': 0.5,
            'markov': 0.6,
            'bayesian': 0.6,
            'ensemble': 0.7,
            'super': 0.7,
            'markov_2nd': 0.7,
            'markov_3rd': 0.7,
            'adaptive_markov': 0.8,
            'pattern_based': 0.6,
            'cluster_based': 0.6,
            'lstm': 0.8
        }
        
        return default_data
    
    def _calculate_adaptive_weights(self, performance_data: Dict) -> Dict:
        """è®¡ç®—è‡ªé€‚åº”æƒé‡"""
        weights = {}
        
        # æ ¹æ®æ€§èƒ½æ•°æ®è®¡ç®—æƒé‡
        for name, performance in performance_data.items():
            # ä½¿ç”¨æ€§èƒ½å€¼çš„å¹³æ–¹ä½œä¸ºæƒé‡ï¼Œä½¿å¾—é«˜æ€§èƒ½çš„é¢„æµ‹å™¨æƒé‡æ›´é«˜
            weights[name] = performance ** 2
        
        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(weights.values())
        if total_weight > 0:
            weights = {name: weight / total_weight for name, weight in weights.items()}
        
        return weights
    
    def transformer_predict(self, count: int = 3, periods: int = 500) -> List[Dict]:
        """Transformeré¢„æµ‹ - çœŸå®çš„Transformeræ³¨æ„åŠ›æœºåˆ¶å®ç°

        Args:
            count: é¢„æµ‹æ³¨æ•°
            periods: åˆ†ææœŸæ•°

        Returns:
            List[Dict]: é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        try:
            logger_manager.info(f"å¼€å§‹Transformeré¢„æµ‹: æ³¨æ•°={count}, åˆ†ææœŸæ•°={periods}")

            # å°è¯•ä½¿ç”¨enhanced_deep_learningæ¨¡å—çš„Transformerå®ç°
            try:
                from enhanced_deep_learning.models.transformer_predictor import TransformerPredictor
                from enhanced_deep_learning.models.base_model import ModelMetadata

                # é…ç½®Transformerå‚æ•°
                config = {
                    'd_model': 256,
                    'num_heads': 8,
                    'num_encoder_layers': 6,
                    'num_decoder_layers': 6,
                    'dff': 1024,
                    'dropout_rate': 0.1,
                    'use_relative_position': True,
                    'use_sparse_attention': True,
                    'use_local_attention': True,
                    'local_attention_window': 32,
                    'sequence_length': 50,
                    'learning_rate': 0.0001,
                    'batch_size': 32,
                    'epochs': 100
                }

                # åˆ›å»ºTransformeré¢„æµ‹å™¨
                transformer = TransformerPredictor(config=config)

                # è·å–å†å²æ•°æ®
                historical_data = data_manager.get_data()
                if historical_data is None or len(historical_data) == 0:
                    raise ValueError("æ— æ³•è·å–å†å²æ•°æ®")

                # ä½¿ç”¨æŒ‡å®šæœŸæ•°çš„æœ€æ–°æ•°æ®
                if len(historical_data) > periods:
                    historical_data = historical_data.tail(periods)

                logger_manager.info(f"ä½¿ç”¨{len(historical_data)}æœŸå†å²æ•°æ®è¿›è¡ŒTransformerè®­ç»ƒå’Œé¢„æµ‹")

                # è¿›è¡Œé¢„æµ‹
                predictions = transformer.predict(historical_data, count=count)

                # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                results = []
                for i, pred in enumerate(predictions):
                    if isinstance(pred, tuple) and len(pred) == 2:
                        front_balls, back_balls = pred
                        result = {
                            'front_balls': list(front_balls),
                            'back_balls': list(back_balls),
                            'method': 'transformer',
                            'confidence': 0.88,
                            'model_type': 'deep_learning',
                            'algorithm': 'multi_head_attention'
                        }
                        results.append(result)
                    elif isinstance(pred, dict):
                        pred['method'] = 'transformer'
                        pred['confidence'] = 0.88
                        pred['model_type'] = 'deep_learning'
                        pred['algorithm'] = 'multi_head_attention'
                        results.append(pred)

                logger_manager.info(f"Transformeré¢„æµ‹å®Œæˆï¼Œç”Ÿæˆ{len(results)}æ³¨é¢„æµ‹")
                return results

            except ImportError as e:
                logger_manager.warning(f"Enhanced Transformeræ¨¡å—ä¸å¯ç”¨: {e}")
                # å›é€€åˆ°ç®€åŒ–çš„Transformerå®ç°
                return self._fallback_transformer_predict(count, periods)

        except Exception as e:
            logger_manager.error(f"Transformeré¢„æµ‹å¤±è´¥: {e}")
            # å›é€€åˆ°ç®€åŒ–å®ç°
            return self._fallback_transformer_predict(count, periods)

    def _fallback_transformer_predict(self, count: int = 3, periods: int = 500) -> List[Dict]:
        """Transformeré¢„æµ‹å›é€€å®ç° - åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„ç®€åŒ–å®ç°"""
        try:
            logger_manager.info("ä½¿ç”¨Transformerå›é€€å®ç°")

            # è·å–å†å²æ•°æ®
            historical_data = data_manager.get_data()
            if historical_data is None or len(historical_data) == 0:
                raise ValueError("æ— æ³•è·å–å†å²æ•°æ®")

            # ä½¿ç”¨æŒ‡å®šæœŸæ•°çš„æœ€æ–°æ•°æ®
            if len(historical_data) > periods:
                historical_data = historical_data.tail(periods)

            # ç®€åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶å®ç°
            front_data = []
            back_data = []

            for _, row in historical_data.iterrows():
                front_nums = [int(x) for x in str(row['front']).split(',') if x.strip().isdigit()]
                back_nums = [int(x) for x in str(row['back']).split(',') if x.strip().isdigit()]

                if len(front_nums) == 5 and len(back_nums) == 2:
                    front_data.append(front_nums)
                    back_data.append(back_nums)

            if len(front_data) < 10:
                raise ValueError("å†å²æ•°æ®ä¸è¶³")

            # ç®€åŒ–çš„æ³¨æ„åŠ›æƒé‡è®¡ç®—
            front_attention_weights = self._calculate_attention_weights(front_data)
            back_attention_weights = self._calculate_attention_weights(back_data)

            results = []
            for i in range(count):
                # åŸºäºæ³¨æ„åŠ›æƒé‡ç”Ÿæˆé¢„æµ‹
                front_pred = self._generate_with_attention(front_attention_weights, 5, 1, 35)
                back_pred = self._generate_with_attention(back_attention_weights, 2, 1, 12)

                result = {
                    'front_balls': sorted(front_pred),
                    'back_balls': sorted(back_pred),
                    'method': 'transformer_fallback',
                    'confidence': 0.75,
                    'model_type': 'attention_based',
                    'algorithm': 'simplified_attention'
                }
                results.append(result)

            logger_manager.info(f"Transformerå›é€€é¢„æµ‹å®Œæˆï¼Œç”Ÿæˆ{len(results)}æ³¨é¢„æµ‹")
            return results

        except Exception as e:
            logger_manager.error(f"Transformerå›é€€é¢„æµ‹å¤±è´¥: {e}")
            return []

    def _calculate_attention_weights(self, data_sequences: List[List[int]]) -> Dict[int, float]:
        """è®¡ç®—ç®€åŒ–çš„æ³¨æ„åŠ›æƒé‡"""
        try:
            import numpy as np

            # å°†åºåˆ—è½¬æ¢ä¸ºnumpyæ•°ç»„
            sequences = np.array(data_sequences)

            # è®¡ç®—æ¯ä¸ªå·ç çš„å‡ºç°é¢‘ç‡
            all_numbers = sequences.flatten()
            unique_numbers, counts = np.unique(all_numbers, return_counts=True)

            # è®¡ç®—ä½ç½®æƒé‡ï¼ˆæœ€è¿‘çš„æ•°æ®æƒé‡æ›´é«˜ï¼‰
            position_weights = np.exp(-0.1 * np.arange(len(data_sequences))[::-1])

            # è®¡ç®—åŠ æƒé¢‘ç‡
            weighted_freq = {}
            for num in unique_numbers:
                weighted_count = 0
                for i, seq in enumerate(data_sequences):
                    if num in seq:
                        weighted_count += position_weights[i]
                weighted_freq[num] = weighted_count

            # å½’ä¸€åŒ–æƒé‡
            total_weight = sum(weighted_freq.values())
            if total_weight > 0:
                attention_weights = {num: weight / total_weight for num, weight in weighted_freq.items()}
            else:
                attention_weights = {num: 1.0 / len(unique_numbers) for num in unique_numbers}

            return attention_weights

        except Exception as e:
            logger_manager.error(f"è®¡ç®—æ³¨æ„åŠ›æƒé‡å¤±è´¥: {e}")
            return {}

    def _generate_with_attention(self, attention_weights: Dict[int, float],
                                count: int, min_num: int, max_num: int) -> List[int]:
        """åŸºäºæ³¨æ„åŠ›æƒé‡ç”Ÿæˆå·ç """
        try:
            import numpy as np

            if not attention_weights:
                # å¦‚æœæ²¡æœ‰æƒé‡ï¼Œéšæœºç”Ÿæˆ
                return list(np.random.choice(range(min_num, max_num + 1), count, replace=False))

            # æ ¹æ®æ³¨æ„åŠ›æƒé‡é€‰æ‹©å·ç 
            numbers = list(attention_weights.keys())
            weights = list(attention_weights.values())

            # è¿‡æ»¤æœ‰æ•ˆèŒƒå›´å†…çš„å·ç 
            valid_numbers = [num for num in numbers if min_num <= num <= max_num]
            valid_weights = [attention_weights[num] for num in valid_numbers]

            if len(valid_numbers) < count:
                # å¦‚æœæœ‰æ•ˆå·ç ä¸è¶³ï¼Œè¡¥å……éšæœºå·ç 
                additional_numbers = [num for num in range(min_num, max_num + 1)
                                    if num not in valid_numbers]
                need_additional = count - len(valid_numbers)
                if need_additional > 0 and additional_numbers:
                    additional_selected = list(np.random.choice(
                        additional_numbers,
                        min(need_additional, len(additional_numbers)),
                        replace=False
                    ))
                    valid_numbers.extend(additional_selected)
                    valid_weights.extend([0.1] * len(additional_selected))

            # å½’ä¸€åŒ–æƒé‡
            total_weight = sum(valid_weights)
            if total_weight > 0:
                normalized_weights = [w / total_weight for w in valid_weights]
            else:
                normalized_weights = [1.0 / len(valid_weights)] * len(valid_weights)

            # åŸºäºæƒé‡é€‰æ‹©å·ç 
            selected = list(np.random.choice(
                valid_numbers,
                min(count, len(valid_numbers)),
                p=normalized_weights,
                replace=False
            ))

            return selected

        except Exception as e:
            logger_manager.error(f"åŸºäºæ³¨æ„åŠ›æƒé‡ç”Ÿæˆå·ç å¤±è´¥: {e}")
            import random
            return random.sample(range(min_num, max_num + 1), count)
    
    def gan_predict(self, count: int = 3, periods: int = 500) -> List[Dict]:
        """GANé¢„æµ‹ - çœŸå®çš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå®ç°

        Args:
            count: é¢„æµ‹æ³¨æ•°
            periods: åˆ†ææœŸæ•°

        Returns:
            List[Dict]: é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        try:
            logger_manager.info(f"å¼€å§‹GANé¢„æµ‹: æ³¨æ•°={count}, åˆ†ææœŸæ•°={periods}")

            # å°è¯•ä½¿ç”¨enhanced_deep_learningæ¨¡å—çš„GANå®ç°
            try:
                from enhanced_deep_learning.models.gan_predictor import GANPredictor
                from enhanced_deep_learning.models.base_model import ModelMetadata

                # é…ç½®GANå‚æ•°
                config = {
                    'latent_dim': 100,
                    'generator_layers': [256, 512, 256, 128],
                    'discriminator_layers': [128, 256, 128],
                    'learning_rate': 0.0002,
                    'beta1': 0.5,
                    'beta2': 0.999,
                    'batch_size': 64,
                    'epochs': 200,
                    'generator_lr': 0.0002,
                    'discriminator_lr': 0.0002,
                    'use_conditional': True,
                    'num_conditions': 10
                }

                # åˆ›å»ºGANé¢„æµ‹å™¨
                gan = GANPredictor(config=config)

                # è·å–å†å²æ•°æ®
                historical_data = data_manager.get_data()
                if historical_data is None or len(historical_data) == 0:
                    raise ValueError("æ— æ³•è·å–å†å²æ•°æ®")

                # ä½¿ç”¨æŒ‡å®šæœŸæ•°çš„æœ€æ–°æ•°æ®
                if len(historical_data) > periods:
                    historical_data = historical_data.tail(periods)

                logger_manager.info(f"ä½¿ç”¨{len(historical_data)}æœŸå†å²æ•°æ®è¿›è¡ŒGANè®­ç»ƒå’Œé¢„æµ‹")

                # è¿›è¡Œé¢„æµ‹
                predictions = gan.predict(historical_data, count=count)

                # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                results = []
                for i, pred in enumerate(predictions):
                    if isinstance(pred, tuple) and len(pred) == 2:
                        front_balls, back_balls = pred
                        result = {
                            'front_balls': list(front_balls),
                            'back_balls': list(back_balls),
                            'method': 'gan',
                            'confidence': 0.82,
                            'model_type': 'generative_adversarial',
                            'algorithm': 'generator_discriminator'
                        }
                        results.append(result)
                    elif isinstance(pred, dict):
                        pred['method'] = 'gan'
                        pred['confidence'] = 0.82
                        pred['model_type'] = 'generative_adversarial'
                        pred['algorithm'] = 'generator_discriminator'
                        results.append(pred)

                logger_manager.info(f"GANé¢„æµ‹å®Œæˆï¼Œç”Ÿæˆ{len(results)}æ³¨é¢„æµ‹")
                return results

            except ImportError as e:
                logger_manager.warning(f"Enhanced GANæ¨¡å—ä¸å¯ç”¨: {e}")
                # å›é€€åˆ°ç®€åŒ–çš„GANå®ç°
                return self._fallback_gan_predict(count, periods)

        except Exception as e:
            logger_manager.error(f"GANé¢„æµ‹å¤±è´¥: {e}")
            # å›é€€åˆ°ç®€åŒ–å®ç°
            return self._fallback_gan_predict(count, periods)

    def _fallback_gan_predict(self, count: int = 3, periods: int = 500) -> List[Dict]:
        """GANé¢„æµ‹å›é€€å®ç° - åŸºäºç”Ÿæˆæ¨¡å‹çš„ç®€åŒ–å®ç°"""
        try:
            logger_manager.info("ä½¿ç”¨GANå›é€€å®ç°")

            # è·å–å†å²æ•°æ®
            historical_data = data_manager.get_data()
            if historical_data is None or len(historical_data) == 0:
                raise ValueError("æ— æ³•è·å–å†å²æ•°æ®")

            # ä½¿ç”¨æŒ‡å®šæœŸæ•°çš„æœ€æ–°æ•°æ®
            if len(historical_data) > periods:
                historical_data = historical_data.tail(periods)

            # ç®€åŒ–çš„ç”Ÿæˆæ¨¡å‹å®ç°
            front_patterns = []
            back_patterns = []

            for _, row in historical_data.iterrows():
                front_nums = [int(x) for x in str(row['front']).split(',') if x.strip().isdigit()]
                back_nums = [int(x) for x in str(row['back']).split(',') if x.strip().isdigit()]

                if len(front_nums) == 5 and len(back_nums) == 2:
                    front_patterns.append(front_nums)
                    back_patterns.append(back_nums)

            if len(front_patterns) < 10:
                raise ValueError("å†å²æ•°æ®ä¸è¶³")

            # ç®€åŒ–çš„ç”Ÿæˆå™¨å®ç°
            results = []
            for i in range(count):
                # åŸºäºå†å²æ¨¡å¼ç”Ÿæˆæ–°çš„å·ç ç»„åˆ
                front_pred = self._generate_with_patterns(front_patterns, 5, 1, 35)
                back_pred = self._generate_with_patterns(back_patterns, 2, 1, 12)

                result = {
                    'front_balls': sorted(front_pred),
                    'back_balls': sorted(back_pred),
                    'method': 'gan_fallback',
                    'confidence': 0.70,
                    'model_type': 'pattern_generator',
                    'algorithm': 'simplified_generation'
                }
                results.append(result)

            logger_manager.info(f"GANå›é€€é¢„æµ‹å®Œæˆï¼Œç”Ÿæˆ{len(results)}æ³¨é¢„æµ‹")
            return results

        except Exception as e:
            logger_manager.error(f"GANå›é€€é¢„æµ‹å¤±è´¥: {e}")
            return []

    def _generate_with_patterns(self, patterns: List[List[int]],
                               count: int, min_num: int, max_num: int) -> List[int]:
        """åŸºäºå†å²æ¨¡å¼ç”Ÿæˆå·ç """
        try:
            import numpy as np

            if not patterns:
                # å¦‚æœæ²¡æœ‰æ¨¡å¼ï¼Œéšæœºç”Ÿæˆ
                return list(np.random.choice(range(min_num, max_num + 1), count, replace=False))

            # åˆ†ææ¨¡å¼ç‰¹å¾
            pattern_features = self._analyze_patterns(patterns)

            # åŸºäºæ¨¡å¼ç‰¹å¾ç”Ÿæˆæ–°å·ç 
            generated = []
            attempts = 0
            max_attempts = count * 10

            while len(generated) < count and attempts < max_attempts:
                attempts += 1

                # éšæœºé€‰æ‹©ä¸€ä¸ªå†å²æ¨¡å¼ä½œä¸ºåŸºç¡€
                base_pattern = patterns[np.random.randint(0, len(patterns))]

                # åŸºäºæ¨¡å¼ç‰¹å¾è¿›è¡Œå˜å¼‚
                candidate = self._mutate_pattern(base_pattern, pattern_features, min_num, max_num)

                # ç¡®ä¿å€™é€‰å·ç åœ¨æœ‰æ•ˆèŒƒå›´å†…ä¸”ä¸é‡å¤
                if min_num <= candidate <= max_num and candidate not in generated:
                    generated.append(candidate)

            # å¦‚æœç”Ÿæˆçš„å·ç ä¸è¶³ï¼Œéšæœºè¡¥å……
            while len(generated) < count:
                candidate = np.random.randint(min_num, max_num + 1)
                if candidate not in generated:
                    generated.append(candidate)

            return generated[:count]

        except Exception as e:
            logger_manager.error(f"åŸºäºæ¨¡å¼ç”Ÿæˆå·ç å¤±è´¥: {e}")
            import random
            return random.sample(range(min_num, max_num + 1), count)

    def _analyze_patterns(self, patterns: List[List[int]]) -> Dict[str, Any]:
        """åˆ†æå†å²æ¨¡å¼ç‰¹å¾"""
        try:
            import numpy as np

            features = {
                'mean_values': [],
                'std_values': [],
                'gaps': [],
                'ranges': []
            }

            for pattern in patterns:
                sorted_pattern = sorted(pattern)
                features['mean_values'].append(np.mean(sorted_pattern))
                features['std_values'].append(np.std(sorted_pattern))
                features['ranges'].append(max(sorted_pattern) - min(sorted_pattern))

                # è®¡ç®—é—´éš”
                gaps = [sorted_pattern[i+1] - sorted_pattern[i] for i in range(len(sorted_pattern)-1)]
                features['gaps'].extend(gaps)

            # è®¡ç®—ç»Ÿè®¡ç‰¹å¾
            analysis = {
                'avg_mean': np.mean(features['mean_values']),
                'avg_std': np.mean(features['std_values']),
                'avg_range': np.mean(features['ranges']),
                'common_gaps': np.bincount(features['gaps']).argmax() if features['gaps'] else 1
            }

            return analysis

        except Exception as e:
            logger_manager.error(f"åˆ†ææ¨¡å¼ç‰¹å¾å¤±è´¥: {e}")
            return {}

    def _mutate_pattern(self, base_pattern: List[int], features: Dict[str, Any],
                       min_num: int, max_num: int) -> int:
        """åŸºäºæ¨¡å¼ç‰¹å¾å˜å¼‚ç”Ÿæˆæ–°å·ç """
        try:
            import numpy as np

            if not features:
                return np.random.randint(min_num, max_num + 1)

            # åŸºäºç»Ÿè®¡ç‰¹å¾ç”Ÿæˆå€™é€‰å·ç 
            avg_mean = features.get('avg_mean', (min_num + max_num) / 2)
            avg_std = features.get('avg_std', 5)

            # åœ¨å¹³å‡å€¼é™„è¿‘ç”Ÿæˆå€™é€‰
            candidate = int(np.random.normal(avg_mean, avg_std))

            # ç¡®ä¿åœ¨æœ‰æ•ˆèŒƒå›´å†…
            candidate = max(min_num, min(max_num, candidate))

            return candidate

        except Exception as e:
            logger_manager.error(f"æ¨¡å¼å˜å¼‚å¤±è´¥: {e}")
            import random
            return random.randint(min_num, max_num)
    
    def ultimate_ensemble_predict(self, count: int = 3) -> List[Dict]:
        """ç»ˆæé›†æˆé¢„æµ‹
        
        ç»“åˆæ‰€æœ‰é¢„æµ‹æ–¹æ³•çš„æœ€ä½³ç»“æœ
        
        Args:
            count: é¢„æµ‹æ³¨æ•°
            
        Returns:
            List[Dict]: é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        # è·å–å„ç§é›†æˆæ–¹æ³•çš„é¢„æµ‹ç»“æœ
        stacking_results = self.stacking_predict(count)
        weighted_results = self.weighted_ensemble_predict(count)
        adaptive_results = self.adaptive_ensemble_predict(count)
        
        # èåˆç»“æœ
        results = []
        for i in range(count):
            # æ”¶é›†æ‰€æœ‰é¢„æµ‹ç»“æœ
            all_front_balls = []
            all_back_balls = []
            
            if i < len(stacking_results):
                all_front_balls.extend(stacking_results[i]['front_balls'])
                all_back_balls.extend(stacking_results[i]['back_balls'])
            
            if i < len(weighted_results):
                all_front_balls.extend(weighted_results[i]['front_balls'])
                all_back_balls.extend(weighted_results[i]['back_balls'])
            
            if i < len(adaptive_results):
                all_front_balls.extend(adaptive_results[i]['front_balls'])
                all_back_balls.extend(adaptive_results[i]['back_balls'])
            
            # ç»Ÿè®¡å·ç å‡ºç°é¢‘ç‡
            front_counter = Counter(all_front_balls)
            back_counter = Counter(all_back_balls)
            
            # é€‰æ‹©å‡ºç°é¢‘ç‡æœ€é«˜çš„å·ç 
            front_balls = [ball for ball, _ in front_counter.most_common(5)]
            back_balls = [ball for ball, _ in back_counter.most_common(2)]
            
            # å¦‚æœå·ç ä¸è¶³ï¼Œä½¿ç”¨é¢‘ç‡åˆ†æè¡¥å……
            if len(front_balls) < 5:
                from analyzer_modules import basic_analyzer
                freq_analysis = basic_analyzer.frequency_analysis()
                front_freq = freq_analysis.get('front_frequency', {})
                sorted_freq = sorted(front_freq.items(), key=lambda x: x[1], reverse=True)

                for ball, _ in sorted_freq:
                    if len(front_balls) >= 5:
                        break
                    ball_int = int(ball) if isinstance(ball, str) else ball
                    if ball_int not in front_balls:
                        front_balls.append(ball_int)

            if len(back_balls) < 2:
                from analyzer_modules import basic_analyzer
                freq_analysis = basic_analyzer.frequency_analysis()
                back_freq = freq_analysis.get('back_frequency', {})
                sorted_freq = sorted(back_freq.items(), key=lambda x: x[1], reverse=True)

                for ball, _ in sorted_freq:
                    if len(back_balls) >= 2:
                        break
                    ball_int = int(ball) if isinstance(ball, str) else ball
                    if ball_int not in back_balls:
                        back_balls.append(ball_int)
            
            # æ„å»ºç»“æœ
            result = {
                'front_balls': sorted(front_balls),
                'back_balls': sorted(back_balls),
                'method': 'ultimate_ensemble',
                'confidence': 0.98,
                'ensemble_methods': ['stacking', 'weighted', 'adaptive']
            }
            
            results.append(result)
        
        return results


# å…¨å±€å®ä¾‹
_integrator = None

def get_integrator() -> IntegratedPredictor:
    """è·å–é›†æˆé¢„æµ‹å™¨å®ä¾‹"""
    global _integrator
    if _integrator is None:
        _integrator = IntegratedPredictor()
    return _integrator


if __name__ == "__main__":
    # æµ‹è¯•é›†æˆæ¨¡å—
    print("ğŸ”„ æµ‹è¯•é›†æˆæ¨¡å—...")
    
    # è·å–é›†æˆé¢„æµ‹å™¨
    integrator = get_integrator()
    
    # æµ‹è¯•Stackingé›†æˆé¢„æµ‹
    print("\nğŸ¯ Stackingé›†æˆé¢„æµ‹...")
    stacking_results = integrator.stacking_predict(3)
    for i, result in enumerate(stacking_results):
        front_str = ' '.join([str(b).zfill(2) for b in result['front_balls']])
        back_str = ' '.join([str(b).zfill(2) for b in result['back_balls']])
        print(f"  ç¬¬ {i+1} æ³¨: {front_str} + {back_str}")
        print(f"  ä½¿ç”¨é¢„æµ‹å™¨: {len(result['predictors_used'])} ä¸ª")
    
    # æµ‹è¯•åŠ æƒé›†æˆé¢„æµ‹
    print("\nğŸ¯ åŠ æƒé›†æˆé¢„æµ‹...")
    weighted_results = integrator.weighted_ensemble_predict(3)
    for i, result in enumerate(weighted_results):
        front_str = ' '.join([str(b).zfill(2) for b in result['front_balls']])
        back_str = ' '.join([str(b).zfill(2) for b in result['back_balls']])
        print(f"  ç¬¬ {i+1} æ³¨: {front_str} + {back_str}")
        print(f"  ä½¿ç”¨é¢„æµ‹å™¨: {len(result['predictors_used'])} ä¸ª")
    
    # æµ‹è¯•è‡ªé€‚åº”é›†æˆé¢„æµ‹
    print("\nğŸ¯ è‡ªé€‚åº”é›†æˆé¢„æµ‹...")
    adaptive_results = integrator.adaptive_ensemble_predict(3)
    for i, result in enumerate(adaptive_results):
        front_str = ' '.join([str(b).zfill(2) for b in result['front_balls']])
        back_str = ' '.join([str(b).zfill(2) for b in result['back_balls']])
        print(f"  ç¬¬ {i+1} æ³¨: {front_str} + {back_str}")
        print(f"  ä½¿ç”¨é¢„æµ‹å™¨: {len(result['predictors_used'])} ä¸ª")
    
    # æµ‹è¯•ç»ˆæé›†æˆé¢„æµ‹
    print("\nğŸ¯ ç»ˆæé›†æˆé¢„æµ‹...")
    ultimate_results = integrator.ultimate_ensemble_predict(3)
    for i, result in enumerate(ultimate_results):
        front_str = ' '.join([str(b).zfill(2) for b in result['front_balls']])
        back_str = ' '.join([str(b).zfill(2) for b in result['back_balls']])
        print(f"  ç¬¬ {i+1} æ³¨: {front_str} + {back_str}")
        print(f"  é›†æˆæ–¹æ³•: {result['ensemble_methods']}")
    
    print("\nâœ… æµ‹è¯•å®Œæˆ")