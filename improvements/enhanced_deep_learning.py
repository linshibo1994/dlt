#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¢å¼ºæ·±åº¦å­¦ä¹ æ¨¡å‹
åŸºäºTransformerå’ŒGANçš„æ·±åº¦å­¦ä¹ é¢„æµ‹æ¨¡å‹
"""

import os
import numpy as np
import pandas as pd
from typing import List, Tuple, Dict, Any
from datetime import datetime

# æ£€æŸ¥TensorFlowå¯ç”¨æ€§
try:
    import tensorflow as tf
    from tensorflow.keras import layers, Model, optimizers
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    from tensorflow.keras.models import load_model, save_model
    from sklearn.preprocessing import MinMaxScaler
    TENSORFLOW_AVAILABLE = True
except ImportError:
    TENSORFLOW_AVAILABLE = False

# å°è¯•å¯¼å…¥æ ¸å¿ƒæ¨¡å—
try:
    from core_modules import logger_manager, data_manager, cache_manager
except ImportError:
    # å¦‚æœåœ¨ä¸åŒç›®å½•è¿è¡Œï¼Œæ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
    import sys
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from core_modules import logger_manager, data_manager, cache_manager


class TransformerLotteryPredictor:
    """åŸºäºTransformerçš„å½©ç¥¨é¢„æµ‹æ¨¡å‹"""
    
    def __init__(self, sequence_length=20, d_model=128, num_heads=8, num_layers=4):
        if not TENSORFLOW_AVAILABLE:
            raise ImportError("TensorFlowæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨Transformeré¢„æµ‹å™¨")
        
        self.sequence_length = sequence_length
        self.d_model = d_model
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.feature_dim = 21  # ä¸LSTMé¢„æµ‹å™¨ä¿æŒä¸€è‡´
        
        # æ¨¡å‹ç»„ä»¶
        self.scaler = MinMaxScaler()
        self.model = None
        self.is_trained = False
        
        # è·å–æ•°æ®
        self.df = data_manager.get_data()
        if self.df is None:
            logger_manager.error("æ•°æ®æœªåŠ è½½")
    
    def _extract_features(self, df_subset) -> np.ndarray:
        """æå–æ·±åº¦ç‰¹å¾ï¼ˆä¸LSTMé¢„æµ‹å™¨ä¿æŒä¸€è‡´ï¼‰"""
        features = []
        
        for _, row in df_subset.iterrows():
            front_balls, back_balls = data_manager.parse_balls(row)
            
            # åŸºç¡€ç‰¹å¾ (7ç»´)
            feature_vector = front_balls + back_balls
            
            # ç»Ÿè®¡ç‰¹å¾ (8ç»´)
            front_sum = sum(front_balls)
            back_sum = sum(back_balls)
            total_sum = front_sum + back_sum
            front_mean = np.mean(front_balls)
            back_mean = np.mean(back_balls)
            front_std = np.std(front_balls)
            back_std = np.std(back_balls)
            span = max(front_balls) - min(front_balls)
            
            # æ¨¡å¼ç‰¹å¾ (6ç»´)
            odd_count = sum(1 for x in front_balls if x % 2 == 1)
            even_count = 5 - odd_count
            big_count = sum(1 for x in front_balls if x > 17)  # å¤§å·(18-35)
            small_count = 5 - big_count  # å°å·(1-17)
            consecutive_count = self._count_consecutive(front_balls)
            prime_count = sum(1 for x in front_balls if self._is_prime(x))
            
            # ç»„åˆæ‰€æœ‰ç‰¹å¾
            all_features = (
                feature_vector +  # 7ç»´
                [front_sum, back_sum, total_sum, front_mean, back_mean, front_std, back_std, span] +  # 8ç»´
                [odd_count, even_count, big_count, small_count, consecutive_count, prime_count]  # 6ç»´
            )
            
            features.append(all_features)
        
        return np.array(features)
    
    def _count_consecutive(self, numbers):
        """è®¡ç®—è¿ç»­å·ç æ•°é‡"""
        sorted_nums = sorted(numbers)
        consecutive = 0
        for i in range(len(sorted_nums) - 1):
            if sorted_nums[i+1] - sorted_nums[i] == 1:
                consecutive += 1
        return consecutive
    
    def _is_prime(self, n):
        """åˆ¤æ–­æ˜¯å¦ä¸ºè´¨æ•°"""
        if n < 2:
            return False
        for i in range(2, int(n**0.5) + 1):
            if n % i == 0:
                return False
        return True
    
    def _prepare_sequences(self, features):
        """å‡†å¤‡æ—¶é—´åºåˆ—æ•°æ®"""
        X, y = [], []
        
        for i in range(self.sequence_length, len(features)):
            X.append(features[i-self.sequence_length:i])
            y.append(features[i][:7])  # åªé¢„æµ‹å‰5ä¸ªå‰åŒºå·ç å’Œ2ä¸ªååŒºå·ç 
        
        return np.array(X), np.array(y)
    
    def _build_model(self):
        """æ„å»ºTransformeræ¨¡å‹"""
        # è¾“å…¥å±‚
        inputs = layers.Input(shape=(self.sequence_length, self.feature_dim))
        
        # ä½ç½®ç¼–ç 
        positions = tf.range(start=0, limit=self.sequence_length, delta=1)
        positions = layers.Embedding(self.sequence_length, self.d_model)(positions)
        
        # è¾“å…¥åµŒå…¥
        x = layers.Dense(self.d_model)(inputs)
        x = x + positions
        
        # Transformerå±‚
        for _ in range(self.num_layers):
            # å¤šå¤´æ³¨æ„åŠ›
            attention_output = layers.MultiHeadAttention(
                num_heads=self.num_heads, 
                key_dim=self.d_model
            )(x, x)
            x = layers.Add()([x, attention_output])
            x = layers.LayerNormalization()(x)
            
            # å‰é¦ˆç½‘ç»œ
            ffn_output = layers.Dense(self.d_model * 4, activation='relu')(x)
            ffn_output = layers.Dense(self.d_model)(ffn_output)
            x = layers.Add()([x, ffn_output])
            x = layers.LayerNormalization()(x)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        x = layers.GlobalAveragePooling1D()(x)
        
        # å…¨è¿æ¥å±‚
        x = layers.Dense(64, activation='relu')(x)
        x = layers.Dropout(0.2)(x)
        
        # è¾“å‡ºå±‚ - ä½¿ç”¨å•ä¸€è¾“å‡ºï¼Œå‰5ä¸ªå€¼ä¸ºå‰åŒºï¼Œå2ä¸ªå€¼ä¸ºååŒº
        outputs = layers.Dense(7, activation='sigmoid')(x)
        
        # æ„å»ºæ¨¡å‹
        model = Model(inputs=inputs, outputs=outputs)
        
        # ç¼–è¯‘æ¨¡å‹
        model.compile(
            optimizer=optimizers.Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae']
        )
        
        return model
    
    def train_model(self, epochs=100, validation_split=0.2, batch_size=32):
        """è®­ç»ƒTransformeræ¨¡å‹"""
        logger_manager.info("å¼€å§‹è®­ç»ƒTransformeræ¨¡å‹...")
        
        # æå–ç‰¹å¾
        features = self._extract_features(self.df)
        
        # æ•°æ®æ ‡å‡†åŒ–
        features_scaled = self.scaler.fit_transform(features)
        
        # å‡†å¤‡åºåˆ—æ•°æ®
        X, y = self._prepare_sequences(features_scaled)
        
        if len(X) == 0:
            logger_manager.error("åºåˆ—æ•°æ®ä¸è¶³ï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹")
            return False
        
        # æ„å»ºæ¨¡å‹
        self.model = self._build_model()
        
        # è®¾ç½®å›è°ƒ
        callbacks = [
            EarlyStopping(patience=10, restore_best_weights=True),
            ReduceLROnPlateau(factor=0.5, patience=5, min_lr=0.0001)
        ]
        
        # è®­ç»ƒæ¨¡å‹
        history = self.model.fit(
            X, y,
            epochs=epochs,
            validation_split=validation_split,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        self.is_trained = True
        logger_manager.info("Transformeræ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        # ä¿å­˜æ¨¡å‹
        self._save_model()
        
        return True
    
    def _save_model(self):
        """ä¿å­˜æ¨¡å‹"""
        try:
            # åˆ›å»ºæ¨¡å‹ç›®å½•
            model_dir = os.path.join("cache", "models")
            os.makedirs(model_dir, exist_ok=True)
            
            # ä¿å­˜æ¨¡å‹
            model_path = os.path.join(model_dir, "transformer_model.h5")
            self.model.save(model_path)
            
            # ä¿å­˜ç¼©æ”¾å™¨
            scaler_path = os.path.join(model_dir, "transformer_scaler.npy")
            np.save(scaler_path, {
                'scale_': self.scaler.scale_,
                'min_': self.scaler.min_,
                'data_min_': self.scaler.data_min_,
                'data_max_': self.scaler.data_max_,
                'data_range_': self.scaler.data_range_
            })
            
            logger_manager.info(f"Transformeræ¨¡å‹å·²ä¿å­˜åˆ° {model_path}")
        except Exception as e:
            logger_manager.error(f"ä¿å­˜Transformeræ¨¡å‹å¤±è´¥: {e}")
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹"""
        try:
            # æ¨¡å‹è·¯å¾„
            model_path = os.path.join("cache", "models", "transformer_model.h5")
            scaler_path = os.path.join("cache", "models", "transformer_scaler.npy")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(model_path) or not os.path.exists(scaler_path):
                logger_manager.warning("Transformeræ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦é‡æ–°è®­ç»ƒ")
                return False
            
            # åŠ è½½æ¨¡å‹
            self.model = load_model(model_path)
            
            # åŠ è½½ç¼©æ”¾å™¨
            scaler_data = np.load(scaler_path, allow_pickle=True).item()
            self.scaler.scale_ = scaler_data['scale_']
            self.scaler.min_ = scaler_data['min_']
            self.scaler.data_min_ = scaler_data['data_min_']
            self.scaler.data_max_ = scaler_data['data_max_']
            self.scaler.data_range_ = scaler_data['data_range_']
            
            self.is_trained = True
            logger_manager.info("Transformeræ¨¡å‹åŠ è½½æˆåŠŸ")
            return True
        except Exception as e:
            logger_manager.error(f"åŠ è½½Transformeræ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def predict(self, count=1) -> List[Tuple[List[int], List[int]]]:
        """ä½¿ç”¨Transformerè¿›è¡Œé¢„æµ‹"""
        # å°è¯•åŠ è½½å·²æœ‰æ¨¡å‹
        if not self.is_trained:
            if not self._load_model():
                logger_manager.info("æ¨¡å‹æœªè®­ç»ƒï¼Œå¼€å§‹è®­ç»ƒ...")
                if not self.train_model():
                    logger_manager.error("æ¨¡å‹è®­ç»ƒå¤±è´¥")
                    return []
        
        # è·å–æœ€è¿‘çš„åºåˆ—æ•°æ®
        recent_features = self._extract_features(self.df.head(self.sequence_length))
        recent_scaled = self.scaler.transform(recent_features)
        
        # å‡†å¤‡è¾“å…¥åºåˆ—
        input_sequence = recent_scaled.reshape(1, self.sequence_length, self.feature_dim)
        
        predictions = []
        
        for _ in range(count):
            # é¢„æµ‹
            pred_scaled = self.model.predict(input_sequence, verbose=0)
            
            # åæ ‡å‡†åŒ–
            # åˆ›å»ºå®Œæ•´ç‰¹å¾å‘é‡ç”¨äºåæ ‡å‡†åŒ–
            full_pred = np.zeros((1, self.feature_dim))
            full_pred[0, :7] = pred_scaled[0]
            pred_original = self.scaler.inverse_transform(full_pred)[0, :7]
            
            # è½¬æ¢ä¸ºå½©ç¥¨å·ç 
            front_balls = [max(1, min(35, int(round(x)))) for x in pred_original[:5]]
            back_balls = [max(1, min(12, int(round(x)))) for x in pred_original[5:7]]
            
            # ç¡®ä¿å·ç å”¯ä¸€æ€§
            front_balls = self._ensure_unique_numbers(front_balls, 1, 35, 5)
            back_balls = self._ensure_unique_numbers(back_balls, 1, 12, 2)
            
            predictions.append((sorted(front_balls), sorted(back_balls)))
            
            # æ›´æ–°è¾“å…¥åºåˆ—ç”¨äºä¸‹ä¸€æ¬¡é¢„æµ‹
            new_feature = np.concatenate([pred_original, recent_scaled[-1, 7:]])
            input_sequence = np.roll(input_sequence, -1, axis=1)
            input_sequence[0, -1] = new_feature
        
        return predictions
    
    def _ensure_unique_numbers(self, numbers, min_val, max_val, target_count):
        """ç¡®ä¿å·ç å”¯ä¸€æ€§"""
        unique_numbers = list(set(numbers))
        
        # å¦‚æœæ•°é‡ä¸è¶³ï¼Œéšæœºè¡¥å……
        while len(unique_numbers) < target_count:
            candidate = np.random.randint(min_val, max_val + 1)
            if candidate not in unique_numbers:
                unique_numbers.append(candidate)
        
        return unique_numbers[:target_count]


class GAN_LotteryPredictor:
    """åŸºäºGANçš„å½©ç¥¨å·ç ç”Ÿæˆå™¨"""
    
    def __init__(self, latent_dim=100):
        if not TENSORFLOW_AVAILABLE:
            raise ImportError("TensorFlowæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨GANé¢„æµ‹å™¨")
        
        self.latent_dim = latent_dim
        self.generator = None
        self.discriminator = None
        self.gan = None
        self.is_trained = False
        
        # è·å–æ•°æ®
        self.df = data_manager.get_data()
        if self.df is None:
            logger_manager.error("æ•°æ®æœªåŠ è½½")
    
    def _build_generator(self):
        """æ„å»ºç”Ÿæˆå™¨"""
        model = tf.keras.Sequential([
            layers.Dense(128, activation='relu', input_dim=self.latent_dim),
            layers.BatchNormalization(),
            layers.Dense(256, activation='relu'),
            layers.BatchNormalization(),
            layers.Dense(512, activation='relu'),
            layers.BatchNormalization(),
            layers.Dense(7, activation='sigmoid')  # 5å‰åŒº+2ååŒº
        ])
        return model
    
    def _build_discriminator(self):
        """æ„å»ºåˆ¤åˆ«å™¨"""
        model = tf.keras.Sequential([
            layers.Dense(512, activation='relu', input_dim=7),
            layers.Dropout(0.3),
            layers.Dense(256, activation='relu'),
            layers.Dropout(0.3),
            layers.Dense(128, activation='relu'),
            layers.Dropout(0.3),
            layers.Dense(1, activation='sigmoid')
        ])
        
        model.compile(
            optimizer=optimizers.Adam(learning_rate=0.0002, beta_1=0.5),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def _build_gan(self):
        """æ„å»ºGAN"""
        self.discriminator.trainable = False
        
        gan_input = layers.Input(shape=(self.latent_dim,))
        generated = self.generator(gan_input)
        validity = self.discriminator(generated)
        
        model = Model(gan_input, validity)
        model.compile(
            optimizer=optimizers.Adam(learning_rate=0.0002, beta_1=0.5),
            loss='binary_crossentropy'
        )
        
        return model
    
    def _prepare_training_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        # æå–å·ç æ•°æ®
        real_samples = []
        
        for _, row in self.df.iterrows():
            front_balls, back_balls = data_manager.parse_balls(row)
            
            # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
            normalized_front = [(x - 1) / 34 for x in front_balls]  # 1-35 -> 0-1
            normalized_back = [(x - 1) / 11 for x in back_balls]    # 1-12 -> 0-1
            
            real_samples.append(normalized_front + normalized_back)
        
        return np.array(real_samples)
    
    def train_model(self, epochs=2000, batch_size=32, sample_interval=100):
        """è®­ç»ƒGANæ¨¡å‹"""
        logger_manager.info("å¼€å§‹è®­ç»ƒGANæ¨¡å‹...")
        
        # æ„å»ºæ¨¡å‹
        self.generator = self._build_generator()
        self.discriminator = self._build_discriminator()
        self.gan = self._build_gan()
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        real_samples = self._prepare_training_data()
        
        if len(real_samples) == 0:
            logger_manager.error("è®­ç»ƒæ•°æ®ä¸è¶³ï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹")
            return False
        
        # è®­ç»ƒGAN
        valid = np.ones((batch_size, 1))
        fake = np.zeros((batch_size, 1))
        
        for epoch in range(epochs):
            # è®­ç»ƒåˆ¤åˆ«å™¨
            idx = np.random.randint(0, real_samples.shape[0], batch_size)
            real_batch = real_samples[idx]
            
            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))
            gen_samples = self.generator.predict(noise, verbose=0)
            
            d_loss_real = self.discriminator.train_on_batch(real_batch, valid)
            d_loss_fake = self.discriminator.train_on_batch(gen_samples, fake)
            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
            
            # è®­ç»ƒç”Ÿæˆå™¨
            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))
            g_loss = self.gan.train_on_batch(noise, valid)
            
            # æ‰“å°è¿›åº¦
            if epoch % sample_interval == 0:
                logger_manager.info(f"Epoch {epoch}/{epochs} [D loss: {d_loss[0]:.4f}, acc.: {100*d_loss[1]:.2f}%] [G loss: {g_loss:.4f}]")
        
        self.is_trained = True
        logger_manager.info("GANæ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        # ä¿å­˜æ¨¡å‹
        self._save_model()
        
        return True
    
    def _save_model(self):
        """ä¿å­˜æ¨¡å‹"""
        try:
            # åˆ›å»ºæ¨¡å‹ç›®å½•
            model_dir = os.path.join("cache", "models")
            os.makedirs(model_dir, exist_ok=True)
            
            # ä¿å­˜ç”Ÿæˆå™¨
            generator_path = os.path.join(model_dir, "gan_generator.h5")
            self.generator.save(generator_path)
            
            # ä¿å­˜åˆ¤åˆ«å™¨
            discriminator_path = os.path.join(model_dir, "gan_discriminator.h5")
            self.discriminator.save(discriminator_path)
            
            logger_manager.info(f"GANæ¨¡å‹å·²ä¿å­˜åˆ° {model_dir}")
        except Exception as e:
            logger_manager.error(f"ä¿å­˜GANæ¨¡å‹å¤±è´¥: {e}")
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹"""
        try:
            # æ¨¡å‹è·¯å¾„
            generator_path = os.path.join("cache", "models", "gan_generator.h5")
            discriminator_path = os.path.join("cache", "models", "gan_discriminator.h5")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(generator_path) or not os.path.exists(discriminator_path):
                logger_manager.warning("GANæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦é‡æ–°è®­ç»ƒ")
                return False
            
            # åŠ è½½æ¨¡å‹
            self.generator = load_model(generator_path)
            self.discriminator = load_model(discriminator_path)
            
            # é‡å»ºGAN
            self.gan = self._build_gan()
            
            self.is_trained = True
            logger_manager.info("GANæ¨¡å‹åŠ è½½æˆåŠŸ")
            return True
        except Exception as e:
            logger_manager.error(f"åŠ è½½GANæ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def predict(self, count=1) -> List[Tuple[List[int], List[int]]]:
        """ä½¿ç”¨GANç”Ÿæˆé¢„æµ‹å·ç """
        # å°è¯•åŠ è½½å·²æœ‰æ¨¡å‹
        if not self.is_trained:
            if not self._load_model():
                logger_manager.info("æ¨¡å‹æœªè®­ç»ƒï¼Œå¼€å§‹è®­ç»ƒ...")
                if not self.train_model(epochs=1000):  # å‡å°‘è®­ç»ƒè½®æ•°ä»¥åŠ å¿«é€Ÿåº¦
                    logger_manager.error("æ¨¡å‹è®­ç»ƒå¤±è´¥")
                    return []
        
        predictions = []
        
        # ç”Ÿæˆå¤šç»„é¢„æµ‹
        for _ in range(count):
            # ç”Ÿæˆéšæœºå™ªå£°
            noise = np.random.normal(0, 1, (1, self.latent_dim))
            
            # ç”Ÿæˆå·ç 
            generated = self.generator.predict(noise, verbose=0)[0]
            
            # è½¬æ¢ä¸ºå®é™…å·ç 
            front_balls = [int(round(x * 34 + 1)) for x in generated[:5]]  # 0-1 -> 1-35
            back_balls = [int(round(x * 11 + 1)) for x in generated[5:7]]  # 0-1 -> 1-12
            
            # ç¡®ä¿å·ç åœ¨æœ‰æ•ˆèŒƒå›´å†…
            front_balls = [max(1, min(35, x)) for x in front_balls]
            back_balls = [max(1, min(12, x)) for x in back_balls]
            
            # ç¡®ä¿å·ç å”¯ä¸€æ€§
            front_balls = self._ensure_unique_numbers(front_balls, 1, 35, 5)
            back_balls = self._ensure_unique_numbers(back_balls, 1, 12, 2)
            
            predictions.append((sorted(front_balls), sorted(back_balls)))
        
        return predictions
    
    def _ensure_unique_numbers(self, numbers, min_val, max_val, target_count):
        """ç¡®ä¿å·ç å”¯ä¸€æ€§"""
        unique_numbers = list(set(numbers))
        
        # å¦‚æœæ•°é‡ä¸è¶³ï¼Œéšæœºè¡¥å……
        while len(unique_numbers) < target_count:
            candidate = np.random.randint(min_val, max_val + 1)
            if candidate not in unique_numbers:
                unique_numbers.append(candidate)
        
        return unique_numbers[:target_count]


# å…¼å®¹æ€§æ£€æŸ¥
if not TENSORFLOW_AVAILABLE:
    class TransformerLotteryPredictor:
        def __init__(self, *args, **kwargs):
            raise ImportError("TensorFlowæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨Transformeré¢„æµ‹å™¨")
        
        def predict(self, *args, **kwargs):
            return []
    
    class GAN_LotteryPredictor:
        def __init__(self, *args, **kwargs):
            raise ImportError("TensorFlowæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨GANé¢„æµ‹å™¨")
        
        def predict(self, *args, **kwargs):
            return []


if __name__ == "__main__":
    # æµ‹è¯•Transformeré¢„æµ‹å™¨
    if TENSORFLOW_AVAILABLE:
        print("ğŸ§  æµ‹è¯•Transformeré¢„æµ‹å™¨...")
        transformer = TransformerLotteryPredictor()
        
        # è®­ç»ƒæ¨¡å‹
        transformer.train_model(epochs=50)
        
        # è¿›è¡Œé¢„æµ‹
        predictions = transformer.predict(3)
        
        print("Transformeré¢„æµ‹ç»“æœ:")
        for i, (front, back) in enumerate(predictions):
            front_str = ' '.join([str(b).zfill(2) for b in front])
            back_str = ' '.join([str(b).zfill(2) for b in back])
            print(f"ç¬¬ {i+1} æ³¨: {front_str} + {back_str}")
        
        # æµ‹è¯•GANé¢„æµ‹å™¨
        print("\nğŸ® æµ‹è¯•GANé¢„æµ‹å™¨...")
        gan = GAN_LotteryPredictor()
        
        # è®­ç»ƒæ¨¡å‹
        gan.train_model(epochs=500, sample_interval=100)
        
        # è¿›è¡Œé¢„æµ‹
        predictions = gan.predict(3)
        
        print("GANé¢„æµ‹ç»“æœ:")
        for i, (front, back) in enumerate(predictions):
            front_str = ' '.join([str(b).zfill(2) for b in front])
            back_str = ' '.join([str(b).zfill(2) for b in back])
            print(f"ç¬¬ {i+1} æ³¨: {front_str} + {back_str}")
    else:
        print("âŒ TensorFlowæœªå®‰è£…ï¼Œæ— æ³•æµ‹è¯•æ·±åº¦å­¦ä¹ æ¨¡å‹")