#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ¨¡å‹è¯„ä¼°æ¡†æ¶
æä¾›ç»Ÿä¸€çš„æ¨¡å‹è¯„ä¼°ã€æ¯”è¾ƒå’Œä¼˜åŒ–åŠŸèƒ½
"""

import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Tuple, Any, Callable, Union
from datetime import datetime
from collections import defaultdict
import json
import time
from tqdm import tqdm

# å°è¯•å¯¼å…¥æ ¸å¿ƒæ¨¡å—
try:
    from core_modules import logger_manager, data_manager, cache_manager
except ImportError:
    # å¦‚æœåœ¨ä¸åŒç›®å½•è¿è¡Œï¼Œæ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from core_modules import logger_manager, data_manager, cache_manager

# å°è¯•å¯¼å…¥æ¨¡å‹åŸºå‡†æµ‹è¯•æ¡†æ¶
try:
    from improvements.model_benchmark import ModelBenchmark, get_model_benchmark
    BENCHMARK_AVAILABLE = True
except ImportError:
    logger_manager.error("æ¨¡å‹åŸºå‡†æµ‹è¯•æ¡†æ¶æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿improvements/model_benchmark.pyæ–‡ä»¶å­˜åœ¨")
    BENCHMARK_AVAILABLE = False
    ModelBenchmark = None
    get_model_benchmark = None

# å°è¯•å¯¼å…¥æ¨¡å‹ä¼˜åŒ–å™¨
try:
    from improvements.model_optimizer import ModelOptimizer
    OPTIMIZER_AVAILABLE = True
except ImportError:
    logger_manager.error("æ¨¡å‹ä¼˜åŒ–å™¨æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿improvements/model_optimizer.pyæ–‡ä»¶å­˜åœ¨")
    OPTIMIZER_AVAILABLE = False
    ModelOptimizer = None


class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°æ¡†æ¶"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¨¡å‹è¯„ä¼°æ¡†æ¶"""
        self.df = data_manager.get_data()
        if self.df is None:
            logger_manager.error("æ•°æ®æœªåŠ è½½")
        
        # è·å–åŸºå‡†æµ‹è¯•å®ä¾‹
        if BENCHMARK_AVAILABLE:
            self.benchmark = get_model_benchmark()
        else:
            self.benchmark = None
            logger_manager.error("æ¨¡å‹åŸºå‡†æµ‹è¯•æ¡†æ¶æœªæ‰¾åˆ°ï¼ŒåŸºå‡†æµ‹è¯•åŠŸèƒ½å°†ä¸å¯ç”¨")
        
        # ä¼˜åŒ–å™¨å­—å…¸
        self.optimizers = {}
        
        # è¯„ä¼°ç»“æœ
        self.evaluation_results = {}
    
    def register_model(self, model_name: str, predict_func: Callable, 
                      description: str = "", category: str = "traditional"):
        """æ³¨å†Œæ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            predict_func: é¢„æµ‹å‡½æ•°ï¼Œæ¥å—æœŸæ•°å‚æ•°ï¼Œè¿”å›é¢„æµ‹ç»“æœ
            description: æ¨¡å‹æè¿°
            category: æ¨¡å‹ç±»åˆ«
        """
        if self.benchmark is None:
            logger_manager.error("æ¨¡å‹åŸºå‡†æµ‹è¯•æ¡†æ¶æœªæ‰¾åˆ°ï¼Œæ— æ³•æ³¨å†Œæ¨¡å‹")
            return False
        
        self.benchmark.register_model(model_name, predict_func, description, category)
        return True
    
    def evaluate_model(self, model_name: str, test_periods: int = 50, 
                      bet_cost: float = 2.0, verbose: bool = True) -> Dict:
        """è¯„ä¼°å•ä¸ªæ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            test_periods: æµ‹è¯•æœŸæ•°
            bet_cost: æ¯æ³¨æŠ•æ³¨æˆæœ¬
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        Returns:
            Dict: è¯„ä¼°ç»“æœ
        """
        if self.benchmark is None:
            logger_manager.error("æ¨¡å‹åŸºå‡†æµ‹è¯•æ¡†æ¶æœªæ‰¾åˆ°ï¼Œæ— æ³•è¯„ä¼°æ¨¡å‹")
            return {}
        
        result = self.benchmark.evaluate_model(model_name, test_periods, bet_cost, verbose)
        self.evaluation_results[model_name] = result
        return result
    
    def evaluate_all_models(self, test_periods: int = 50, bet_cost: float = 2.0, verbose: bool = True):
        """è¯„ä¼°æ‰€æœ‰æ³¨å†Œçš„æ¨¡å‹
        
        Args:
            test_periods: æµ‹è¯•æœŸæ•°
            bet_cost: æ¯æ³¨æŠ•æ³¨æˆæœ¬
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        """
        if self.benchmark is None:
            logger_manager.error("æ¨¡å‹åŸºå‡†æµ‹è¯•æ¡†æ¶æœªæ‰¾åˆ°ï¼Œæ— æ³•è¯„ä¼°æ¨¡å‹")
            return
        
        self.benchmark.evaluate_all_models(test_periods, bet_cost, verbose)
        
        # æ›´æ–°è¯„ä¼°ç»“æœ
        for model_name, model_info in self.benchmark.results.items():
            if model_info.get('evaluated', False):
                self.evaluation_results[model_name] = model_info
    
    def compare_models(self, metrics: List[str] = None, categories: List[str] = None, 
                      verbose: bool = True) -> Dict:
        """æ¯”è¾ƒæ¨¡å‹æ€§èƒ½
        
        Args:
            metrics: è¦æ¯”è¾ƒçš„æŒ‡æ ‡åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºæ‰€æœ‰æŒ‡æ ‡
            categories: è¦æ¯”è¾ƒçš„æ¨¡å‹ç±»åˆ«ï¼Œé»˜è®¤ä¸ºæ‰€æœ‰ç±»åˆ«
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        Returns:
            Dict: æ¯”è¾ƒç»“æœ
        """
        if self.benchmark is None:
            logger_manager.error("æ¨¡å‹åŸºå‡†æµ‹è¯•æ¡†æ¶æœªæ‰¾åˆ°ï¼Œæ— æ³•æ¯”è¾ƒæ¨¡å‹")
            return {}
        
        return self.benchmark.compare_models(metrics, categories, verbose)
    
    def optimize_model(self, model_name: str, model_creator: Callable, param_space: Dict,
                      optimization_method: str = 'grid', n_iter: int = 50,
                      train_periods: int = 500, val_periods: int = 50,
                      metric: str = 'accuracy', verbose: bool = True) -> Dict:
        """ä¼˜åŒ–æ¨¡å‹å‚æ•°
        
        Args:
            model_name: æ¨¡å‹åç§°
            model_creator: æ¨¡å‹åˆ›å»ºå‡½æ•°ï¼Œæ¥å—å‚æ•°å­—å…¸ï¼Œè¿”å›æ¨¡å‹å®ä¾‹
            param_space: å‚æ•°ç©ºé—´ï¼Œå­—å…¸å½¢å¼ï¼Œé”®ä¸ºå‚æ•°åï¼Œå€¼ä¸ºå‚æ•°å¯èƒ½çš„å–å€¼åˆ—è¡¨
            optimization_method: ä¼˜åŒ–æ–¹æ³•ï¼Œå¯é€‰å€¼: 'grid', 'random', 'bayesian'
            n_iter: éšæœºæœç´¢æˆ–è´å¶æ–¯ä¼˜åŒ–çš„è¿­ä»£æ¬¡æ•°
            train_periods: è®­ç»ƒæ•°æ®æœŸæ•°
            val_periods: éªŒè¯æ•°æ®æœŸæ•°
            metric: ä¼˜åŒ–æŒ‡æ ‡ï¼Œå¯é€‰å€¼: 'accuracy', 'hit_rate', 'roi'
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        Returns:
            Dict: ä¼˜åŒ–ç»“æœ
        """
        if not OPTIMIZER_AVAILABLE:
            logger_manager.error("æ¨¡å‹ä¼˜åŒ–å™¨æœªæ‰¾åˆ°ï¼Œæ— æ³•ä¼˜åŒ–æ¨¡å‹")
            return {}
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = ModelOptimizer(
            model_creator=model_creator,
            param_space=param_space,
            optimization_method=optimization_method,
            n_iter=n_iter
        )
        
        # æ‰§è¡Œä¼˜åŒ–
        result = optimizer.optimize(train_periods, val_periods, metric, verbose)
        
        # ä¿å­˜ä¼˜åŒ–å™¨
        self.optimizers[model_name] = optimizer
        
        # æ³¨å†Œä¼˜åŒ–åçš„æ¨¡å‹
        if optimizer.best_model:
            optimized_model_name = f"ä¼˜åŒ–åçš„{model_name}"
            self.register_model(
                optimized_model_name,
                lambda data: optimizer.best_model.predict(data) if hasattr(optimizer.best_model, 'predict') else optimizer.best_model.predict_next(data),
                f"ä¼˜åŒ–åçš„{model_name} (å‚æ•°: {optimizer.best_params})",
                "optimized"
            )
        
        return result
    
    def visualize_optimization(self, model_name: str, output_dir: str = "output"):
        """å¯è§†åŒ–ä¼˜åŒ–è¿‡ç¨‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            output_dir: è¾“å‡ºç›®å½•
        """
        if model_name not in self.optimizers:
            logger_manager.error(f"æœªæ‰¾åˆ°æ¨¡å‹ {model_name} çš„ä¼˜åŒ–å™¨")
            return
        
        optimizer = self.optimizers[model_name]
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # å¯è§†åŒ–ä¼˜åŒ–è¿‡ç¨‹
        process_path = os.path.join(output_dir, f"{model_name}_optimization_process.png")
        optimizer.visualize_optimization_process(process_path)
        
        # å¯è§†åŒ–å‚æ•°é‡è¦æ€§
        importance_path = os.path.join(output_dir, f"{model_name}_parameter_importance.png")
        optimizer.visualize_parameter_importance(importance_path)
    
    def generate_report(self, output_path=None) -> str:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        
        Args:
            output_path: æŠ¥å‘Šè¾“å‡ºè·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è¿”å›æŠ¥å‘Šå†…å®¹
            
        Returns:
            str: æŠ¥å‘Šå†…å®¹
        """
        if self.benchmark is None:
            logger_manager.error("æ¨¡å‹åŸºå‡†æµ‹è¯•æ¡†æ¶æœªæ‰¾åˆ°ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
            return "æ¨¡å‹åŸºå‡†æµ‹è¯•æ¡†æ¶æœªæ‰¾åˆ°ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š"
        
        return self.benchmark.generate_report(output_path)
    
    def visualize_comparison(self, metrics=None, output_path=None, figsize=(12, 10)):
        """å¯è§†åŒ–æ¨¡å‹æ¯”è¾ƒç»“æœ
        
        Args:
            metrics: è¦å¯è§†åŒ–çš„æŒ‡æ ‡åˆ—è¡¨ï¼Œé»˜è®¤ä¸ºæ‰€æœ‰æŒ‡æ ‡
            output_path: å›¾è¡¨è¾“å‡ºè·¯å¾„
            figsize: å›¾è¡¨å¤§å°
        """
        if self.benchmark is None:
            logger_manager.error("æ¨¡å‹åŸºå‡†æµ‹è¯•æ¡†æ¶æœªæ‰¾åˆ°ï¼Œæ— æ³•å¯è§†åŒ–æ¯”è¾ƒ")
            return
        
        self.benchmark.visualize_comparison(metrics, output_path, figsize)
    
    def visualize_model_performance(self, model_name, output_path=None):
        """å¯è§†åŒ–å•ä¸ªæ¨¡å‹çš„æ€§èƒ½
        
        Args:
            model_name: æ¨¡å‹åç§°
            output_path: å›¾è¡¨è¾“å‡ºè·¯å¾„
        """
        if self.benchmark is None:
            logger_manager.error("æ¨¡å‹åŸºå‡†æµ‹è¯•æ¡†æ¶æœªæ‰¾åˆ°ï¼Œæ— æ³•å¯è§†åŒ–æ¨¡å‹æ€§èƒ½")
            return
        
        self.benchmark.visualize_model_performance(model_name, output_path)
    
    def save_results(self, output_dir="output"):
        """ä¿å­˜æ‰€æœ‰è¯„ä¼°å’Œä¼˜åŒ–ç»“æœ
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(output_dir, exist_ok=True)
            
            # ä¿å­˜è¯„ä¼°ç»“æœ
            if self.benchmark:
                benchmark_path = os.path.join(output_dir, "model_benchmark_results.json")
                self.benchmark.save_results(benchmark_path)
            
            # ä¿å­˜ä¼˜åŒ–ç»“æœ
            for model_name, optimizer in self.optimizers.items():
                optimizer_path = os.path.join(output_dir, f"{model_name}_optimization.json")
                optimizer.save_results(optimizer_path)
            
            logger_manager.info(f"æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° {output_dir} ç›®å½•")
            
        except Exception as e:
            logger_manager.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    def load_results(self, benchmark_path=None, optimizer_paths=None):
        """åŠ è½½è¯„ä¼°å’Œä¼˜åŒ–ç»“æœ
        
        Args:
            benchmark_path: åŸºå‡†æµ‹è¯•ç»“æœè·¯å¾„
            optimizer_paths: ä¼˜åŒ–å™¨ç»“æœè·¯å¾„å­—å…¸ï¼Œé”®ä¸ºæ¨¡å‹åç§°ï¼Œå€¼ä¸ºè·¯å¾„
            
        Returns:
            bool: æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        success = True
        
        # åŠ è½½åŸºå‡†æµ‹è¯•ç»“æœ
        if benchmark_path and self.benchmark:
            if not self.benchmark.load_results(benchmark_path):
                logger_manager.error(f"åŠ è½½åŸºå‡†æµ‹è¯•ç»“æœå¤±è´¥: {benchmark_path}")
                success = False
        
        # åŠ è½½ä¼˜åŒ–å™¨ç»“æœ
        if optimizer_paths:
            for model_name, path in optimizer_paths.items():
                if not OPTIMIZER_AVAILABLE:
                    logger_manager.error("æ¨¡å‹ä¼˜åŒ–å™¨æœªæ‰¾åˆ°ï¼Œæ— æ³•åŠ è½½ä¼˜åŒ–ç»“æœ")
                    success = False
                    continue
                
                # åˆ›å»ºç©ºä¼˜åŒ–å™¨
                optimizer = ModelOptimizer(lambda: None, {})
                
                # åŠ è½½ç»“æœ
                if optimizer.load_results(path):
                    self.optimizers[model_name] = optimizer
                else:
                    logger_manager.error(f"åŠ è½½ä¼˜åŒ–å™¨ç»“æœå¤±è´¥: {path}")
                    success = False
        
        return success


# å…¨å±€å®ä¾‹
_model_evaluator = None

def get_model_evaluator() -> ModelEvaluator:
    """è·å–æ¨¡å‹è¯„ä¼°å™¨å®ä¾‹"""
    global _model_evaluator
    if _model_evaluator is None:
        _model_evaluator = ModelEvaluator()
    return _model_evaluator


if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å‹è¯„ä¼°æ¡†æ¶
    print("ğŸ” æµ‹è¯•æ¨¡å‹è¯„ä¼°æ¡†æ¶...")
    evaluator = get_model_evaluator()
    
    # æ¨¡æ‹Ÿé¢„æµ‹å‡½æ•°
    def mock_random_predict(historical_data):
        """éšæœºé¢„æµ‹"""
        return [(sorted(np.random.choice(range(1, 36), 5, replace=False)), 
                sorted(np.random.choice(range(1, 13), 2, replace=False))) for _ in range(3)]
    
    def mock_frequency_predict(historical_data):
        """é¢‘ç‡é¢„æµ‹"""
        # ç®€åŒ–ç‰ˆé¢‘ç‡é¢„æµ‹
        from collections import Counter
        
        front_counter = Counter()
        back_counter = Counter()
        
        for _, row in historical_data.iterrows():
            front_balls, back_balls = data_manager.parse_balls(row)
            front_counter.update(front_balls)
            back_counter.update(back_balls)
        
        front_most_common = [ball for ball, _ in front_counter.most_common(5)]
        back_most_common = [ball for ball, _ in back_counter.most_common(2)]
        
        return [(sorted(front_most_common), sorted(back_most_common)) for _ in range(3)]
    
    # æ³¨å†Œæ¨¡å‹
    print("\nğŸ“ æ³¨å†Œæµ‹è¯•æ¨¡å‹...")
    evaluator.register_model("éšæœºé¢„æµ‹", mock_random_predict, "å®Œå…¨éšæœºçš„é¢„æµ‹æ–¹æ³•", "baseline")
    evaluator.register_model("é¢‘ç‡é¢„æµ‹", mock_frequency_predict, "åŸºäºå†å²é¢‘ç‡çš„é¢„æµ‹æ–¹æ³•", "traditional")
    
    # è¯„ä¼°æ¨¡å‹
    print("\nğŸ“Š è¯„ä¼°æ¨¡å‹...")
    evaluator.evaluate_all_models(test_periods=20, verbose=True)
    
    # æ¯”è¾ƒæ¨¡å‹
    print("\nğŸ”„ æ¯”è¾ƒæ¨¡å‹...")
    evaluator.compare_models(verbose=True)
    
    # å®šä¹‰ç®€å•æ¨¡å‹ç±»
    class SimplePredictor:
        """ç®€å•é¢„æµ‹æ¨¡å‹ï¼Œç”¨äºæµ‹è¯•å‚æ•°ä¼˜åŒ–"""
        
        def __init__(self, weight_frequency=0.5, weight_missing=0.3):
            self.weight_frequency = weight_frequency
            self.weight_missing = weight_missing
            self.frequency_stats = None
            self.missing_stats = None
        
        def fit(self, train_data):
            # ç®€åŒ–ç‰ˆè®­ç»ƒ
            front_counter = {}
            back_counter = {}
            
            for _, row in train_data.iterrows():
                front_balls, back_balls = data_manager.parse_balls(row)
                
                for ball in front_balls:
                    front_counter[ball] = front_counter.get(ball, 0) + 1
                
                for ball in back_balls:
                    back_counter[ball] = back_counter.get(ball, 0) + 1
            
            # å½’ä¸€åŒ–é¢‘ç‡
            total_front = sum(front_counter.values()) if front_counter else 0
            total_back = sum(back_counter.values()) if back_counter else 0
            
            self.frequency_stats = {
                'front': {ball: count/total_front if total_front > 0 else 0 for ball, count in front_counter.items()},
                'back': {ball: count/total_back if total_back > 0 else 0 for ball, count in back_counter.items()}
            }
            
            # è®¡ç®—é—æ¼ç»Ÿè®¡
            front_missing = {i: 0 for i in range(1, 36)}
            back_missing = {i: 0 for i in range(1, 13)}
            
            for i, row in train_data.iterrows():
                front_balls, back_balls = data_manager.parse_balls(row)
                
                for ball in range(1, 36):
                    if ball in front_balls:
                        front_missing[ball] = 0
                    else:
                        front_missing[ball] += 1
                
                for ball in range(1, 13):
                    if ball in back_balls:
                        back_missing[ball] = 0
                    else:
                        back_missing[ball] += 1
            
            # å½’ä¸€åŒ–é—æ¼å€¼
            max_front_missing = max(front_missing.values()) if front_missing else 1
            max_back_missing = max(back_missing.values()) if back_missing else 1
            
            self.missing_stats = {
                'front': {ball: missing/max_front_missing if max_front_missing > 0 else 0 for ball, missing in front_missing.items()},
                'back': {ball: missing/max_back_missing if max_back_missing > 0 else 0 for ball, missing in back_missing.items()}
            }
        
        def predict(self, data=None):
            if self.frequency_stats is None or self.missing_stats is None:
                return []
            
            # è®¡ç®—ç»¼åˆå¾—åˆ†
            front_scores = {}
            back_scores = {}
            
            for ball in range(1, 36):
                # é¢‘ç‡å¾—åˆ†
                freq_score = self.frequency_stats['front'].get(ball, 0)
                # é—æ¼å¾—åˆ†
                missing_score = self.missing_stats['front'].get(ball, 0)
                
                # ç»¼åˆå¾—åˆ†
                front_scores[ball] = self.weight_frequency * freq_score + self.weight_missing * missing_score
            
            for ball in range(1, 13):
                # é¢‘ç‡å¾—åˆ†
                freq_score = self.frequency_stats['back'].get(ball, 0)
                # é—æ¼å¾—åˆ†
                missing_score = self.missing_stats['back'].get(ball, 0)
                
                # ç»¼åˆå¾—åˆ†
                back_scores[ball] = self.weight_frequency * freq_score + self.weight_missing * missing_score
            
            # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„å·ç 
            front_sorted = sorted(front_scores.items(), key=lambda x: x[1], reverse=True)
            back_sorted = sorted(back_scores.items(), key=lambda x: x[1], reverse=True)
            
            front_balls = [ball for ball, _ in front_sorted[:5]]
            back_balls = [ball for ball, _ in back_sorted[:2]]
            
            return [(sorted(front_balls), sorted(back_balls))]
    
    # ä¼˜åŒ–æ¨¡å‹å‚æ•°
    if OPTIMIZER_AVAILABLE:
        print("\nğŸ”§ ä¼˜åŒ–æ¨¡å‹å‚æ•°...")
        
        # å®šä¹‰å‚æ•°ç©ºé—´
        param_space = {
            'weight_frequency': [0.3, 0.5, 0.7, 0.9],
            'weight_missing': [0.1, 0.3, 0.5, 0.7]
        }
        
        # å®šä¹‰æ¨¡å‹åˆ›å»ºå‡½æ•°
        def create_model(**params):
            return SimplePredictor(**params)
        
        # æ‰§è¡Œä¼˜åŒ–
        evaluator.optimize_model(
            model_name="ç®€å•é¢„æµ‹å™¨",
            model_creator=create_model,
            param_space=param_space,
            optimization_method='grid',
            train_periods=200,
            val_periods=30,
            metric='accuracy',
            verbose=True
        )
        
        # å¯è§†åŒ–ä¼˜åŒ–è¿‡ç¨‹
        evaluator.visualize_optimization("ç®€å•é¢„æµ‹å™¨")
        
        # è¯„ä¼°ä¼˜åŒ–åçš„æ¨¡å‹
        evaluator.evaluate_model("ä¼˜åŒ–åçš„ç®€å•é¢„æµ‹å™¨", test_periods=20)
        
        # æ¯”è¾ƒä¼˜åŒ–å‰åçš„æ¨¡å‹
        evaluator.compare_models(categories=["traditional", "optimized"])
    
    # ç”ŸæˆæŠ¥å‘Š
    print("\nğŸ“ ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
    report = evaluator.generate_report("output/model_evaluation_report.md")
    
    # å¯è§†åŒ–æ¯”è¾ƒ
    print("\nğŸ“ˆ å¯è§†åŒ–æ¨¡å‹æ¯”è¾ƒ...")
    evaluator.visualize_comparison(output_path="output/model_evaluation_comparison.png")
    
    # ä¿å­˜ç»“æœ
    print("\nğŸ’¾ ä¿å­˜è¯„ä¼°ç»“æœ...")
    evaluator.save_results("output")
    
    print("\nâœ… æ¨¡å‹è¯„ä¼°æ¡†æ¶æµ‹è¯•å®Œæˆ")