#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¢é‡è®­ç»ƒæ•°æ®æ›´æ–°å™¨
å®ç°å¢é‡æ›´æ–°è®­ç»ƒæ•°æ®çš„åŠŸèƒ½
"""

import os
import pandas as pd
import numpy as np
from typing import List, Dict, Tuple, Any, Optional, Union
from datetime import datetime
import hashlib
import json

from core_modules import logger_manager, data_manager, cache_manager


class IncrementalDataUpdater:
    """å¢é‡æ•°æ®æ›´æ–°å™¨"""
    
    def __init__(self, data_dir: str = "data", cache_dir: str = "cache/incremental"):
        """
        åˆå§‹åŒ–å¢é‡æ•°æ®æ›´æ–°å™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•
            cache_dir: ç¼“å­˜ç›®å½•
        """
        self.data_dir = data_dir
        self.cache_dir = cache_dir
        self.df = data_manager.get_data()
        self.cache_manager = cache_manager
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        os.makedirs(cache_dir, exist_ok=True)
        
        # åŠ è½½æ•°æ®æŒ‡çº¹
        self.data_fingerprints = self._load_fingerprints()
        
        if self.df is None:
            logger_manager.error("æ•°æ®æœªåŠ è½½")
        else:
            logger_manager.info(f"å¢é‡æ•°æ®æ›´æ–°å™¨åˆå§‹åŒ–å®Œæˆï¼Œæ•°æ®é‡: {len(self.df)}")
    
    def update_data(self, new_data: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        æ›´æ–°æ•°æ®
        
        Args:
            new_data: æ–°æ•°æ®
            
        Returns:
            æ›´æ–°åçš„æ•°æ®å’Œæ›´æ–°ä¿¡æ¯
        """
        if self.df is None:
            logger_manager.error("åŸå§‹æ•°æ®æœªåŠ è½½ï¼Œæ— æ³•æ›´æ–°")
            return new_data, {'status': 'error', 'message': 'åŸå§‹æ•°æ®æœªåŠ è½½'}
        
        if new_data is None or len(new_data) == 0:
            logger_manager.warning("æ²¡æœ‰æ–°æ•°æ®å¯æ›´æ–°")
            return self.df, {'status': 'no_update', 'message': 'æ²¡æœ‰æ–°æ•°æ®'}
        
        # æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
        if not self._check_data_consistency(new_data):
            logger_manager.error("æ–°æ•°æ®æ ¼å¼ä¸ç°æœ‰æ•°æ®ä¸ä¸€è‡´")
            return self.df, {'status': 'error', 'message': 'æ•°æ®æ ¼å¼ä¸ä¸€è‡´'}
        
        # å¤‡ä»½åŸå§‹æ•°æ®
        self._backup_data()
        
        # è®¡ç®—æ–°æ•°æ®æŒ‡çº¹
        new_fingerprints = self._calculate_fingerprints(new_data)
        
        # æ‰¾å‡ºæ–°å¢æ•°æ®
        added_data = []
        for _, row in new_data.iterrows():
            issue = str(row['issue'])
            fingerprint = new_fingerprints.get(issue)
            
            if issue not in self.data_fingerprints or self.data_fingerprints[issue] != fingerprint:
                added_data.append(row)
        
        if not added_data:
            logger_manager.info("æ²¡æœ‰æ–°å¢æ•°æ®")
            return self.df, {'status': 'no_update', 'message': 'æ²¡æœ‰æ–°å¢æ•°æ®'}
        
        # åˆå¹¶æ•°æ®
        added_df = pd.DataFrame(added_data)
        updated_df = pd.concat([added_df, self.df])
        
        # å»é‡å¹¶æ’åº
        updated_df = updated_df.drop_duplicates(subset=['issue'])
        updated_df = updated_df.sort_values(by='issue', ascending=False).reset_index(drop=True)
        
        # æ›´æ–°æ•°æ®æŒ‡çº¹
        self.data_fingerprints.update(new_fingerprints)
        self._save_fingerprints()
        
        # ä¿å­˜æ›´æ–°åçš„æ•°æ®
        self._save_updated_data(updated_df)
        
        # æ›´æ–°å†…å­˜ä¸­çš„æ•°æ®
        self.df = updated_df
        
        logger_manager.info(f"æ•°æ®æ›´æ–°å®Œæˆï¼Œæ–°å¢ {len(added_data)} æ¡æ•°æ®ï¼Œæ€»æ•°æ®é‡: {len(updated_df)}")
        
        return updated_df, {
            'status': 'success',
            'added_count': len(added_data),
            'total_count': len(updated_df),
            'added_issues': [str(row['issue']) for row in added_data]
        }
    
    def _check_data_consistency(self, new_data: pd.DataFrame) -> bool:
        """
        æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
        
        Args:
            new_data: æ–°æ•°æ®
            
        Returns:
            æ•°æ®æ˜¯å¦ä¸€è‡´
        """
        # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
        required_columns = ['issue', 'date', 'front_balls', 'back_balls']
        for col in required_columns:
            if col not in new_data.columns:
                logger_manager.error(f"æ–°æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {col}")
                return False
            
            if col not in self.df.columns:
                logger_manager.error(f"åŸå§‹æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {col}")
                return False
        
        # æ£€æŸ¥æ•°æ®ç±»å‹
        try:
            # å°è¯•è§£æä¸€è¡Œæ•°æ®
            if len(new_data) > 0:
                row = new_data.iloc[0]
                front_balls, back_balls = data_manager.parse_balls(row)
                
                # æ£€æŸ¥å·ç æ ¼å¼
                if len(front_balls) != 5 or len(back_balls) != 2:
                    logger_manager.error(f"å·ç æ ¼å¼é”™è¯¯: å‰åŒº {len(front_balls)}/5, ååŒº {len(back_balls)}/2")
                    return False
        except Exception as e:
            logger_manager.error(f"æ•°æ®æ ¼å¼æ£€æŸ¥å¤±è´¥: {e}")
            return False
        
        return True
    
    def _calculate_fingerprints(self, data: pd.DataFrame) -> Dict[str, str]:
        """
        è®¡ç®—æ•°æ®æŒ‡çº¹
        
        Args:
            data: æ•°æ®
            
        Returns:
            æ•°æ®æŒ‡çº¹å­—å…¸ï¼Œé”®ä¸ºæœŸå·ï¼Œå€¼ä¸ºæŒ‡çº¹
        """
        fingerprints = {}
        
        for _, row in data.iterrows():
            issue = str(row['issue'])
            
            # è®¡ç®—è¡Œæ•°æ®çš„æŒ‡çº¹
            row_data = f"{row['issue']}_{row['date']}_{row['front_balls']}_{row['back_balls']}"
            fingerprint = hashlib.md5(row_data.encode()).hexdigest()
            
            fingerprints[issue] = fingerprint
        
        return fingerprints
    
    def _load_fingerprints(self) -> Dict[str, str]:
        """
        åŠ è½½æ•°æ®æŒ‡çº¹
        
        Returns:
            æ•°æ®æŒ‡çº¹å­—å…¸
        """
        fingerprint_file = os.path.join(self.cache_dir, "data_fingerprints.json")
        
        if os.path.exists(fingerprint_file):
            try:
                with open(fingerprint_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger_manager.warning(f"åŠ è½½æ•°æ®æŒ‡çº¹å¤±è´¥: {e}")
        
        # å¦‚æœåŠ è½½å¤±è´¥æˆ–æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè®¡ç®—å½“å‰æ•°æ®çš„æŒ‡çº¹
        if self.df is not None:
            fingerprints = self._calculate_fingerprints(self.df)
            self._save_fingerprints(fingerprints)
            return fingerprints
        
        return {}
    
    def _save_fingerprints(self, fingerprints: Optional[Dict[str, str]] = None) -> None:
        """
        ä¿å­˜æ•°æ®æŒ‡çº¹
        
        Args:
            fingerprints: æ•°æ®æŒ‡çº¹å­—å…¸ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰æŒ‡çº¹
        """
        if fingerprints is None:
            fingerprints = self.data_fingerprints
        
        fingerprint_file = os.path.join(self.cache_dir, "data_fingerprints.json")
        
        try:
            with open(fingerprint_file, 'w', encoding='utf-8') as f:
                json.dump(fingerprints, f, ensure_ascii=False, indent=2)
            
            logger_manager.debug(f"æ•°æ®æŒ‡çº¹å·²ä¿å­˜ï¼Œå…± {len(fingerprints)} æ¡")
        except Exception as e:
            logger_manager.error(f"ä¿å­˜æ•°æ®æŒ‡çº¹å¤±è´¥: {e}")
    
    def _backup_data(self) -> None:
        """å¤‡ä»½åŸå§‹æ•°æ®"""
        if self.df is None:
            return
        
        try:
            # ä¿å­˜åˆ°ç¼“å­˜
            self.cache_manager.save_cache("data", "dlt_data_backup", self.df)
            logger_manager.debug("åŸå§‹æ•°æ®å·²å¤‡ä»½")
        except Exception as e:
            logger_manager.error(f"å¤‡ä»½åŸå§‹æ•°æ®å¤±è´¥: {e}")
    
    def _save_updated_data(self, updated_df: pd.DataFrame) -> None:
        """
        ä¿å­˜æ›´æ–°åçš„æ•°æ®
        
        Args:
            updated_df: æ›´æ–°åçš„æ•°æ®
        """
        try:
            # ä¿å­˜åˆ°æ–‡ä»¶
            data_file = os.path.join(self.data_dir, "dlt_data_all.csv")
            updated_df.to_csv(data_file, index=False)
            
            # ä¿å­˜åˆ°ç¼“å­˜
            self.cache_manager.save_cache("data", "dlt_data_all", updated_df)
            
            logger_manager.info(f"æ›´æ–°åçš„æ•°æ®å·²ä¿å­˜ï¼Œæ•°æ®é‡: {len(updated_df)}")
        except Exception as e:
            logger_manager.error(f"ä¿å­˜æ›´æ–°åçš„æ•°æ®å¤±è´¥: {e}")
    
    def restore_backup(self) -> pd.DataFrame:
        """
        æ¢å¤å¤‡ä»½æ•°æ®
        
        Returns:
            æ¢å¤åçš„æ•°æ®
        """
        try:
            # ä»ç¼“å­˜åŠ è½½å¤‡ä»½
            backup_df = self.cache_manager.load_cache("data", "dlt_data_backup")
            
            if backup_df is None or len(backup_df) == 0:
                logger_manager.warning("æ²¡æœ‰å¯ç”¨çš„å¤‡ä»½æ•°æ®")
                return self.df
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            data_file = os.path.join(self.data_dir, "dlt_data_all.csv")
            backup_df.to_csv(data_file, index=False)
            
            # æ›´æ–°å†…å­˜ä¸­çš„æ•°æ®
            self.df = backup_df
            
            logger_manager.info(f"æ•°æ®å·²æ¢å¤åˆ°å¤‡ä»½ï¼Œæ•°æ®é‡: {len(backup_df)}")
            
            return backup_df
        except Exception as e:
            logger_manager.error(f"æ¢å¤å¤‡ä»½æ•°æ®å¤±è´¥: {e}")
            return self.df
    
    def create_update_trigger(self, trigger_file: str) -> bool:
        """
        åˆ›å»ºæ›´æ–°è§¦å‘å™¨
        
        Args:
            trigger_file: è§¦å‘å™¨æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ˜¯å¦åˆ›å»ºæˆåŠŸ
        """
        try:
            # åˆ›å»ºè§¦å‘å™¨æ–‡ä»¶
            with open(trigger_file, 'w', encoding='utf-8') as f:
                f.write(f"update_trigger_{datetime.now().isoformat()}")
            
            logger_manager.info(f"æ›´æ–°è§¦å‘å™¨å·²åˆ›å»º: {trigger_file}")
            return True
        except Exception as e:
            logger_manager.error(f"åˆ›å»ºæ›´æ–°è§¦å‘å™¨å¤±è´¥: {e}")
            return False
    
    def check_update_trigger(self, trigger_file: str) -> bool:
        """
        æ£€æŸ¥æ›´æ–°è§¦å‘å™¨
        
        Args:
            trigger_file: è§¦å‘å™¨æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ˜¯å¦éœ€è¦æ›´æ–°
        """
        if not os.path.exists(trigger_file):
            return False
        
        try:
            # è¯»å–è§¦å‘å™¨æ–‡ä»¶
            with open(trigger_file, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„è§¦å‘å™¨
            if content.startswith("update_trigger_"):
                # åˆ é™¤è§¦å‘å™¨æ–‡ä»¶
                os.remove(trigger_file)
                logger_manager.info(f"æ£€æµ‹åˆ°æ›´æ–°è§¦å‘å™¨: {trigger_file}")
                return True
        except Exception as e:
            logger_manager.error(f"æ£€æŸ¥æ›´æ–°è§¦å‘å™¨å¤±è´¥: {e}")
        
        return False


class AutoIncrementalUpdater(IncrementalDataUpdater):
    """è‡ªåŠ¨å¢é‡æ›´æ–°å™¨"""
    
    def __init__(self, data_dir: str = "data", cache_dir: str = "cache/incremental",
               update_interval: int = 24, max_retries: int = 3):
        """
        åˆå§‹åŒ–è‡ªåŠ¨å¢é‡æ›´æ–°å™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•
            cache_dir: ç¼“å­˜ç›®å½•
            update_interval: æ›´æ–°é—´éš”ï¼ˆå°æ—¶ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        super().__init__(data_dir, cache_dir)
        self.update_interval = update_interval
        self.max_retries = max_retries
        self.last_update = self._load_last_update()
        self.update_history = self._load_update_history()
        
        logger_manager.info(f"è‡ªåŠ¨å¢é‡æ›´æ–°å™¨åˆå§‹åŒ–å®Œæˆï¼Œæ›´æ–°é—´éš”: {update_interval} å°æ—¶")
    
    def _load_last_update(self) -> Optional[datetime]:
        """
        åŠ è½½ä¸Šæ¬¡æ›´æ–°æ—¶é—´
        
        Returns:
            ä¸Šæ¬¡æ›´æ–°æ—¶é—´
        """
        last_update_file = os.path.join(self.cache_dir, "last_update.txt")
        
        if os.path.exists(last_update_file):
            try:
                with open(last_update_file, 'r', encoding='utf-8') as f:
                    timestamp = f.read().strip()
                    return datetime.fromisoformat(timestamp)
            except Exception as e:
                logger_manager.warning(f"åŠ è½½ä¸Šæ¬¡æ›´æ–°æ—¶é—´å¤±è´¥: {e}")
        
        return None
    
    def _save_last_update(self, timestamp: Optional[datetime] = None) -> None:
        """
        ä¿å­˜ä¸Šæ¬¡æ›´æ–°æ—¶é—´
        
        Args:
            timestamp: æ—¶é—´æˆ³ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰æ—¶é—´
        """
        if timestamp is None:
            timestamp = datetime.now()
        
        last_update_file = os.path.join(self.cache_dir, "last_update.txt")
        
        try:
            with open(last_update_file, 'w', encoding='utf-8') as f:
                f.write(timestamp.isoformat())
            
            self.last_update = timestamp
            logger_manager.debug(f"ä¸Šæ¬¡æ›´æ–°æ—¶é—´å·²ä¿å­˜: {timestamp}")
        except Exception as e:
            logger_manager.error(f"ä¿å­˜ä¸Šæ¬¡æ›´æ–°æ—¶é—´å¤±è´¥: {e}")
    
    def _load_update_history(self) -> List[Dict[str, Any]]:
        """
        åŠ è½½æ›´æ–°å†å²
        
        Returns:
            æ›´æ–°å†å²åˆ—è¡¨
        """
        history_file = os.path.join(self.cache_dir, "update_history.json")
        
        if os.path.exists(history_file):
            try:
                with open(history_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger_manager.warning(f"åŠ è½½æ›´æ–°å†å²å¤±è´¥: {e}")
        
        return []
    
    def _save_update_history(self) -> None:
        """ä¿å­˜æ›´æ–°å†å²"""
        history_file = os.path.join(self.cache_dir, "update_history.json")
        
        try:
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(self.update_history, f, ensure_ascii=False, indent=2)
            
            logger_manager.debug(f"æ›´æ–°å†å²å·²ä¿å­˜ï¼Œå…± {len(self.update_history)} æ¡")
        except Exception as e:
            logger_manager.error(f"ä¿å­˜æ›´æ–°å†å²å¤±è´¥: {e}")
    
    def _add_update_history(self, update_info: Dict[str, Any]) -> None:
        """
        æ·»åŠ æ›´æ–°å†å²
        
        Args:
            update_info: æ›´æ–°ä¿¡æ¯
        """
        # æ·»åŠ æ—¶é—´æˆ³
        update_info['timestamp'] = datetime.now().isoformat()
        
        # æ·»åŠ åˆ°å†å²
        self.update_history.append(update_info)
        
        # é™åˆ¶å†å²å¤§å°
        if len(self.update_history) > 100:
            self.update_history = self.update_history[-100:]
        
        # ä¿å­˜å†å²
        self._save_update_history()
    
    def should_update(self) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥æ›´æ–°
        
        Returns:
            æ˜¯å¦åº”è¯¥æ›´æ–°
        """
        if self.last_update is None:
            return True
        
        # è®¡ç®—è·ç¦»ä¸Šæ¬¡æ›´æ–°çš„æ—¶é—´
        elapsed = datetime.now() - self.last_update
        elapsed_hours = elapsed.total_seconds() / 3600
        
        return elapsed_hours >= self.update_interval
    
    def auto_update(self, new_data_source: Optional[Callable[[], pd.DataFrame]] = None) -> Dict[str, Any]:
        """
        è‡ªåŠ¨æ›´æ–°
        
        Args:
            new_data_source: æ–°æ•°æ®æºå‡½æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æ•°æ®æº
            
        Returns:
            æ›´æ–°ç»“æœ
        """
        if not self.should_update():
            logger_manager.info("ä¸éœ€è¦æ›´æ–°")
            return {'status': 'skipped', 'message': 'ä¸éœ€è¦æ›´æ–°'}
        
        # è·å–æ–°æ•°æ®
        if new_data_source is None:
            # ä½¿ç”¨é»˜è®¤æ•°æ®æº
            # è¿™é‡Œåº”è¯¥å®ç°ä¸€ä¸ªé»˜è®¤çš„æ•°æ®æºï¼Œä¾‹å¦‚ä»ç½‘ç»œè·å–æœ€æ–°æ•°æ®
            # ç”±äºæ²¡æœ‰å…·ä½“çš„æ•°æ®æºï¼Œè¿™é‡Œåªæ˜¯ä¸€ä¸ªç¤ºä¾‹
            logger_manager.error("æ²¡æœ‰æä¾›æ•°æ®æº")
            return {'status': 'error', 'message': 'æ²¡æœ‰æä¾›æ•°æ®æº'}
        
        # å°è¯•è·å–æ–°æ•°æ®
        retries = 0
        while retries < self.max_retries:
            try:
                new_data = new_data_source()
                break
            except Exception as e:
                retries += 1
                logger_manager.error(f"è·å–æ–°æ•°æ®å¤±è´¥ ({retries}/{self.max_retries}): {e}")
                if retries >= self.max_retries:
                    return {'status': 'error', 'message': f'è·å–æ–°æ•°æ®å¤±è´¥: {e}'}
        
        # æ›´æ–°æ•°æ®
        _, update_info = self.update_data(new_data)
        
        # æ›´æ–°ä¸Šæ¬¡æ›´æ–°æ—¶é—´
        self._save_last_update()
        
        # æ·»åŠ æ›´æ–°å†å²
        self._add_update_history(update_info)
        
        return update_info
    
    def get_update_history(self) -> List[Dict[str, Any]]:
        """
        è·å–æ›´æ–°å†å²
        
        Returns:
            æ›´æ–°å†å²åˆ—è¡¨
        """
        return self.update_history


if __name__ == "__main__":
    # æµ‹è¯•å¢é‡æ•°æ®æ›´æ–°å™¨
    print("ğŸ”„ æµ‹è¯•å¢é‡æ•°æ®æ›´æ–°å™¨...")
    
    # åˆ›å»ºå¢é‡æ•°æ®æ›´æ–°å™¨
    updater = IncrementalDataUpdater()
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ–°æ•°æ®
    if updater.df is not None and len(updater.df) > 0:
        # ä½¿ç”¨ç°æœ‰æ•°æ®çš„å‰å‡ è¡Œä½œä¸ºæ¨¡æ‹Ÿæ–°æ•°æ®
        mock_new_data = updater.df.head(5).copy()
        
        # ä¿®æ”¹æœŸå·ï¼Œæ¨¡æ‹Ÿæ–°æ•°æ®
        for i, row in mock_new_data.iterrows():
            mock_new_data.at[i, 'issue'] = f"mock_{row['issue']}"
        
        # æ›´æ–°æ•°æ®
        updated_df, update_info = updater.update_data(mock_new_data)
        
        print(f"æ›´æ–°ä¿¡æ¯: {update_info}")
        print(f"æ›´æ–°åæ•°æ®é‡: {len(updated_df)}")
    
    # æµ‹è¯•è‡ªåŠ¨å¢é‡æ›´æ–°å™¨
    print("\nğŸ”„ æµ‹è¯•è‡ªåŠ¨å¢é‡æ›´æ–°å™¨...")
    
    # åˆ›å»ºè‡ªåŠ¨å¢é‡æ›´æ–°å™¨
    auto_updater = AutoIncrementalUpdater(update_interval=24)
    
    # æ£€æŸ¥æ˜¯å¦åº”è¯¥æ›´æ–°
    should_update = auto_updater.should_update()
    print(f"æ˜¯å¦åº”è¯¥æ›´æ–°: {should_update}")
    
    # æ¨¡æ‹Ÿè‡ªåŠ¨æ›´æ–°
    def mock_data_source():
        if updater.df is not None and len(updater.df) > 0:
            mock_data = updater.df.head(3).copy()
            for i, row in mock_data.iterrows():
                mock_data.at[i, 'issue'] = f"auto_{row['issue']}"
            return mock_data
        return pd.DataFrame()
    
    update_result = auto_updater.auto_update(mock_data_source)
    print(f"è‡ªåŠ¨æ›´æ–°ç»“æœ: {update_result}")
    
    # è·å–æ›´æ–°å†å²
    history = auto_updater.get_update_history()
    print(f"æ›´æ–°å†å²: {len(history)} æ¡")
    
    print("å¢é‡æ•°æ®æ›´æ–°å™¨æµ‹è¯•å®Œæˆ")