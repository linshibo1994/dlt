#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Transformeré¢„æµ‹å™¨
åŸºäºTransformeræ¶æ„çš„æ·±åº¦å­¦ä¹ é¢„æµ‹æ¨¡å‹
"""

import os
import numpy as np
import tensorflow as tf
from typing import List, Tuple, Dict, Any
from tensorflow.keras import layers, Model, optimizers
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler
from datetime import datetime
import math

from . import BaseDeepPredictor
from ..utils.config import DEFAULT_TRANSFORMER_CONFIG
from ..utils.exceptions import ModelInitializationError, handle_model_error
from core_modules import logger_manager


class TransformerPredictor(BaseDeepPredictor):
    """åŸºäºTransformerçš„å½©ç¥¨é¢„æµ‹æ¨¡å‹"""
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–Transformeré¢„æµ‹å™¨
        
        Args:
            config: é…ç½®å‚æ•°å­—å…¸
        """
        # åˆå¹¶é»˜è®¤é…ç½®å’Œç”¨æˆ·é…ç½®
        merged_config = DEFAULT_TRANSFORMER_CONFIG.copy()
        if config:
            merged_config.update(config)
        
        super().__init__(name="Transformer", config=merged_config)
        
        # ä»é…ç½®ä¸­æå–å‚æ•°
        self.d_model = self.config.get('d_model', 256)
        self.num_heads = self.config.get('num_heads', 8)
        self.num_encoder_layers = self.config.get('num_encoder_layers', 6)
        self.num_decoder_layers = self.config.get('num_decoder_layers', 6)
        self.dff = self.config.get('dff', 1024)
        self.dropout_rate = self.config.get('dropout_rate', 0.1)

        # é«˜çº§Transformerå‚æ•°
        self.use_relative_position = self.config.get('use_relative_position', True)
        self.use_sparse_attention = self.config.get('use_sparse_attention', False)
        self.use_local_attention = self.config.get('use_local_attention', False)
        self.local_attention_window = self.config.get('local_attention_window', 64)
        self.max_position_encoding = self.config.get('max_position_encoding', 1000)

        logger_manager.info(f"åˆå§‹åŒ–å¢å¼ºTransformeré¢„æµ‹å™¨: d_model={self.d_model}, heads={self.num_heads}, "
                          f"encoder_layers={self.num_encoder_layers}, decoder_layers={self.num_decoder_layers}")
    
    def _build_model(self):
        """æ„å»ºå¢å¼ºTransformeræ¨¡å‹ï¼ˆç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼‰"""
        try:
            # ç¼–ç å™¨è¾“å…¥
            encoder_inputs = layers.Input(shape=(self.sequence_length, self.feature_dim), name='encoder_inputs')

            # è§£ç å™¨è¾“å…¥ï¼ˆç”¨äºè®­ç»ƒæ—¶çš„teacher forcingï¼‰
            decoder_inputs = layers.Input(shape=(None, 7), name='decoder_inputs')  # 7 = 5å‰åŒº + 2ååŒº

            # æ„å»ºç¼–ç å™¨
            encoder_outputs = self._build_encoder(encoder_inputs)

            # æ„å»ºè§£ç å™¨
            decoder_outputs = self._build_decoder(decoder_inputs, encoder_outputs)

            # æ„å»ºå®Œæ•´æ¨¡å‹
            model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs, name='Enhanced_Transformer')

            # ç¼–è¯‘æ¨¡å‹
            optimizer = optimizers.Adam(
                learning_rate=self.config.get('learning_rate', 0.0001),
                beta_1=0.9,
                beta_2=0.98,
                epsilon=1e-9
            )

            model.compile(
                optimizer=optimizer,
                loss='mse',
                metrics=['mae', 'mape']
            )

            # æ‰“å°æ¨¡å‹æ‘˜è¦
            model.summary()

            return model
        except Exception as e:
            raise ModelInitializationError("Enhanced_Transformer", str(e))

    def _build_encoder(self, inputs):
        """æ„å»ºTransformerç¼–ç å™¨"""
        # è¾“å…¥åµŒå…¥å’Œä½ç½®ç¼–ç 
        x = layers.Dense(self.d_model, name='encoder_embedding')(inputs)
        x = x * tf.math.sqrt(tf.cast(self.d_model, tf.float32))

        # ä½ç½®ç¼–ç 
        if self.use_relative_position:
            x = self._add_relative_position_encoding(x)
        else:
            x = self._add_absolute_position_encoding(x)

        x = layers.Dropout(self.dropout_rate)(x)

        # ç¼–ç å™¨å±‚
        for i in range(self.num_encoder_layers):
            x = self._encoder_layer(x, i)

        return x

    def _build_decoder(self, inputs, encoder_outputs):
        """æ„å»ºTransformerè§£ç å™¨"""
        # è¾“å…¥åµŒå…¥å’Œä½ç½®ç¼–ç 
        x = layers.Dense(self.d_model, name='decoder_embedding')(inputs)
        x = x * tf.math.sqrt(tf.cast(self.d_model, tf.float32))

        # ä½ç½®ç¼–ç 
        if self.use_relative_position:
            x = self._add_relative_position_encoding(x, name_prefix='decoder')
        else:
            x = self._add_absolute_position_encoding(x, name_prefix='decoder')

        x = layers.Dropout(self.dropout_rate)(x)

        # è§£ç å™¨å±‚
        for i in range(self.num_decoder_layers):
            x = self._decoder_layer(x, encoder_outputs, i)

        # è¾“å‡ºæŠ•å½±
        outputs = layers.Dense(7, activation='linear', name='output_projection')(x)

        return outputs

    def _encoder_layer(self, x, layer_idx):
        """å•ä¸ªç¼–ç å™¨å±‚"""
        # å¤šå¤´è‡ªæ³¨æ„åŠ›
        if self.use_sparse_attention:
            attention_output = self._sparse_multi_head_attention(x, x, layer_idx, 'encoder')
        elif self.use_local_attention:
            attention_output = self._local_multi_head_attention(x, x, layer_idx, 'encoder')
        else:
            attention_output = layers.MultiHeadAttention(
                num_heads=self.num_heads,
                key_dim=self.d_model // self.num_heads,
                dropout=self.dropout_rate,
                name=f'encoder_mha_{layer_idx}'
            )(x, x)

        # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
        x = layers.Add(name=f'encoder_add_1_{layer_idx}')([x, attention_output])
        x = layers.LayerNormalization(epsilon=1e-6, name=f'encoder_ln_1_{layer_idx}')(x)

        # å‰é¦ˆç½‘ç»œ
        ffn_output = self._point_wise_feed_forward_network(x, layer_idx, 'encoder')

        # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
        x = layers.Add(name=f'encoder_add_2_{layer_idx}')([x, ffn_output])
        x = layers.LayerNormalization(epsilon=1e-6, name=f'encoder_ln_2_{layer_idx}')(x)

        return x

    def _decoder_layer(self, x, encoder_outputs, layer_idx):
        """å•ä¸ªè§£ç å™¨å±‚"""
        # æ©ç å¤šå¤´è‡ªæ³¨æ„åŠ›
        masked_attention_output = layers.MultiHeadAttention(
            num_heads=self.num_heads,
            key_dim=self.d_model // self.num_heads,
            dropout=self.dropout_rate,
            name=f'decoder_masked_mha_{layer_idx}'
        )(x, x, use_causal_mask=True)

        # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
        x = layers.Add(name=f'decoder_add_1_{layer_idx}')([x, masked_attention_output])
        x = layers.LayerNormalization(epsilon=1e-6, name=f'decoder_ln_1_{layer_idx}')(x)

        # ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›
        cross_attention_output = layers.MultiHeadAttention(
            num_heads=self.num_heads,
            key_dim=self.d_model // self.num_heads,
            dropout=self.dropout_rate,
            name=f'decoder_cross_mha_{layer_idx}'
        )(x, encoder_outputs)

        # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
        x = layers.Add(name=f'decoder_add_2_{layer_idx}')([x, cross_attention_output])
        x = layers.LayerNormalization(epsilon=1e-6, name=f'decoder_ln_2_{layer_idx}')(x)

        # å‰é¦ˆç½‘ç»œ
        ffn_output = self._point_wise_feed_forward_network(x, layer_idx, 'decoder')

        # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
        x = layers.Add(name=f'decoder_add_3_{layer_idx}')([x, ffn_output])
        x = layers.LayerNormalization(epsilon=1e-6, name=f'decoder_ln_3_{layer_idx}')(x)

        return x
    
    def _point_wise_feed_forward_network(self, x, layer_idx=0, layer_type='encoder'):
        """å®ç°Transformerçš„å‰é¦ˆç½‘ç»œ"""
        x = layers.Dense(self.dff, activation='relu', name=f'{layer_type}_ffn_1_{layer_idx}')(x)
        x = layers.Dropout(self.dropout_rate)(x)
        x = layers.Dense(self.d_model, name=f'{layer_type}_ffn_2_{layer_idx}')(x)
        return x

    def _add_absolute_position_encoding(self, x, name_prefix='encoder'):
        """æ·»åŠ ç»å¯¹ä½ç½®ç¼–ç """
        seq_len = tf.shape(x)[1]
        positions = tf.range(start=0, limit=seq_len, delta=1)
        position_embeddings = layers.Embedding(
            self.max_position_encoding,
            self.d_model,
            name=f'{name_prefix}_position_embedding'
        )(positions)
        return x + position_embeddings

    def _add_relative_position_encoding(self, x, name_prefix='encoder'):
        """æ·»åŠ ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆæ›´é«˜çº§çš„ä½ç½®ç¼–ç ï¼‰"""
        seq_len = tf.shape(x)[1]

        # åˆ›å»ºç›¸å¯¹ä½ç½®çŸ©é˜µ
        positions = tf.range(seq_len)
        relative_positions = positions[:, None] - positions[None, :]

        # é™åˆ¶ç›¸å¯¹ä½ç½®çš„èŒƒå›´
        max_relative_position = 32
        relative_positions = tf.clip_by_value(
            relative_positions,
            -max_relative_position,
            max_relative_position
        )

        # ç›¸å¯¹ä½ç½®åµŒå…¥
        relative_position_embeddings = layers.Embedding(
            2 * max_relative_position + 1,
            self.d_model,
            name=f'{name_prefix}_relative_position_embedding'
        )(relative_positions + max_relative_position)

        # å°†ç›¸å¯¹ä½ç½®ç¼–ç æ·»åŠ åˆ°è¾“å…¥
        return x + tf.reduce_mean(relative_position_embeddings, axis=1, keepdims=True)

    def _sparse_multi_head_attention(self, query, key, layer_idx, layer_type):
        """ç¨€ç–å¤šå¤´æ³¨æ„åŠ›ï¼ˆå‡å°‘è®¡ç®—å¤æ‚åº¦ï¼‰"""
        # ç®€åŒ–çš„ç¨€ç–æ³¨æ„åŠ›å®ç°
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šå®ç°æ›´å¤æ‚çš„ç¨€ç–æ¨¡å¼

        # ä½¿ç”¨å±€éƒ¨çª—å£æ³¨æ„åŠ›ä½œä¸ºç¨€ç–æ³¨æ„åŠ›çš„ç®€åŒ–ç‰ˆæœ¬
        return self._local_multi_head_attention(query, key, layer_idx, layer_type)

    def _local_multi_head_attention(self, query, key, layer_idx, layer_type):
        """å±€éƒ¨å¤šå¤´æ³¨æ„åŠ›"""
        # åˆ›å»ºå±€éƒ¨æ³¨æ„åŠ›æ©ç 
        seq_len = tf.shape(query)[1]
        window_size = min(self.local_attention_window, seq_len)

        # ç®€åŒ–å®ç°ï¼šä½¿ç”¨æ ‡å‡†å¤šå¤´æ³¨æ„åŠ›ä½†é™åˆ¶æ³¨æ„åŠ›èŒƒå›´
        attention_output = layers.MultiHeadAttention(
            num_heads=self.num_heads,
            key_dim=self.d_model // self.num_heads,
            dropout=self.dropout_rate,
            name=f'{layer_type}_local_mha_{layer_idx}'
        )(query, key)

        return attention_output

    def _create_learning_rate_scheduler(self):
        """åˆ›å»ºTransformerä¸“ç”¨çš„å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        def scheduler(epoch, lr):
            # Transformerçš„é¢„çƒ­å­¦ä¹ ç‡è°ƒåº¦
            warmup_steps = 4000
            step = epoch + 1

            arg1 = tf.math.rsqrt(step)
            arg2 = step * (warmup_steps ** -1.5)

            return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)

        return LearningRateScheduler(scheduler, verbose=0)

    def _get_advanced_callbacks(self):
        """è·å–é«˜çº§å›è°ƒå‡½æ•°"""
        callbacks = [
            EarlyStopping(
                monitor='val_loss',
                patience=20,
                restore_best_weights=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.8,
                patience=10,
                min_lr=1e-8,
                verbose=1
            ),
            self._create_learning_rate_scheduler()
        ]

        return callbacks
    
    @handle_model_error
    def train(self, epochs=None, validation_split=0.2, batch_size=None):
        """
        è®­ç»ƒTransformeræ¨¡å‹
        
        Args:
            epochs: è®­ç»ƒè½®æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„å€¼
            validation_split: éªŒè¯é›†æ¯”ä¾‹
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„å€¼
            
        Returns:
            è®­ç»ƒæ˜¯å¦æˆåŠŸ
        """
        from .data_manager import DeepLearningDataManager
        from .training_utils import get_callbacks, TrainingVisualizer
        
        if epochs is None:
            epochs = self.config.get('epochs', 100)
        
        if batch_size is None:
            batch_size = self.config.get('batch_size', 64)
        
        logger_manager.info(f"å¼€å§‹è®­ç»ƒTransformeræ¨¡å‹: epochs={epochs}, batch_size={batch_size}")
        
        try:
            # åˆ›å»ºæ•°æ®ç®¡ç†å™¨
            data_manager = DeepLearningDataManager()
            
            # å‡†å¤‡æ‰¹å¤„ç†æ•°æ®
            batch_data = data_manager.prepare_batch_data(
                sequence_length=self.sequence_length,
                batch_size=batch_size,
                validation_split=validation_split
            )
            
            # æ›´æ–°ç‰¹å¾ç»´åº¦
            self.feature_dim = batch_data['feature_dim']
            
            # æ„å»ºæ¨¡å‹
            if self.model is None:
                self.model = self._build_model()
            
            # è·å–å›è°ƒå‡½æ•°
            callbacks = get_callbacks(self.name, epochs)
            
            # è®­ç»ƒæ¨¡å‹
            history = self.model.fit(
                batch_data['train_dataset'],
                epochs=epochs,
                validation_data=batch_data['val_dataset'],
                callbacks=callbacks,
                verbose=0  # ä½¿ç”¨è‡ªå®šä¹‰è¿›åº¦æ¡ï¼Œç¦ç”¨TensorFlowçš„è¿›åº¦æ¡
            )
            
            self.is_trained = True
            
            # ä¿å­˜æ¨¡å‹
            self._save_model()
            
            # å¯è§†åŒ–è®­ç»ƒå†å²
            visualizer = TrainingVisualizer(self.name)
            visualizer.plot_training_history(history.history)
            
            logger_manager.info(f"{self.name}æ¨¡å‹è®­ç»ƒå®Œæˆ")
            
            return True
        
        except Exception as e:
            logger_manager.error(f"{self.name}æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return False
    
    @handle_model_error
    def predict(self, count=1, verbose=True) -> List[Tuple[List[int], List[int]]]:
        """
        ç”Ÿæˆé¢„æµ‹ç»“æœ
        
        Args:
            count: é¢„æµ‹æ³¨æ•°
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        Returns:
            é¢„æµ‹ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º(å‰åŒºå·ç åˆ—è¡¨, ååŒºå·ç åˆ—è¡¨)
        """
        from .prediction_utils import PredictionProcessor
        
        # å°è¯•åŠ è½½å·²æœ‰æ¨¡å‹
        if not self.is_trained:
            if not self._load_model():
                logger_manager.info(f"{self.name}æ¨¡å‹æœªè®­ç»ƒï¼Œå¼€å§‹è®­ç»ƒ...")
                if not self.train():
                    logger_manager.error(f"{self.name}æ¨¡å‹è®­ç»ƒå¤±è´¥")
                    return []
        
        # è·å–æœ€è¿‘çš„åºåˆ—æ•°æ®
        recent_features = self._extract_features(self.df.head(self.sequence_length))
        recent_scaled = self.scaler.transform(recent_features)
        
        # å‡†å¤‡è¾“å…¥åºåˆ—
        input_sequence = recent_scaled.reshape(1, self.sequence_length, self.feature_dim)
        
        # åˆ›å»ºé¢„æµ‹å¤„ç†å™¨
        processor = PredictionProcessor()
        
        predictions = []
        raw_predictions = []
        
        if verbose:
            logger_manager.info(f"ä½¿ç”¨{self.name}æ¨¡å‹ç”Ÿæˆ{count}æ³¨é¢„æµ‹...")
        
        for i in range(count):
            # é¢„æµ‹
            pred_scaled = self.model.predict(input_sequence, verbose=0)
            
            # åæ ‡å‡†åŒ–
            # åˆ›å»ºå®Œæ•´ç‰¹å¾å‘é‡ç”¨äºåæ ‡å‡†åŒ–
            full_pred = np.zeros((1, self.feature_dim))
            full_pred[0, :7] = pred_scaled[0]
            pred_original = self.scaler.inverse_transform(full_pred)[0, :7]
            
            # ä¿å­˜åŸå§‹é¢„æµ‹ç»“æœ
            raw_predictions.append(pred_original)
            
            # å¤„ç†é¢„æµ‹ç»“æœ
            front_balls, back_balls = processor.process_raw_prediction(pred_original)
            predictions.append((front_balls, back_balls))
            
            # æ›´æ–°è¾“å…¥åºåˆ—ç”¨äºä¸‹ä¸€æ¬¡é¢„æµ‹
            new_feature = np.concatenate([pred_original, recent_scaled[-1, 7:]])
            input_sequence = np.roll(input_sequence, -1, axis=1)
            input_sequence[0, -1] = new_feature
            
            if verbose:
                formatted = processor.format_prediction((front_balls, back_balls))
                logger_manager.info(f"é¢„æµ‹ {i+1}/{count}: {formatted}")
        
        # è®¡ç®—é¢„æµ‹ç½®ä¿¡åº¦
        confidence = processor.calculate_confidence(predictions)
        
        if verbose:
            logger_manager.info(f"{self.name}é¢„æµ‹å®Œæˆï¼Œç½®ä¿¡åº¦: {confidence:.2f}")
        
        return predictions
    
    def predict_with_details(self, count=1) -> Dict[str, Any]:
        """
        ç”Ÿæˆå¸¦è¯¦ç»†ä¿¡æ¯çš„é¢„æµ‹ç»“æœ
        
        Args:
            count: é¢„æµ‹æ³¨æ•°
            
        Returns:
            åŒ…å«é¢„æµ‹ç»“æœå’Œè¯¦ç»†ä¿¡æ¯çš„å­—å…¸
        """
        from .prediction_utils import PredictionProcessor
        
        # æ‰§è¡Œé¢„æµ‹
        predictions = self.predict(count, verbose=False)
        
        # åˆ›å»ºé¢„æµ‹å¤„ç†å™¨
        processor = PredictionProcessor()
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = processor.calculate_confidence(predictions)
        
        # æ ¼å¼åŒ–é¢„æµ‹ç»“æœ
        formatted_predictions = []
        for i, pred in enumerate(predictions):
            formatted = processor.format_prediction(pred)
            formatted_predictions.append({
                'index': i + 1,
                'front_balls': pred[0],
                'back_balls': pred[1],
                'formatted': formatted
            })
        
        # è¿”å›è¯¦ç»†ç»“æœ
        return {
            'model_name': self.name,
            'count': count,
            'predictions': formatted_predictions,
            'confidence': confidence,
            'model_config': {
                'd_model': self.d_model,
                'num_heads': self.num_heads,
                'num_layers': self.num_layers
            },
            'timestamp': datetime.now().isoformat()
        }
    
    def evaluate_predictions(self, predictions: List[Tuple[List[int], List[int]]], 
                           actuals: List[Tuple[List[int], List[int]]]) -> Dict[str, Any]:
        """
        è¯„ä¼°é¢„æµ‹ç»“æœ
        
        Args:
            predictions: é¢„æµ‹ç»“æœåˆ—è¡¨
            actuals: å®é™…ç»“æœåˆ—è¡¨
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        from .prediction_utils import PredictionEvaluator
        
        evaluator = PredictionEvaluator()
        return evaluator.evaluate_multiple_predictions(predictions, actuals)
    
    def get_confidence(self) -> float:
        """
        è·å–é¢„æµ‹ç½®ä¿¡åº¦
        
        Returns:
            ç½®ä¿¡åº¦åˆ†æ•° (0.0-1.0)
        """
        if not self.is_trained:
            return 0.0
        
        # åŸºäºæ¨¡å‹éªŒè¯æ€§èƒ½è®¡ç®—ç½®ä¿¡åº¦
        # è¿™é‡Œä½¿ç”¨ä¸€ä¸ªç®€å•çš„å¯å‘å¼æ–¹æ³•ï¼Œå®é™…åº”ç”¨ä¸­å¯ä»¥åŸºäºéªŒè¯é›†æ€§èƒ½
        base_confidence = 0.7
        
        # æ ¹æ®æ¨¡å‹å¤æ‚åº¦è°ƒæ•´
        complexity_factor = min(1.0, (self.num_layers * self.num_heads) / 40)
        
        # æ ¹æ®è®­ç»ƒæ•°æ®é‡è°ƒæ•´
        data_factor = min(1.0, len(self.df) / 1000)
        
        confidence = base_confidence * complexity_factor * data_factor
        
        return min(0.95, confidence)  # æœ€é«˜ç½®ä¿¡åº¦é™åˆ¶åœ¨0.95
    
    def use_fallback_config(self):
        """ä½¿ç”¨å¤‡ç”¨é…ç½®"""
        logger_manager.info("ä½¿ç”¨Transformerå¤‡ç”¨é…ç½®")
        
        # ç®€åŒ–æ¨¡å‹é…ç½®
        self.d_model = 64
        self.num_heads = 4
        self.num_layers = 2
        self.dff = 256
        self.dropout_rate = 0.2
        
        # æ›´æ–°é…ç½®å­—å…¸
        self.config.update({
            'd_model': self.d_model,
            'num_heads': self.num_heads,
            'num_layers': self.num_layers,
            'dff': self.dff,
            'dropout_rate': self.dropout_rate
        })
    
    def use_simple_model(self):
        """ä½¿ç”¨ç®€å•æ¨¡å‹"""
        logger_manager.info("ä½¿ç”¨ç®€å•Transformeræ¨¡å‹")
        
        # æç®€é…ç½®
        self.d_model = 32
        self.num_heads = 2
        self.num_layers = 1
        self.dff = 128
        self.dropout_rate = 0.1
        
        # æ›´æ–°é…ç½®å­—å…¸
        self.config.update({
            'd_model': self.d_model,
            'num_heads': self.num_heads,
            'num_layers': self.num_layers,
            'dff': self.dff,
            'dropout_rate': self.dropout_rate
        })


if __name__ == "__main__":
    # æµ‹è¯•Transformeré¢„æµ‹å™¨
    print("ğŸ§  æµ‹è¯•Transformeré¢„æµ‹å™¨...")
    
    # åˆ›å»ºé¢„æµ‹å™¨
    transformer = TransformerPredictor()
    
    # è®­ç»ƒæ¨¡å‹
    transformer.train(epochs=10)
    
    # è¿›è¡Œé¢„æµ‹
    predictions = transformer.predict(3)
    
    print("Transformeré¢„æµ‹ç»“æœ:")
    for i, (front, back) in enumerate(predictions):
        front_str = ' '.join([str(b).zfill(2) for b in front])
        back_str = ' '.join([str(b).zfill(2) for b in back])
        print(f"ç¬¬ {i+1} æ³¨: {front_str} + {back_str}")