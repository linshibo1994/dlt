#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ¨¡å‹ä¼˜åŒ–å™¨
æä¾›æ¨¡å‹é‡åŒ–ã€ä¸­é—´ç»“æœç¼“å­˜å’Œèµ„æºä½¿ç”¨ç›‘æ§åŠŸèƒ½
"""

import os
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Any, Optional, Union, Callable
import time
import psutil
import functools
import hashlib
import pickle
import json
from pathlib import Path

from core_modules import logger_manager, cache_manager


class ModelOptimizer:
    """æ¨¡å‹ä¼˜åŒ–å™¨"""
    
    def __init__(self, cache_dir: Optional[str] = None):
        """
        åˆå§‹åŒ–æ¨¡å‹ä¼˜åŒ–å™¨
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤ç¼“å­˜ç›®å½•
        """
        self.cache_dir = cache_dir or os.path.join(cache_manager.get_cache_dir(), 'model_optimizer')
        self._ensure_cache_dir()
        
        # èµ„æºç›‘æ§
        self.resource_usage = {
            'cpu': [],
            'memory': [],
            'disk': [],
            'time': []
        }
        
        logger_manager.info("æ¨¡å‹ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _ensure_cache_dir(self) -> None:
        """ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨"""
        os.makedirs(self.cache_dir, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        for subdir in ['quantized_models', 'result_cache']:
            os.makedirs(os.path.join(self.cache_dir, subdir), exist_ok=True)
    
    def quantize_tensorflow_model(self, model: Any, quantization_type: str = 'float16') -> Any:
        """
        é‡åŒ–TensorFlowæ¨¡å‹
        
        Args:
            model: TensorFlowæ¨¡å‹
            quantization_type: é‡åŒ–ç±»å‹ï¼Œæ”¯æŒ'float16', 'int8', 'dynamic'
            
        Returns:
            é‡åŒ–åçš„æ¨¡å‹
        """
        try:
            import tensorflow as tf
            
            # æ£€æŸ¥æ¨¡å‹ç±»å‹
            if not isinstance(model, tf.keras.Model):
                logger_manager.warning("ä¸æ˜¯æœ‰æ•ˆçš„TensorFlowæ¨¡å‹ï¼Œæ— æ³•é‡åŒ–")
                return model
            
            # æ ¹æ®é‡åŒ–ç±»å‹è¿›è¡Œé‡åŒ–
            if quantization_type == 'float16':
                # ä½¿ç”¨æ··åˆç²¾åº¦ç­–ç•¥
                policy = tf.keras.mixed_precision.Policy('mixed_float16')
                tf.keras.mixed_precision.set_global_policy(policy)
                
                # å…‹éš†æ¨¡å‹å¹¶è½¬æ¢ä¸ºfloat16
                quantized_model = tf.keras.models.clone_model(model)
                quantized_model.set_weights(model.get_weights())
                
                logger_manager.info("TensorFlowæ¨¡å‹å·²é‡åŒ–ä¸ºfloat16")
                return quantized_model
            
            elif quantization_type == 'int8':
                # ä¿å­˜åŸå§‹æ¨¡å‹
                temp_model_path = os.path.join(self.cache_dir, 'temp_model')
                model.save(temp_model_path)
                
                # ä½¿ç”¨TFLiteè½¬æ¢å™¨è¿›è¡Œint8é‡åŒ–
                converter = tf.lite.TFLiteConverter.from_saved_model(temp_model_path)
                converter.optimizations = [tf.lite.Optimize.DEFAULT]
                converter.target_spec.supported_types = [tf.int8]
                
                # è½¬æ¢æ¨¡å‹
                tflite_model = converter.convert()
                
                # ä¿å­˜é‡åŒ–åçš„æ¨¡å‹
                quantized_model_path = os.path.join(self.cache_dir, 'quantized_models', 'model_int8.tflite')
                with open(quantized_model_path, 'wb') as f:
                    f.write(tflite_model)
                
                logger_manager.info(f"TensorFlowæ¨¡å‹å·²é‡åŒ–ä¸ºint8ï¼Œä¿å­˜åˆ° {quantized_model_path}")
                
                # è¿”å›åŸå§‹æ¨¡å‹ï¼ˆå› ä¸ºTFLiteæ¨¡å‹éœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
                return model
            
            elif quantization_type == 'dynamic':
                # ä¿å­˜åŸå§‹æ¨¡å‹
                temp_model_path = os.path.join(self.cache_dir, 'temp_model')
                model.save(temp_model_path)
                
                # ä½¿ç”¨TFLiteè½¬æ¢å™¨è¿›è¡ŒåŠ¨æ€èŒƒå›´é‡åŒ–
                converter = tf.lite.TFLiteConverter.from_saved_model(temp_model_path)
                converter.optimizations = [tf.lite.Optimize.DEFAULT]
                
                # è½¬æ¢æ¨¡å‹
                tflite_model = converter.convert()
                
                # ä¿å­˜é‡åŒ–åçš„æ¨¡å‹
                quantized_model_path = os.path.join(self.cache_dir, 'quantized_models', 'model_dynamic.tflite')
                with open(quantized_model_path, 'wb') as f:
                    f.write(tflite_model)
                
                logger_manager.info(f"TensorFlowæ¨¡å‹å·²åŠ¨æ€é‡åŒ–ï¼Œä¿å­˜åˆ° {quantized_model_path}")
                
                # è¿”å›åŸå§‹æ¨¡å‹ï¼ˆå› ä¸ºTFLiteæ¨¡å‹éœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
                return model
            
            else:
                logger_manager.warning(f"æœªçŸ¥çš„é‡åŒ–ç±»å‹: {quantization_type}")
                return model
        
        except Exception as e:
            logger_manager.error(f"TensorFlowæ¨¡å‹é‡åŒ–å¤±è´¥: {e}")
            return model
    
    def quantize_pytorch_model(self, model: Any, quantization_type: str = 'float16') -> Any:
        """
        é‡åŒ–PyTorchæ¨¡å‹
        
        Args:
            model: PyTorchæ¨¡å‹
            quantization_type: é‡åŒ–ç±»å‹ï¼Œæ”¯æŒ'float16', 'int8', 'dynamic'
            
        Returns:
            é‡åŒ–åçš„æ¨¡å‹
        """
        try:
            import torch
            
            # æ£€æŸ¥æ¨¡å‹ç±»å‹
            if not isinstance(model, torch.nn.Module):
                logger_manager.warning("ä¸æ˜¯æœ‰æ•ˆçš„PyTorchæ¨¡å‹ï¼Œæ— æ³•é‡åŒ–")
                return model
            
            # æ ¹æ®é‡åŒ–ç±»å‹è¿›è¡Œé‡åŒ–
            if quantization_type == 'float16':
                # è½¬æ¢ä¸ºfloat16
                model_fp16 = model.half()
                
                logger_manager.info("PyTorchæ¨¡å‹å·²é‡åŒ–ä¸ºfloat16")
                return model_fp16
            
            elif quantization_type == 'int8':
                # é™æ€é‡åŒ–
                model_int8 = torch.quantization.quantize_dynamic(
                    model, {torch.nn.Linear, torch.nn.Conv2d}, dtype=torch.qint8
                )
                
                logger_manager.info("PyTorchæ¨¡å‹å·²é‡åŒ–ä¸ºint8")
                return model_int8
            
            elif quantization_type == 'dynamic':
                # åŠ¨æ€é‡åŒ–
                model_dynamic = torch.quantization.quantize_dynamic(
                    model, {torch.nn.Linear}, dtype=torch.qint8
                )
                
                logger_manager.info("PyTorchæ¨¡å‹å·²åŠ¨æ€é‡åŒ–")
                return model_dynamic
            
            else:
                logger_manager.warning(f"æœªçŸ¥çš„é‡åŒ–ç±»å‹: {quantization_type}")
                return model
        
        except Exception as e:
            logger_manager.error(f"PyTorchæ¨¡å‹é‡åŒ–å¤±è´¥: {e}")
            return model
    
    def quantize_model(self, model: Any, framework: str = 'tensorflow', 
                     quantization_type: str = 'float16') -> Any:
        """
        é‡åŒ–æ¨¡å‹
        
        Args:
            model: æ¨¡å‹å¯¹è±¡
            framework: æ¡†æ¶åç§°ï¼Œæ”¯æŒ'tensorflow', 'pytorch'
            quantization_type: é‡åŒ–ç±»å‹
            
        Returns:
            é‡åŒ–åçš„æ¨¡å‹
        """
        if framework == 'tensorflow':
            return self.quantize_tensorflow_model(model, quantization_type)
        elif framework == 'pytorch':
            return self.quantize_pytorch_model(model, quantization_type)
        else:
            logger_manager.warning(f"æœªçŸ¥çš„æ¡†æ¶: {framework}")
            return model
    
    def cache_result(self, func: Callable) -> Callable:
        """
        ç¼“å­˜å‡½æ•°ç»“æœçš„è£…é¥°å™¨
        
        Args:
            func: è¦ç¼“å­˜ç»“æœçš„å‡½æ•°
            
        Returns:
            è£…é¥°åçš„å‡½æ•°
        """
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = self._generate_cache_key(func.__name__, args, kwargs)
            cache_file = os.path.join(self.cache_dir, 'result_cache', f"{cache_key}.pkl")
            
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨
            if os.path.exists(cache_file):
                try:
                    with open(cache_file, 'rb') as f:
                        result = pickle.load(f)
                    logger_manager.debug(f"ä»ç¼“å­˜åŠ è½½ç»“æœ: {func.__name__}")
                    return result
                except Exception as e:
                    logger_manager.warning(f"åŠ è½½ç¼“å­˜ç»“æœå¤±è´¥: {e}")
            
            # æ‰§è¡Œå‡½æ•°
            result = func(*args, **kwargs)
            
            # ç¼“å­˜ç»“æœ
            try:
                with open(cache_file, 'wb') as f:
                    pickle.dump(result, f)
                logger_manager.debug(f"ç¼“å­˜å‡½æ•°ç»“æœ: {func.__name__}")
            except Exception as e:
                logger_manager.warning(f"ç¼“å­˜ç»“æœå¤±è´¥: {e}")
            
            return result
        
        return wrapper
    
    def _generate_cache_key(self, func_name: str, args: tuple, kwargs: dict) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®
        
        Args:
            func_name: å‡½æ•°åç§°
            args: ä½ç½®å‚æ•°
            kwargs: å…³é”®å­—å‚æ•°
            
        Returns:
            ç¼“å­˜é”®
        """
        # åˆ›å»ºå¯å“ˆå¸Œçš„å‚æ•°è¡¨ç¤º
        def make_hashable(obj):
            if isinstance(obj, (str, int, float, bool, type(None))):
                return obj
            elif isinstance(obj, (list, tuple)):
                return tuple(make_hashable(x) for x in obj)
            elif isinstance(obj, dict):
                return tuple(sorted((k, make_hashable(v)) for k, v in obj.items()))
            elif isinstance(obj, np.ndarray):
                return obj.tobytes()
            else:
                return str(obj)
        
        # ç»„åˆå‡½æ•°åå’Œå‚æ•°
        key_parts = [func_name]
        key_parts.extend([make_hashable(arg) for arg in args])
        key_parts.extend([f"{k}={make_hashable(v)}" for k, v in sorted(kwargs.items())])
        
        # ç”Ÿæˆå“ˆå¸Œ
        key_str = str(key_parts).encode('utf-8')
        return hashlib.md5(key_str).hexdigest()
    
    def clear_result_cache(self) -> None:
        """æ¸…é™¤ç»“æœç¼“å­˜"""
        cache_dir = os.path.join(self.cache_dir, 'result_cache')
        
        # åˆ é™¤ç¼“å­˜æ–‡ä»¶
        for file_path in Path(cache_dir).glob('*.pkl'):
            try:
                os.remove(file_path)
            except Exception as e:
                logger_manager.warning(f"åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
        
        logger_manager.info("ç»“æœç¼“å­˜å·²æ¸…é™¤")
    
    def start_resource_monitoring(self) -> None:
        """å¼€å§‹èµ„æºç›‘æ§"""
        # æ¸…ç©ºå†å²è®°å½•
        self.resource_usage = {
            'cpu': [],
            'memory': [],
            'disk': [],
            'time': []
        }
        
        logger_manager.info("èµ„æºç›‘æ§å·²å¼€å§‹")
    
    def record_resource_usage(self) -> Dict[str, float]:
        """
        è®°å½•èµ„æºä½¿ç”¨æƒ…å†µ
        
        Returns:
            å½“å‰èµ„æºä½¿ç”¨æƒ…å†µ
        """
        # è·å–CPUä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent()
        
        # è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
        memory = psutil.virtual_memory()
        memory_percent = memory.percent
        
        # è·å–ç£ç›˜ä½¿ç”¨æƒ…å†µ
        disk = psutil.disk_usage('/')
        disk_percent = disk.percent
        
        # è®°å½•æ—¶é—´
        current_time = time.time()
        
        # è®°å½•èµ„æºä½¿ç”¨æƒ…å†µ
        self.resource_usage['cpu'].append(cpu_percent)
        self.resource_usage['memory'].append(memory_percent)
        self.resource_usage['disk'].append(disk_percent)
        self.resource_usage['time'].append(current_time)
        
        # è¿”å›å½“å‰èµ„æºä½¿ç”¨æƒ…å†µ
        current_usage = {
            'cpu': cpu_percent,
            'memory': memory_percent,
            'disk': disk_percent,
            'time': current_time
        }
        
        return current_usage
    
    def get_resource_usage_summary(self) -> Dict[str, Dict[str, float]]:
        """
        è·å–èµ„æºä½¿ç”¨æƒ…å†µæ‘˜è¦
        
        Returns:
            èµ„æºä½¿ç”¨æƒ…å†µæ‘˜è¦
        """
        summary = {}
        
        for resource, values in self.resource_usage.items():
            if resource != 'time' and values:
                summary[resource] = {
                    'min': min(values),
                    'max': max(values),
                    'avg': sum(values) / len(values),
                    'current': values[-1] if values else 0
                }
        
        return summary
    
    def save_resource_usage_report(self, file_path: Optional[str] = None) -> str:
        """
        ä¿å­˜èµ„æºä½¿ç”¨æƒ…å†µæŠ¥å‘Š
        
        Args:
            file_path: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
            
        Returns:
            æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        # è®¾ç½®é»˜è®¤æ–‡ä»¶è·¯å¾„
        if file_path is None:
            timestamp = time.strftime('%Y%m%d_%H%M%S')
            file_path = os.path.join(self.cache_dir, f'resource_usage_{timestamp}.json')
        
        # è·å–èµ„æºä½¿ç”¨æƒ…å†µæ‘˜è¦
        summary = self.get_resource_usage_summary()
        
        # æ·»åŠ è¯¦ç»†æ•°æ®
        report = {
            'summary': summary,
            'details': {
                'cpu': self.resource_usage['cpu'],
                'memory': self.resource_usage['memory'],
                'disk': self.resource_usage['disk'],
                'time': [t - self.resource_usage['time'][0] for t in self.resource_usage['time']]
            }
        }
        
        # ä¿å­˜æŠ¥å‘Š
        try:
            with open(file_path, 'w') as f:
                json.dump(report, f, indent=2)
            logger_manager.info(f"èµ„æºä½¿ç”¨æƒ…å†µæŠ¥å‘Šå·²ä¿å­˜åˆ° {file_path}")
        except Exception as e:
            logger_manager.error(f"ä¿å­˜èµ„æºä½¿ç”¨æƒ…å†µæŠ¥å‘Šå¤±è´¥: {e}")
        
        return file_path
    
    def optimize_model_inference(self, model: Any, framework: str = 'tensorflow') -> Any:
        """
        ä¼˜åŒ–æ¨¡å‹æ¨ç†
        
        Args:
            model: æ¨¡å‹å¯¹è±¡
            framework: æ¡†æ¶åç§°ï¼Œæ”¯æŒ'tensorflow', 'pytorch'
            
        Returns:
            ä¼˜åŒ–åçš„æ¨¡å‹
        """
        if framework == 'tensorflow':
            try:
                import tensorflow as tf
                
                # æ£€æŸ¥æ¨¡å‹ç±»å‹
                if not isinstance(model, tf.keras.Model):
                    logger_manager.warning("ä¸æ˜¯æœ‰æ•ˆçš„TensorFlowæ¨¡å‹ï¼Œæ— æ³•ä¼˜åŒ–")
                    return model
                
                # è½¬æ¢ä¸ºSavedModelæ ¼å¼
                temp_model_path = os.path.join(self.cache_dir, 'temp_optimized_model')
                model.save(temp_model_path)
                
                # åŠ è½½å¹¶ä¼˜åŒ–æ¨¡å‹
                optimized_model = tf.saved_model.load(temp_model_path)
                
                # åˆ›å»ºä¼˜åŒ–å‡½æ•°
                @tf.function
                def optimized_predict(x):
                    return optimized_model(x)
                
                # è¿”å›ä¼˜åŒ–åçš„æ¨¡å‹å’Œé¢„æµ‹å‡½æ•°
                logger_manager.info("TensorFlowæ¨¡å‹æ¨ç†å·²ä¼˜åŒ–")
                return optimized_model, optimized_predict
            
            except Exception as e:
                logger_manager.error(f"ä¼˜åŒ–TensorFlowæ¨¡å‹æ¨ç†å¤±è´¥: {e}")
                return model, None
        
        elif framework == 'pytorch':
            try:
                import torch
                
                # æ£€æŸ¥æ¨¡å‹ç±»å‹
                if not isinstance(model, torch.nn.Module):
                    logger_manager.warning("ä¸æ˜¯æœ‰æ•ˆçš„PyTorchæ¨¡å‹ï¼Œæ— æ³•ä¼˜åŒ–")
                    return model
                
                # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
                model.eval()
                
                # ä½¿ç”¨JITç¼–è¯‘
                example_input = torch.randn(1, *model.input_shape[1:])
                optimized_model = torch.jit.trace(model, example_input)
                
                logger_manager.info("PyTorchæ¨¡å‹æ¨ç†å·²ä¼˜åŒ–")
                return optimized_model
            
            except Exception as e:
                logger_manager.error(f"ä¼˜åŒ–PyTorchæ¨¡å‹æ¨ç†å¤±è´¥: {e}")
                return model
        
        else:
            logger_manager.warning(f"æœªçŸ¥çš„æ¡†æ¶: {framework}")
            return model


class CachedModel:
    """ç¼“å­˜æ¨¡å‹ç»“æœ"""
    
    def __init__(self, model: Any, optimizer: ModelOptimizer, cache_size: int = 100):
        """
        åˆå§‹åŒ–ç¼“å­˜æ¨¡å‹
        
        Args:
            model: æ¨¡å‹å¯¹è±¡
            optimizer: æ¨¡å‹ä¼˜åŒ–å™¨
            cache_size: ç¼“å­˜å¤§å°
        """
        self.model = model
        self.optimizer = optimizer
        self.cache_size = cache_size
        self.cache = {}
        self.cache_hits = 0
        self.cache_misses = 0
        
        logger_manager.info(f"ç¼“å­˜æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œç¼“å­˜å¤§å°: {cache_size}")
    
    def predict(self, inputs: np.ndarray) -> np.ndarray:
        """
        é¢„æµ‹
        
        Args:
            inputs: è¾“å…¥æ•°æ®
            
        Returns:
            é¢„æµ‹ç»“æœ
        """
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self._generate_cache_key(inputs)
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self.cache:
            self.cache_hits += 1
            return self.cache[cache_key]
        
        # æ‰§è¡Œé¢„æµ‹
        result = self.model.predict(inputs)
        
        # æ›´æ–°ç¼“å­˜
        self.cache_misses += 1
        self._update_cache(cache_key, result)
        
        return result
    
    def _generate_cache_key(self, inputs: np.ndarray) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®
        
        Args:
            inputs: è¾“å…¥æ•°æ®
            
        Returns:
            ç¼“å­˜é”®
        """
        # ä½¿ç”¨è¾“å…¥æ•°æ®çš„å“ˆå¸Œä½œä¸ºç¼“å­˜é”®
        return hashlib.md5(inputs.tobytes()).hexdigest()
    
    def _update_cache(self, key: str, value: np.ndarray) -> None:
        """
        æ›´æ–°ç¼“å­˜
        
        Args:
            key: ç¼“å­˜é”®
            value: ç¼“å­˜å€¼
        """
        # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œåˆ é™¤æœ€æ—©çš„é¡¹
        if len(self.cache) >= self.cache_size:
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]
        
        # æ·»åŠ æ–°é¡¹
        self.cache[key] = value
    
    def get_cache_stats(self) -> Dict[str, int]:
        """
        è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        """
        total_requests = self.cache_hits + self.cache_misses
        hit_rate = self.cache_hits / total_requests if total_requests > 0 else 0
        
        return {
            'cache_size': len(self.cache),
            'max_cache_size': self.cache_size,
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses,
            'hit_rate': hit_rate
        }
    
    def clear_cache(self) -> None:
        """æ¸…é™¤ç¼“å­˜"""
        self.cache = {}
        logger_manager.info("æ¨¡å‹ç¼“å­˜å·²æ¸…é™¤")


if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å‹ä¼˜åŒ–å™¨
    print("ğŸš€ æµ‹è¯•æ¨¡å‹ä¼˜åŒ–å™¨...")
    
    # åˆ›å»ºæ¨¡å‹ä¼˜åŒ–å™¨
    optimizer = ModelOptimizer()
    
    # æµ‹è¯•èµ„æºç›‘æ§
    print("\næµ‹è¯•èµ„æºç›‘æ§...")
    optimizer.start_resource_monitoring()
    
    # æ¨¡æ‹Ÿä¸€äº›æ“ä½œ
    for _ in range(5):
        time.sleep(0.5)
        usage = optimizer.record_resource_usage()
        print(f"CPU: {usage['cpu']}%, å†…å­˜: {usage['memory']}%, ç£ç›˜: {usage['disk']}%")
    
    # è·å–èµ„æºä½¿ç”¨æƒ…å†µæ‘˜è¦
    summary = optimizer.get_resource_usage_summary()
    print(f"\nèµ„æºä½¿ç”¨æƒ…å†µæ‘˜è¦: {summary}")
    
    # ä¿å­˜èµ„æºä½¿ç”¨æƒ…å†µæŠ¥å‘Š
    report_path = optimizer.save_resource_usage_report()
    print(f"èµ„æºä½¿ç”¨æƒ…å†µæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    
    # æµ‹è¯•ç»“æœç¼“å­˜è£…é¥°å™¨
    print("\næµ‹è¯•ç»“æœç¼“å­˜è£…é¥°å™¨...")
    
    @optimizer.cache_result
    def expensive_calculation(x):
        print("æ‰§è¡Œæ˜‚è´µè®¡ç®—...")
        time.sleep(1)
        return x * 2
    
    # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆåº”è¯¥æ‰§è¡Œè®¡ç®—ï¼‰
    result1 = expensive_calculation(10)
    print(f"ç»“æœ1: {result1}")
    
    # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆåº”è¯¥ä»ç¼“å­˜åŠ è½½ï¼‰
    result2 = expensive_calculation(10)
    print(f"ç»“æœ2: {result2}")
    
    # ä¸åŒå‚æ•°çš„è°ƒç”¨ï¼ˆåº”è¯¥æ‰§è¡Œè®¡ç®—ï¼‰
    result3 = expensive_calculation(20)
    print(f"ç»“æœ3: {result3}")
    
    # æ¸…é™¤ç»“æœç¼“å­˜
    optimizer.clear_result_cache()
    
    print("æ¨¡å‹ä¼˜åŒ–å™¨æµ‹è¯•å®Œæˆ")