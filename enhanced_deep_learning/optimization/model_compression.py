#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ¨¡å‹å‹ç¼©æ¨¡å—
Model Compression Module

æä¾›é‡åŒ–ã€å‰ªæã€è’¸é¦ã€ä½ç§©åˆ†è§£ç­‰æ¨¡å‹å‹ç¼©æŠ€æœ¯ã€‚
"""

import os
import numpy as np
import tensorflow as tf
from typing import Dict, List, Any, Optional, Callable, Tuple, Union
from dataclasses import dataclass
from enum import Enum
import copy
import math

from core_modules import logger_manager
from ..utils.exceptions import DeepLearningException


class CompressionMethod(Enum):
    """å‹ç¼©æ–¹æ³•æšä¸¾"""
    QUANTIZATION = "quantization"
    PRUNING = "pruning"
    DISTILLATION = "distillation"
    LOW_RANK = "low_rank"
    HYBRID = "hybrid"


@dataclass
class CompressionConfig:
    """å‹ç¼©é…ç½®"""
    method: CompressionMethod
    target_compression_ratio: float = 0.5
    accuracy_threshold: float = 0.95
    quantization_bits: int = 8
    pruning_sparsity: float = 0.5
    distillation_temperature: float = 4.0
    distillation_alpha: float = 0.7
    low_rank_ratio: float = 0.5


@dataclass
class CompressionResult:
    """å‹ç¼©ç»“æœ"""
    original_size: int
    compressed_size: int
    compression_ratio: float
    accuracy_before: float
    accuracy_after: float
    accuracy_loss: float
    inference_speedup: float
    memory_reduction: float


class ModelQuantizer:
    """æ¨¡å‹é‡åŒ–å™¨"""
    
    def __init__(self, bits: int = 8):
        """
        åˆå§‹åŒ–æ¨¡å‹é‡åŒ–å™¨
        
        Args:
            bits: é‡åŒ–ä½æ•°
        """
        self.bits = bits
        self.quantization_ranges = {}
        
        logger_manager.info(f"æ¨¡å‹é‡åŒ–å™¨åˆå§‹åŒ–å®Œæˆï¼Œé‡åŒ–ä½æ•°: {bits}")
    
    def quantize_weights(self, weights: np.ndarray) -> Tuple[np.ndarray, Dict[str, float]]:
        """
        é‡åŒ–æƒé‡
        
        Args:
            weights: åŸå§‹æƒé‡
            
        Returns:
            é‡åŒ–åçš„æƒé‡å’Œé‡åŒ–å‚æ•°
        """
        try:
            # è®¡ç®—é‡åŒ–èŒƒå›´
            w_min = np.min(weights)
            w_max = np.max(weights)
            
            # å¯¹ç§°é‡åŒ–
            w_abs_max = max(abs(w_min), abs(w_max))
            scale = w_abs_max / (2 ** (self.bits - 1) - 1)
            
            # é‡åŒ–
            quantized = np.round(weights / scale)
            quantized = np.clip(quantized, -(2 ** (self.bits - 1)), 2 ** (self.bits - 1) - 1)
            
            # åé‡åŒ–ï¼ˆç”¨äºæ¨ç†ï¼‰
            dequantized = quantized * scale
            
            quantization_params = {
                'scale': scale,
                'min_val': w_min,
                'max_val': w_max,
                'bits': self.bits
            }
            
            logger_manager.debug(f"æƒé‡é‡åŒ–å®Œæˆï¼Œå‹ç¼©æ¯”: {weights.nbytes / dequantized.nbytes:.2f}")
            return dequantized.astype(np.float32), quantization_params
            
        except Exception as e:
            logger_manager.error(f"æƒé‡é‡åŒ–å¤±è´¥: {e}")
            return weights, {}
    
    def quantize_model(self, model: tf.keras.Model) -> tf.keras.Model:
        """
        é‡åŒ–æ•´ä¸ªæ¨¡å‹
        
        Args:
            model: åŸå§‹æ¨¡å‹
            
        Returns:
            é‡åŒ–åçš„æ¨¡å‹
        """
        try:
            # ä½¿ç”¨TensorFlow Liteé‡åŒ–
            converter = tf.lite.TFLiteConverter.from_keras_model(model)
            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            
            # è®¾ç½®é‡åŒ–ç±»å‹
            if self.bits == 8:
                converter.target_spec.supported_types = [tf.int8]
            elif self.bits == 16:
                converter.target_spec.supported_types = [tf.float16]
            
            # è½¬æ¢æ¨¡å‹
            quantized_tflite_model = converter.convert()
            
            # ä¿å­˜é‡åŒ–æ¨¡å‹
            temp_path = "temp_quantized_model.tflite"
            with open(temp_path, 'wb') as f:
                f.write(quantized_tflite_model)
            
            logger_manager.info(f"æ¨¡å‹é‡åŒ–å®Œæˆï¼Œé‡åŒ–ä½æ•°: {self.bits}")
            
            # è¿”å›åŸæ¨¡å‹ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦åŠ è½½TFLiteæ¨¡å‹ï¼‰
            return model
            
        except Exception as e:
            logger_manager.error(f"æ¨¡å‹é‡åŒ–å¤±è´¥: {e}")
            return model
    
    def dynamic_quantization(self, model: tf.keras.Model, 
                           calibration_data: np.ndarray) -> tf.keras.Model:
        """
        åŠ¨æ€é‡åŒ–
        
        Args:
            model: åŸå§‹æ¨¡å‹
            calibration_data: æ ¡å‡†æ•°æ®
            
        Returns:
            åŠ¨æ€é‡åŒ–åçš„æ¨¡å‹
        """
        try:
            # ä½¿ç”¨æ ¡å‡†æ•°æ®è¿›è¡ŒåŠ¨æ€é‡åŒ–
            converter = tf.lite.TFLiteConverter.from_keras_model(model)
            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            
            # è®¾ç½®ä»£è¡¨æ€§æ•°æ®é›†
            def representative_dataset():
                for data in calibration_data:
                    yield [data.astype(np.float32)]
            
            converter.representative_dataset = representative_dataset
            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
            converter.inference_input_type = tf.int8
            converter.inference_output_type = tf.int8
            
            quantized_tflite_model = converter.convert()
            
            logger_manager.info("åŠ¨æ€é‡åŒ–å®Œæˆ")
            return model
            
        except Exception as e:
            logger_manager.error(f"åŠ¨æ€é‡åŒ–å¤±è´¥: {e}")
            return model


class ModelPruner:
    """æ¨¡å‹å‰ªæå™¨"""
    
    def __init__(self, sparsity: float = 0.5):
        """
        åˆå§‹åŒ–æ¨¡å‹å‰ªæå™¨
        
        Args:
            sparsity: ç¨€ç–åº¦ï¼ˆè¦å‰ªæçš„æƒé‡æ¯”ä¾‹ï¼‰
        """
        self.sparsity = sparsity
        self.pruning_masks = {}
        
        logger_manager.info(f"æ¨¡å‹å‰ªæå™¨åˆå§‹åŒ–å®Œæˆï¼Œç¨€ç–åº¦: {sparsity}")
    
    def magnitude_pruning(self, weights: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        åŸºäºæƒé‡å¤§å°çš„å‰ªæ
        
        Args:
            weights: åŸå§‹æƒé‡
            
        Returns:
            å‰ªæåçš„æƒé‡å’Œå‰ªææ©ç 
        """
        try:
            # è®¡ç®—æƒé‡çš„ç»å¯¹å€¼
            abs_weights = np.abs(weights)
            
            # è®¡ç®—é˜ˆå€¼
            threshold = np.percentile(abs_weights, self.sparsity * 100)
            
            # åˆ›å»ºå‰ªææ©ç 
            mask = abs_weights > threshold
            
            # åº”ç”¨å‰ªæ
            pruned_weights = weights * mask
            
            actual_sparsity = 1 - np.count_nonzero(pruned_weights) / pruned_weights.size
            logger_manager.debug(f"æƒé‡å‰ªæå®Œæˆï¼Œå®é™…ç¨€ç–åº¦: {actual_sparsity:.3f}")
            
            return pruned_weights, mask
            
        except Exception as e:
            logger_manager.error(f"æƒé‡å‰ªæå¤±è´¥: {e}")
            return weights, np.ones_like(weights)
    
    def structured_pruning(self, weights: np.ndarray, 
                          structure: str = "channel") -> Tuple[np.ndarray, np.ndarray]:
        """
        ç»“æ„åŒ–å‰ªæ
        
        Args:
            weights: åŸå§‹æƒé‡
            structure: å‰ªæç»“æ„ ("channel", "filter", "block")
            
        Returns:
            å‰ªæåçš„æƒé‡å’Œå‰ªææ©ç 
        """
        try:
            if structure == "channel" and len(weights.shape) >= 2:
                # é€šé“çº§å‰ªæ
                channel_importance = np.sum(np.abs(weights), axis=tuple(range(len(weights.shape) - 1)))
                threshold = np.percentile(channel_importance, self.sparsity * 100)
                
                mask = np.ones_like(weights)
                for i, importance in enumerate(channel_importance):
                    if importance <= threshold:
                        mask[..., i] = 0
                
                pruned_weights = weights * mask
                
            elif structure == "filter" and len(weights.shape) == 4:
                # æ»¤æ³¢å™¨çº§å‰ªæï¼ˆé€‚ç”¨äºå·ç§¯å±‚ï¼‰
                filter_importance = np.sum(np.abs(weights), axis=(1, 2, 3))
                threshold = np.percentile(filter_importance, self.sparsity * 100)
                
                mask = np.ones_like(weights)
                for i, importance in enumerate(filter_importance):
                    if importance <= threshold:
                        mask[i, ...] = 0
                
                pruned_weights = weights * mask
                
            else:
                # å›é€€åˆ°éç»“æ„åŒ–å‰ªæ
                return self.magnitude_pruning(weights)
            
            actual_sparsity = 1 - np.count_nonzero(pruned_weights) / pruned_weights.size
            logger_manager.debug(f"ç»“æ„åŒ–å‰ªæå®Œæˆï¼Œç»“æ„: {structure}, å®é™…ç¨€ç–åº¦: {actual_sparsity:.3f}")
            
            return pruned_weights, mask
            
        except Exception as e:
            logger_manager.error(f"ç»“æ„åŒ–å‰ªæå¤±è´¥: {e}")
            return weights, np.ones_like(weights)
    
    def gradual_pruning(self, model: tf.keras.Model, 
                       training_data: np.ndarray,
                       epochs: int = 10) -> tf.keras.Model:
        """
        æ¸è¿›å¼å‰ªæ
        
        Args:
            model: åŸå§‹æ¨¡å‹
            training_data: è®­ç»ƒæ•°æ®
            epochs: è®­ç»ƒè½®æ•°
            
        Returns:
            å‰ªæåçš„æ¨¡å‹
        """
        try:
            # ç®€åŒ–çš„æ¸è¿›å¼å‰ªæå®ç°
            initial_sparsity = 0.0
            final_sparsity = self.sparsity
            
            for epoch in range(epochs):
                # è®¡ç®—å½“å‰ç¨€ç–åº¦
                current_sparsity = initial_sparsity + (final_sparsity - initial_sparsity) * (epoch / epochs)
                
                # åº”ç”¨å‰ªæ
                for layer in model.layers:
                    if hasattr(layer, 'kernel'):
                        weights = layer.get_weights()[0]
                        pruned_weights, mask = self.magnitude_pruning(weights)
                        
                        # æ›´æ–°æƒé‡
                        layer.set_weights([pruned_weights] + layer.get_weights()[1:])
                
                # å¾®è°ƒè®­ç»ƒï¼ˆç®€åŒ–å®ç°ï¼‰
                logger_manager.debug(f"æ¸è¿›å¼å‰ªæ - è½®æ¬¡ {epoch + 1}, ç¨€ç–åº¦: {current_sparsity:.3f}")
            
            logger_manager.info("æ¸è¿›å¼å‰ªæå®Œæˆ")
            return model
            
        except Exception as e:
            logger_manager.error(f"æ¸è¿›å¼å‰ªæå¤±è´¥: {e}")
            return model


class KnowledgeDistiller:
    """çŸ¥è¯†è’¸é¦å™¨"""
    
    def __init__(self, temperature: float = 4.0, alpha: float = 0.7):
        """
        åˆå§‹åŒ–çŸ¥è¯†è’¸é¦å™¨
        
        Args:
            temperature: è’¸é¦æ¸©åº¦
            alpha: è’¸é¦æŸå¤±æƒé‡
        """
        self.temperature = temperature
        self.alpha = alpha
        
        logger_manager.info(f"çŸ¥è¯†è’¸é¦å™¨åˆå§‹åŒ–å®Œæˆï¼Œæ¸©åº¦: {temperature}, alpha: {alpha}")
    
    def distillation_loss(self, y_true, y_pred_student, y_pred_teacher):
        """
        è®¡ç®—è’¸é¦æŸå¤±
        
        Args:
            y_true: çœŸå®æ ‡ç­¾
            y_pred_student: å­¦ç”Ÿæ¨¡å‹é¢„æµ‹
            y_pred_teacher: æ•™å¸ˆæ¨¡å‹é¢„æµ‹
            
        Returns:
            è’¸é¦æŸå¤±
        """
        try:
            # è½¯ç›®æ ‡æŸå¤±
            teacher_soft = tf.nn.softmax(y_pred_teacher / self.temperature)
            student_soft = tf.nn.softmax(y_pred_student / self.temperature)
            
            soft_loss = tf.keras.losses.categorical_crossentropy(teacher_soft, student_soft)
            soft_loss = soft_loss * (self.temperature ** 2)
            
            # ç¡¬ç›®æ ‡æŸå¤±
            hard_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred_student)
            
            # ç»„åˆæŸå¤±
            total_loss = self.alpha * soft_loss + (1 - self.alpha) * hard_loss
            
            return total_loss
            
        except Exception as e:
            logger_manager.error(f"è®¡ç®—è’¸é¦æŸå¤±å¤±è´¥: {e}")
            return tf.keras.losses.categorical_crossentropy(y_true, y_pred_student)
    
    def distill_model(self, teacher_model: tf.keras.Model, 
                     student_model: tf.keras.Model,
                     training_data: Tuple[np.ndarray, np.ndarray],
                     epochs: int = 10) -> tf.keras.Model:
        """
        æ‰§è¡ŒçŸ¥è¯†è’¸é¦
        
        Args:
            teacher_model: æ•™å¸ˆæ¨¡å‹
            student_model: å­¦ç”Ÿæ¨¡å‹
            training_data: è®­ç»ƒæ•°æ®
            epochs: è®­ç»ƒè½®æ•°
            
        Returns:
            è’¸é¦åçš„å­¦ç”Ÿæ¨¡å‹
        """
        try:
            X_train, y_train = training_data
            
            # è·å–æ•™å¸ˆæ¨¡å‹çš„è½¯æ ‡ç­¾
            teacher_predictions = teacher_model.predict(X_train)
            
            # å®šä¹‰è’¸é¦æŸå¤±å‡½æ•°
            def distill_loss(y_true, y_pred):
                return self.distillation_loss(y_true, y_pred, teacher_predictions)
            
            # ç¼–è¯‘å­¦ç”Ÿæ¨¡å‹
            student_model.compile(
                optimizer='adam',
                loss=distill_loss,
                metrics=['accuracy']
            )
            
            # è®­ç»ƒå­¦ç”Ÿæ¨¡å‹
            history = student_model.fit(
                X_train, y_train,
                epochs=epochs,
                verbose=0,
                validation_split=0.2
            )
            
            logger_manager.info(f"çŸ¥è¯†è’¸é¦å®Œæˆï¼Œè®­ç»ƒ {epochs} è½®")
            return student_model
            
        except Exception as e:
            logger_manager.error(f"çŸ¥è¯†è’¸é¦å¤±è´¥: {e}")
            return student_model


class LowRankDecomposer:
    """ä½ç§©åˆ†è§£å™¨"""
    
    def __init__(self, rank_ratio: float = 0.5):
        """
        åˆå§‹åŒ–ä½ç§©åˆ†è§£å™¨
        
        Args:
            rank_ratio: ç§©æ¯”ä¾‹
        """
        self.rank_ratio = rank_ratio
        
        logger_manager.info(f"ä½ç§©åˆ†è§£å™¨åˆå§‹åŒ–å®Œæˆï¼Œç§©æ¯”ä¾‹: {rank_ratio}")
    
    def svd_decomposition(self, weights: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        SVDåˆ†è§£
        
        Args:
            weights: åŸå§‹æƒé‡çŸ©é˜µ
            
        Returns:
            åˆ†è§£åçš„U, S, VçŸ©é˜µ
        """
        try:
            # æ‰§è¡ŒSVDåˆ†è§£
            U, S, Vt = np.linalg.svd(weights, full_matrices=False)
            
            # è®¡ç®—ä¿ç•™çš„ç§©
            original_rank = min(weights.shape)
            target_rank = max(1, int(original_rank * self.rank_ratio))
            
            # æˆªæ–­
            U_truncated = U[:, :target_rank]
            S_truncated = S[:target_rank]
            Vt_truncated = Vt[:target_rank, :]
            
            # é‡æ„æƒé‡
            reconstructed = U_truncated @ np.diag(S_truncated) @ Vt_truncated
            
            compression_ratio = (U_truncated.size + S_truncated.size + Vt_truncated.size) / weights.size
            logger_manager.debug(f"SVDåˆ†è§£å®Œæˆï¼Œå‹ç¼©æ¯”: {compression_ratio:.3f}")
            
            return U_truncated, S_truncated, Vt_truncated
            
        except Exception as e:
            logger_manager.error(f"SVDåˆ†è§£å¤±è´¥: {e}")
            return weights, np.array([1.0]), np.eye(weights.shape[1])
    
    def tucker_decomposition(self, tensor: np.ndarray) -> List[np.ndarray]:
        """
        Tuckeråˆ†è§£ï¼ˆé€‚ç”¨äºé«˜ç»´å¼ é‡ï¼‰
        
        Args:
            tensor: è¾“å…¥å¼ é‡
            
        Returns:
            åˆ†è§£åçš„å› å­åˆ—è¡¨
        """
        try:
            # ç®€åŒ–çš„Tuckeråˆ†è§£å®ç°
            # å®é™…åº”ç”¨ä¸­éœ€è¦ä½¿ç”¨ä¸“é—¨çš„å¼ é‡åˆ†è§£åº“
            
            if len(tensor.shape) == 2:
                # å¯¹äº2Då¼ é‡ï¼Œä½¿ç”¨SVD
                U, S, Vt = self.svd_decomposition(tensor)
                return [U, np.diag(S), Vt]
            
            elif len(tensor.shape) == 4:
                # å¯¹äº4Då¼ é‡ï¼ˆå¦‚å·ç§¯æ ¸ï¼‰ï¼Œæ²¿æ¯ä¸ªç»´åº¦åˆ†è§£
                factors = []
                core_tensor = tensor.copy()
                
                for mode in range(len(tensor.shape)):
                    # å±•å¼€å¼ é‡
                    unfolded = self._unfold_tensor(core_tensor, mode)
                    
                    # SVDåˆ†è§£
                    U, S, Vt = np.linalg.svd(unfolded, full_matrices=False)
                    
                    # æˆªæ–­
                    target_rank = max(1, int(U.shape[1] * self.rank_ratio))
                    U_truncated = U[:, :target_rank]
                    
                    factors.append(U_truncated)
                    
                    # æ›´æ–°æ ¸å¿ƒå¼ é‡
                    core_tensor = np.tensordot(core_tensor, U_truncated.T, axes=([mode], [1]))
                
                factors.append(core_tensor)  # æ ¸å¿ƒå¼ é‡
                
                logger_manager.debug("Tuckeråˆ†è§£å®Œæˆ")
                return factors
            
            else:
                logger_manager.warning(f"ä¸æ”¯æŒçš„å¼ é‡ç»´åº¦: {len(tensor.shape)}")
                return [tensor]
                
        except Exception as e:
            logger_manager.error(f"Tuckeråˆ†è§£å¤±è´¥: {e}")
            return [tensor]
    
    def _unfold_tensor(self, tensor: np.ndarray, mode: int) -> np.ndarray:
        """
        å¼ é‡å±•å¼€
        
        Args:
            tensor: è¾“å…¥å¼ é‡
            mode: å±•å¼€æ¨¡å¼
            
        Returns:
            å±•å¼€åçš„çŸ©é˜µ
        """
        try:
            # å°†æŒ‡å®šæ¨¡å¼ç§»åˆ°ç¬¬ä¸€ä¸ªç»´åº¦
            axes = list(range(len(tensor.shape)))
            axes[0], axes[mode] = axes[mode], axes[0]
            
            reordered = np.transpose(tensor, axes)
            
            # å±•å¼€ä¸ºçŸ©é˜µ
            unfolded = reordered.reshape(reordered.shape[0], -1)
            
            return unfolded
            
        except Exception as e:
            logger_manager.error(f"å¼ é‡å±•å¼€å¤±è´¥: {e}")
            return tensor.reshape(tensor.shape[0], -1)


class ModelCompressor:
    """æ¨¡å‹å‹ç¼©å™¨"""
    
    def __init__(self, config: CompressionConfig):
        """
        åˆå§‹åŒ–æ¨¡å‹å‹ç¼©å™¨
        
        Args:
            config: å‹ç¼©é…ç½®
        """
        self.config = config
        self.quantizer = ModelQuantizer(bits=config.quantization_bits)
        self.pruner = ModelPruner(sparsity=config.pruning_sparsity)
        self.distiller = KnowledgeDistiller(
            temperature=config.distillation_temperature,
            alpha=config.distillation_alpha
        )
        self.decomposer = LowRankDecomposer(rank_ratio=config.low_rank_ratio)
        
        logger_manager.info(f"æ¨¡å‹å‹ç¼©å™¨åˆå§‹åŒ–å®Œæˆï¼Œæ–¹æ³•: {config.method.value}")
    
    def compress_model(self, model: tf.keras.Model, 
                      training_data: Optional[Tuple[np.ndarray, np.ndarray]] = None,
                      teacher_model: Optional[tf.keras.Model] = None) -> Tuple[tf.keras.Model, CompressionResult]:
        """
        å‹ç¼©æ¨¡å‹
        
        Args:
            model: åŸå§‹æ¨¡å‹
            training_data: è®­ç»ƒæ•°æ®ï¼ˆç”¨äºè’¸é¦å’Œå¾®è°ƒï¼‰
            teacher_model: æ•™å¸ˆæ¨¡å‹ï¼ˆç”¨äºè’¸é¦ï¼‰
            
        Returns:
            å‹ç¼©åçš„æ¨¡å‹å’Œå‹ç¼©ç»“æœ
        """
        try:
            # è®°å½•åŸå§‹æ¨¡å‹ä¿¡æ¯
            original_size = self._calculate_model_size(model)
            
            # æ ¹æ®é…ç½®é€‰æ‹©å‹ç¼©æ–¹æ³•
            if self.config.method == CompressionMethod.QUANTIZATION:
                compressed_model = self.quantizer.quantize_model(model)
            
            elif self.config.method == CompressionMethod.PRUNING:
                if training_data:
                    compressed_model = self.pruner.gradual_pruning(model, training_data[0])
                else:
                    # ç®€å•å‰ªæ
                    compressed_model = self._apply_simple_pruning(model)
            
            elif self.config.method == CompressionMethod.DISTILLATION:
                if teacher_model and training_data:
                    compressed_model = self.distiller.distill_model(
                        teacher_model, model, training_data
                    )
                else:
                    logger_manager.warning("è’¸é¦éœ€è¦æ•™å¸ˆæ¨¡å‹å’Œè®­ç»ƒæ•°æ®")
                    compressed_model = model
            
            elif self.config.method == CompressionMethod.LOW_RANK:
                compressed_model = self._apply_low_rank_decomposition(model)
            
            elif self.config.method == CompressionMethod.HYBRID:
                # æ··åˆå‹ç¼©æ–¹æ³•
                compressed_model = self._apply_hybrid_compression(model, training_data, teacher_model)
            
            else:
                compressed_model = model
            
            # è®¡ç®—å‹ç¼©ç»“æœ
            compressed_size = self._calculate_model_size(compressed_model)
            compression_ratio = compressed_size / original_size
            
            result = CompressionResult(
                original_size=original_size,
                compressed_size=compressed_size,
                compression_ratio=compression_ratio,
                accuracy_before=0.0,  # éœ€è¦å®é™…è¯„ä¼°
                accuracy_after=0.0,   # éœ€è¦å®é™…è¯„ä¼°
                accuracy_loss=0.0,
                inference_speedup=1.0 / compression_ratio,  # ä¼°ç®—
                memory_reduction=1.0 - compression_ratio
            )
            
            logger_manager.info(f"æ¨¡å‹å‹ç¼©å®Œæˆï¼Œå‹ç¼©æ¯”: {compression_ratio:.3f}")
            return compressed_model, result
            
        except Exception as e:
            logger_manager.error(f"æ¨¡å‹å‹ç¼©å¤±è´¥: {e}")
            return model, CompressionResult(0, 0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0)
    
    def _calculate_model_size(self, model: tf.keras.Model) -> int:
        """è®¡ç®—æ¨¡å‹å¤§å°ï¼ˆå‚æ•°æ•°é‡ï¼‰"""
        try:
            return model.count_params()
        except:
            return 0
    
    def _apply_simple_pruning(self, model: tf.keras.Model) -> tf.keras.Model:
        """åº”ç”¨ç®€å•å‰ªæ"""
        try:
            for layer in model.layers:
                if hasattr(layer, 'kernel'):
                    weights = layer.get_weights()
                    if weights:
                        pruned_kernel, _ = self.pruner.magnitude_pruning(weights[0])
                        layer.set_weights([pruned_kernel] + weights[1:])
            
            return model
            
        except Exception as e:
            logger_manager.error(f"ç®€å•å‰ªæå¤±è´¥: {e}")
            return model
    
    def _apply_low_rank_decomposition(self, model: tf.keras.Model) -> tf.keras.Model:
        """åº”ç”¨ä½ç§©åˆ†è§£"""
        try:
            for layer in model.layers:
                if hasattr(layer, 'kernel'):
                    weights = layer.get_weights()
                    if weights and len(weights[0].shape) == 2:
                        # å¯¹å…¨è¿æ¥å±‚åº”ç”¨SVDåˆ†è§£
                        U, S, Vt = self.decomposer.svd_decomposition(weights[0])
                        
                        # é‡æ„æƒé‡
                        reconstructed = U @ np.diag(S) @ Vt
                        layer.set_weights([reconstructed] + weights[1:])
            
            return model
            
        except Exception as e:
            logger_manager.error(f"ä½ç§©åˆ†è§£å¤±è´¥: {e}")
            return model
    
    def _apply_hybrid_compression(self, model: tf.keras.Model,
                                 training_data: Optional[Tuple[np.ndarray, np.ndarray]],
                                 teacher_model: Optional[tf.keras.Model]) -> tf.keras.Model:
        """åº”ç”¨æ··åˆå‹ç¼©æ–¹æ³•"""
        try:
            # 1. å…ˆåº”ç”¨å‰ªæ
            compressed_model = self._apply_simple_pruning(model)
            
            # 2. å†åº”ç”¨é‡åŒ–
            compressed_model = self.quantizer.quantize_model(compressed_model)
            
            # 3. å¦‚æœæœ‰æ•™å¸ˆæ¨¡å‹ï¼Œåº”ç”¨è’¸é¦
            if teacher_model and training_data:
                compressed_model = self.distiller.distill_model(
                    teacher_model, compressed_model, training_data, epochs=5
                )
            
            logger_manager.info("æ··åˆå‹ç¼©å®Œæˆ")
            return compressed_model
            
        except Exception as e:
            logger_manager.error(f"æ··åˆå‹ç¼©å¤±è´¥: {e}")
            return model


# å…¨å±€æ¨¡å‹å‹ç¼©å™¨å®ä¾‹
default_config = CompressionConfig(
    method=CompressionMethod.HYBRID,
    target_compression_ratio=0.3,
    quantization_bits=8,
    pruning_sparsity=0.5
)
model_compressor = ModelCompressor(default_config)


if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å‹å‹ç¼©åŠŸèƒ½
    print("ğŸ—œï¸ æµ‹è¯•æ¨¡å‹å‹ç¼©åŠŸèƒ½...")
    
    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    test_model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, activation='relu', input_shape=(10,)),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    
    # æµ‹è¯•é‡åŒ–
    quantizer = ModelQuantizer(bits=8)
    test_weights = np.random.randn(128, 64)
    quantized_weights, params = quantizer.quantize_weights(test_weights)
    print(f"é‡åŒ–æµ‹è¯•å®Œæˆï¼Œé‡åŒ–å‚æ•°: {params}")
    
    # æµ‹è¯•å‰ªæ
    pruner = ModelPruner(sparsity=0.3)
    pruned_weights, mask = pruner.magnitude_pruning(test_weights)
    sparsity = 1 - np.count_nonzero(pruned_weights) / pruned_weights.size
    print(f"å‰ªææµ‹è¯•å®Œæˆï¼Œå®é™…ç¨€ç–åº¦: {sparsity:.3f}")
    
    # æµ‹è¯•ä½ç§©åˆ†è§£
    decomposer = LowRankDecomposer(rank_ratio=0.5)
    U, S, Vt = decomposer.svd_decomposition(test_weights)
    print(f"ä½ç§©åˆ†è§£æµ‹è¯•å®Œæˆï¼Œåˆ†è§£å½¢çŠ¶: U{U.shape}, S{S.shape}, Vt{Vt.shape}")
    
    print("æ¨¡å‹å‹ç¼©åŠŸèƒ½æµ‹è¯•å®Œæˆ")
