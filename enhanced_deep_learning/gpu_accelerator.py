#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
GPUåŠ é€Ÿå™¨
æä¾›GPUåŠ é€Ÿæ”¯æŒå’Œå†…å­˜ç®¡ç†åŠŸèƒ½
"""

import os
import numpy as np
from typing import List, Dict, Tuple, Any, Optional, Union
import platform
import psutil

from core_modules import logger_manager


class GPUAccelerator:
    """GPUåŠ é€Ÿå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–GPUåŠ é€Ÿå™¨"""
        self.gpu_available = False
        self.gpu_devices = []
        self.gpu_memory = {}
        self.tf_gpu_available = False
        self.torch_gpu_available = False
        
        # æ£€æŸ¥GPUå¯ç”¨æ€§
        self._check_gpu_availability()
        
        if self.gpu_available:
            logger_manager.info(f"GPUåŠ é€Ÿå™¨åˆå§‹åŒ–å®Œæˆï¼Œå¯ç”¨GPU: {len(self.gpu_devices)}")
        else:
            logger_manager.info("GPUåŠ é€Ÿå™¨åˆå§‹åŒ–å®Œæˆï¼Œæ²¡æœ‰å¯ç”¨GPU")
    
    def _check_gpu_availability(self) -> None:
        """æ£€æŸ¥GPUå¯ç”¨æ€§"""
        # æ£€æŸ¥TensorFlow GPU
        try:
            import tensorflow as tf
            gpus = tf.config.list_physical_devices('GPU')
            if gpus:
                self.gpu_available = True
                self.tf_gpu_available = True
                self.gpu_devices.extend([f"TensorFlow GPU {i}" for i in range(len(gpus))])
                
                # è·å–GPUå†…å­˜ä¿¡æ¯
                for i, gpu in enumerate(gpus):
                    try:
                        gpu_memory = tf.config.experimental.get_memory_info(gpu)
                        self.gpu_memory[f"TensorFlow GPU {i}"] = {
                            'total': gpu_memory['total'] / (1024 ** 3),  # GB
                            'used': gpu_memory['used'] / (1024 ** 3)  # GB
                        }
                    except:
                        # å¦‚æœæ— æ³•è·å–å†…å­˜ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤å€¼
                        self.gpu_memory[f"TensorFlow GPU {i}"] = {
                            'total': 0,
                            'used': 0
                        }
                
                logger_manager.info(f"æ£€æµ‹åˆ° {len(gpus)} ä¸ª TensorFlow GPU")
        except Exception as e:
            logger_manager.debug(f"TensorFlow GPUæ£€æµ‹å¤±è´¥: {e}")
        
        # æ£€æŸ¥PyTorch GPU
        try:
            import torch
            if torch.cuda.is_available():
                self.gpu_available = True
                self.torch_gpu_available = True
                
                # è·å–GPUæ•°é‡
                device_count = torch.cuda.device_count()
                self.gpu_devices.extend([f"PyTorch GPU {i}" for i in range(device_count)])
                
                # è·å–GPUå†…å­˜ä¿¡æ¯
                for i in range(device_count):
                    try:
                        total_memory = torch.cuda.get_device_properties(i).total_memory / (1024 ** 3)  # GB
                        reserved_memory = torch.cuda.memory_reserved(i) / (1024 ** 3)  # GB
                        allocated_memory = torch.cuda.memory_allocated(i) / (1024 ** 3)  # GB
                        
                        self.gpu_memory[f"PyTorch GPU {i}"] = {
                            'total': total_memory,
                            'reserved': reserved_memory,
                            'allocated': allocated_memory,
                            'free': total_memory - allocated_memory
                        }
                    except:
                        # å¦‚æœæ— æ³•è·å–å†…å­˜ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤å€¼
                        self.gpu_memory[f"PyTorch GPU {i}"] = {
                            'total': 0,
                            'reserved': 0,
                            'allocated': 0,
                            'free': 0
                        }
                
                logger_manager.info(f"æ£€æµ‹åˆ° {device_count} ä¸ª PyTorch GPU")
        except Exception as e:
            logger_manager.debug(f"PyTorch GPUæ£€æµ‹å¤±è´¥: {e}")
    
    def is_gpu_available(self) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„GPU
        
        Returns:
            æ˜¯å¦æœ‰å¯ç”¨çš„GPU
        """
        return self.gpu_available
    
    def get_gpu_devices(self) -> List[str]:
        """
        è·å–å¯ç”¨çš„GPUè®¾å¤‡åˆ—è¡¨
        
        Returns:
            GPUè®¾å¤‡åˆ—è¡¨
        """
        return self.gpu_devices
    
    def get_gpu_memory_info(self) -> Dict[str, Dict[str, float]]:
        """
        è·å–GPUå†…å­˜ä¿¡æ¯
        
        Returns:
            GPUå†…å­˜ä¿¡æ¯å­—å…¸
        """
        return self.gpu_memory
    
    def enable_tensorflow_gpu(self, memory_limit: Optional[int] = None) -> bool:
        """
        å¯ç”¨TensorFlow GPU
        
        Args:
            memory_limit: å†…å­˜é™åˆ¶ï¼ˆMBï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä¸é™åˆ¶
            
        Returns:
            æ˜¯å¦æˆåŠŸå¯ç”¨
        """
        if not self.tf_gpu_available:
            logger_manager.warning("TensorFlow GPUä¸å¯ç”¨")
            return False
        
        try:
            import tensorflow as tf
            
            # è·å–å¯ç”¨çš„GPUè®¾å¤‡
            gpus = tf.config.list_physical_devices('GPU')
            
            if not gpus:
                logger_manager.warning("æ²¡æœ‰å¯ç”¨çš„TensorFlow GPU")
                return False
            
            # é…ç½®GPUå†…å­˜å¢é•¿
            for gpu in gpus:
                if memory_limit:
                    # é™åˆ¶GPUå†…å­˜ä½¿ç”¨
                    tf.config.set_logical_device_configuration(
                        gpu,
                        [tf.config.LogicalDeviceConfiguration(memory_limit=memory_limit)]
                    )
                else:
                    # å…è®¸GPUå†…å­˜å¢é•¿
                    tf.config.experimental.set_memory_growth(gpu, True)
            
            logger_manager.info(f"TensorFlow GPUå·²å¯ç”¨ï¼Œå†…å­˜é™åˆ¶: {memory_limit if memory_limit else 'æ— é™åˆ¶'}")
            return True
        except Exception as e:
            logger_manager.error(f"å¯ç”¨TensorFlow GPUå¤±è´¥: {e}")
            return False
    
    def enable_pytorch_gpu(self, device_id: int = 0) -> bool:
        """
        å¯ç”¨PyTorch GPU
        
        Args:
            device_id: è®¾å¤‡ID
            
        Returns:
            æ˜¯å¦æˆåŠŸå¯ç”¨
        """
        if not self.torch_gpu_available:
            logger_manager.warning("PyTorch GPUä¸å¯ç”¨")
            return False
        
        try:
            import torch
            
            # æ£€æŸ¥è®¾å¤‡IDæ˜¯å¦æœ‰æ•ˆ
            if device_id >= torch.cuda.device_count():
                logger_manager.warning(f"æ— æ•ˆçš„è®¾å¤‡ID: {device_id}ï¼Œæœ€å¤§ID: {torch.cuda.device_count() - 1}")
                return False
            
            # è®¾ç½®é»˜è®¤è®¾å¤‡
            torch.cuda.set_device(device_id)
            
            # æ¸…ç©ºç¼“å­˜
            torch.cuda.empty_cache()
            
            logger_manager.info(f"PyTorch GPUå·²å¯ç”¨ï¼Œè®¾å¤‡ID: {device_id}")
            return True
        except Exception as e:
            logger_manager.error(f"å¯ç”¨PyTorch GPUå¤±è´¥: {e}")
            return False
    
    def migrate_model_to_gpu(self, model: Any, framework: str = 'tensorflow') -> Any:
        """
        å°†æ¨¡å‹è¿ç§»åˆ°GPU
        
        Args:
            model: æ¨¡å‹å¯¹è±¡
            framework: æ¡†æ¶åç§°ï¼Œæ”¯æŒ'tensorflow', 'pytorch'
            
        Returns:
            è¿ç§»åçš„æ¨¡å‹
        """
        if not self.gpu_available:
            logger_manager.warning("GPUä¸å¯ç”¨ï¼Œæ— æ³•è¿ç§»æ¨¡å‹")
            return model
        
        if framework == 'tensorflow':
            if not self.tf_gpu_available:
                logger_manager.warning("TensorFlow GPUä¸å¯ç”¨ï¼Œæ— æ³•è¿ç§»æ¨¡å‹")
                return model
            
            try:
                import tensorflow as tf
                
                # è·å–å¯ç”¨çš„GPUè®¾å¤‡
                gpus = tf.config.list_physical_devices('GPU')
                
                if not gpus:
                    logger_manager.warning("æ²¡æœ‰å¯ç”¨çš„TensorFlow GPUï¼Œæ— æ³•è¿ç§»æ¨¡å‹")
                    return model
                
                # å°†æ¨¡å‹è¿ç§»åˆ°GPU
                with tf.device('/GPU:0'):
                    # å¦‚æœæ˜¯Kerasæ¨¡å‹ï¼Œå¯ä»¥ç›´æ¥å…‹éš†
                    if hasattr(model, 'clone_model'):
                        gpu_model = tf.keras.models.clone_model(model)
                        gpu_model.set_weights(model.get_weights())
                    else:
                        # å¦åˆ™ç›´æ¥è¿”å›åŸæ¨¡å‹ï¼ˆTensorFlowä¼šè‡ªåŠ¨ä½¿ç”¨GPUï¼‰
                        gpu_model = model
                
                logger_manager.info("æ¨¡å‹å·²è¿ç§»åˆ°TensorFlow GPU")
                return gpu_model
            except Exception as e:
                logger_manager.error(f"å°†æ¨¡å‹è¿ç§»åˆ°TensorFlow GPUå¤±è´¥: {e}")
                return model
        
        elif framework == 'pytorch':
            if not self.torch_gpu_available:
                logger_manager.warning("PyTorch GPUä¸å¯ç”¨ï¼Œæ— æ³•è¿ç§»æ¨¡å‹")
                return model
            
            try:
                import torch
                
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç»åœ¨GPUä¸Š
                if next(model.parameters()).is_cuda:
                    logger_manager.info("æ¨¡å‹å·²ç»åœ¨PyTorch GPUä¸Š")
                    return model
                
                # å°†æ¨¡å‹è¿ç§»åˆ°GPU
                gpu_model = model.cuda()
                
                logger_manager.info("æ¨¡å‹å·²è¿ç§»åˆ°PyTorch GPU")
                return gpu_model
            except Exception as e:
                logger_manager.error(f"å°†æ¨¡å‹è¿ç§»åˆ°PyTorch GPUå¤±è´¥: {e}")
                return model
        
        else:
            logger_manager.warning(f"æœªçŸ¥çš„æ¡†æ¶: {framework}")
            return model
    
    def clear_gpu_memory(self, framework: str = 'all') -> bool:
        """
        æ¸…ç†GPUå†…å­˜
        
        Args:
            framework: æ¡†æ¶åç§°ï¼Œæ”¯æŒ'tensorflow', 'pytorch', 'all'
            
        Returns:
            æ˜¯å¦æˆåŠŸæ¸…ç†
        """
        success = True
        
        if framework in ['tensorflow', 'all']:
            if self.tf_gpu_available:
                try:
                    import tensorflow as tf
                    
                    # æ¸…ç†TensorFlow GPUå†…å­˜
                    tf.keras.backend.clear_session()
                    
                    logger_manager.info("TensorFlow GPUå†…å­˜å·²æ¸…ç†")
                except Exception as e:
                    logger_manager.error(f"æ¸…ç†TensorFlow GPUå†…å­˜å¤±è´¥: {e}")
                    success = False
        
        if framework in ['pytorch', 'all']:
            if self.torch_gpu_available:
                try:
                    import torch
                    
                    # æ¸…ç†PyTorch GPUå†…å­˜
                    torch.cuda.empty_cache()
                    
                    logger_manager.info("PyTorch GPUå†…å­˜å·²æ¸…ç†")
                except Exception as e:
                    logger_manager.error(f"æ¸…ç†PyTorch GPUå†…å­˜å¤±è´¥: {e}")
                    success = False
        
        return success
    
    def monitor_gpu_usage(self) -> Dict[str, Any]:
        """
        ç›‘æ§GPUä½¿ç”¨æƒ…å†µ
        
        Returns:
            GPUä½¿ç”¨æƒ…å†µå­—å…¸
        """
        usage_info = {
            'gpu_available': self.gpu_available,
            'gpu_devices': self.gpu_devices,
            'gpu_memory': {},
            'system_memory': {}
        }
        
        # æ›´æ–°GPUå†…å­˜ä¿¡æ¯
        if self.tf_gpu_available:
            try:
                import tensorflow as tf
                
                gpus = tf.config.list_physical_devices('GPU')
                for i, gpu in enumerate(gpus):
                    try:
                        gpu_memory = tf.config.experimental.get_memory_info(gpu)
                        usage_info['gpu_memory'][f"TensorFlow GPU {i}"] = {
                            'total': gpu_memory['total'] / (1024 ** 3),  # GB
                            'used': gpu_memory['used'] / (1024 ** 3)  # GB
                        }
                    except:
                        pass
            except:
                pass
        
        if self.torch_gpu_available:
            try:
                import torch
                
                for i in range(torch.cuda.device_count()):
                    try:
                        total_memory = torch.cuda.get_device_properties(i).total_memory / (1024 ** 3)  # GB
                        reserved_memory = torch.cuda.memory_reserved(i) / (1024 ** 3)  # GB
                        allocated_memory = torch.cuda.memory_allocated(i) / (1024 ** 3)  # GB
                        
                        usage_info['gpu_memory'][f"PyTorch GPU {i}"] = {
                            'total': total_memory,
                            'reserved': reserved_memory,
                            'allocated': allocated_memory,
                            'free': total_memory - allocated_memory
                        }
                    except:
                        pass
            except:
                pass
        
        # è·å–ç³»ç»Ÿå†…å­˜ä¿¡æ¯
        try:
            memory = psutil.virtual_memory()
            usage_info['system_memory'] = {
                'total': memory.total / (1024 ** 3),  # GB
                'available': memory.available / (1024 ** 3),  # GB
                'used': memory.used / (1024 ** 3),  # GB
                'percent': memory.percent
            }
        except:
            pass
        
        return usage_info


class GPUMemoryManager:
    """GPUå†…å­˜ç®¡ç†å™¨"""
    
    def __init__(self, accelerator: Optional[GPUAccelerator] = None):
        """
        åˆå§‹åŒ–GPUå†…å­˜ç®¡ç†å™¨
        
        Args:
            accelerator: GPUåŠ é€Ÿå™¨ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºæ–°çš„
        """
        self.accelerator = accelerator or GPUAccelerator()
        self.memory_limit = None
        self.auto_clear = True
        
        logger_manager.info("GPUå†…å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def set_memory_limit(self, limit_mb: int) -> bool:
        """
        è®¾ç½®å†…å­˜é™åˆ¶
        
        Args:
            limit_mb: å†…å­˜é™åˆ¶ï¼ˆMBï¼‰
            
        Returns:
            æ˜¯å¦æˆåŠŸè®¾ç½®
        """
        self.memory_limit = limit_mb
        
        # åº”ç”¨å†…å­˜é™åˆ¶
        if self.accelerator.tf_gpu_available:
            success = self.accelerator.enable_tensorflow_gpu(memory_limit=limit_mb)
            if not success:
                logger_manager.warning("è®¾ç½®TensorFlow GPUå†…å­˜é™åˆ¶å¤±è´¥")
                return False
        
        logger_manager.info(f"GPUå†…å­˜é™åˆ¶å·²è®¾ç½®ä¸º {limit_mb} MB")
        return True
    
    def enable_auto_clear(self, enabled: bool = True) -> None:
        """
        å¯ç”¨è‡ªåŠ¨æ¸…ç†
        
        Args:
            enabled: æ˜¯å¦å¯ç”¨
        """
        self.auto_clear = enabled
        logger_manager.info(f"GPUå†…å­˜è‡ªåŠ¨æ¸…ç†å·²{'å¯ç”¨' if enabled else 'ç¦ç”¨'}")
    
    def clear_if_needed(self, threshold_percent: float = 80.0) -> bool:
        """
        æ ¹æ®éœ€è¦æ¸…ç†å†…å­˜
        
        Args:
            threshold_percent: é˜ˆå€¼ç™¾åˆ†æ¯”ï¼Œè¶…è¿‡æ­¤å€¼æ—¶æ¸…ç†å†…å­˜
            
        Returns:
            æ˜¯å¦å·²æ¸…ç†
        """
        if not self.auto_clear:
            return False
        
        # è·å–GPUä½¿ç”¨æƒ…å†µ
        usage_info = self.accelerator.monitor_gpu_usage()
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç†
        need_clear = False
        for device, memory in usage_info.get('gpu_memory', {}).items():
            if 'total' in memory and 'used' in memory and memory['total'] > 0:
                usage_percent = memory['used'] / memory['total'] * 100
                if usage_percent > threshold_percent:
                    need_clear = True
                    break
        
        if need_clear:
            logger_manager.info(f"GPUå†…å­˜ä½¿ç”¨ç‡è¶…è¿‡ {threshold_percent}%ï¼Œæ‰§è¡Œæ¸…ç†")
            return self.accelerator.clear_gpu_memory()
        
        return False
    
    def optimize_batch_size(self, initial_batch_size: int, model_size_mb: int) -> int:
        """
        ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°
        
        Args:
            initial_batch_size: åˆå§‹æ‰¹å¤„ç†å¤§å°
            model_size_mb: æ¨¡å‹å¤§å°ï¼ˆMBï¼‰
            
        Returns:
            ä¼˜åŒ–åçš„æ‰¹å¤„ç†å¤§å°
        """
        if not self.accelerator.gpu_available:
            logger_manager.warning("GPUä¸å¯ç”¨ï¼Œä½¿ç”¨åˆå§‹æ‰¹å¤„ç†å¤§å°")
            return initial_batch_size
        
        # è·å–GPUå†…å­˜ä¿¡æ¯
        usage_info = self.accelerator.monitor_gpu_usage()
        
        # è®¡ç®—å¯ç”¨å†…å­˜
        available_memory_mb = 0
        for device, memory in usage_info.get('gpu_memory', {}).items():
            if 'total' in memory and 'used' in memory:
                available = (memory['total'] - memory['used']) * 1024  # MB
                if available > available_memory_mb:
                    available_memory_mb = available
        
        if available_memory_mb == 0:
            logger_manager.warning("æ— æ³•è·å–GPUå¯ç”¨å†…å­˜ï¼Œä½¿ç”¨åˆå§‹æ‰¹å¤„ç†å¤§å°")
            return initial_batch_size
        
        # ä¼°è®¡æ¯ä¸ªæ ·æœ¬çš„å†…å­˜ä½¿ç”¨é‡
        estimated_sample_memory = model_size_mb / initial_batch_size * 2  # ä¹˜ä»¥2ä½œä¸ºå®‰å…¨ç³»æ•°
        
        # è®¡ç®—æœ€å¤§æ‰¹å¤„ç†å¤§å°
        max_batch_size = int(available_memory_mb / estimated_sample_memory * 0.8)  # ä½¿ç”¨80%çš„å¯ç”¨å†…å­˜
        
        # ç¡®ä¿æ‰¹å¤„ç†å¤§å°æ˜¯2çš„å¹‚æ¬¡æ–¹
        optimized_batch_size = 2 ** int(np.log2(max_batch_size))
        
        # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
        optimized_batch_size = max(16, min(optimized_batch_size, 1024))
        
        logger_manager.info(f"æ‰¹å¤„ç†å¤§å°å·²ä¼˜åŒ–: {initial_batch_size} -> {optimized_batch_size}")
        
        return optimized_batch_size


if __name__ == "__main__":
    # æµ‹è¯•GPUåŠ é€Ÿå™¨
    print("ğŸš€ æµ‹è¯•GPUåŠ é€Ÿå™¨...")
    
    # åˆ›å»ºGPUåŠ é€Ÿå™¨
    accelerator = GPUAccelerator()
    
    # æ£€æŸ¥GPUå¯ç”¨æ€§
    gpu_available = accelerator.is_gpu_available()
    print(f"GPUå¯ç”¨æ€§: {gpu_available}")
    
    # è·å–GPUè®¾å¤‡åˆ—è¡¨
    gpu_devices = accelerator.get_gpu_devices()
    print(f"GPUè®¾å¤‡åˆ—è¡¨: {gpu_devices}")
    
    # è·å–GPUå†…å­˜ä¿¡æ¯
    gpu_memory = accelerator.get_gpu_memory_info()
    print(f"GPUå†…å­˜ä¿¡æ¯: {gpu_memory}")
    
    # æµ‹è¯•GPUå†…å­˜ç®¡ç†å™¨
    print("\nğŸš€ æµ‹è¯•GPUå†…å­˜ç®¡ç†å™¨...")
    
    # åˆ›å»ºGPUå†…å­˜ç®¡ç†å™¨
    memory_manager = GPUMemoryManager(accelerator)
    
    # è®¾ç½®å†…å­˜é™åˆ¶
    memory_manager.set_memory_limit(1024)
    
    # å¯ç”¨è‡ªåŠ¨æ¸…ç†
    memory_manager.enable_auto_clear(True)
    
    # ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°
    optimized_batch_size = memory_manager.optimize_batch_size(32, 500)
    print(f"ä¼˜åŒ–åçš„æ‰¹å¤„ç†å¤§å°: {optimized_batch_size}")
    
    print("GPUåŠ é€Ÿå™¨æµ‹è¯•å®Œæˆ")