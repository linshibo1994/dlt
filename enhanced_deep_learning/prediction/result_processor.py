#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç»“æœå¤„ç†å™¨æ¨¡å—
Result Processor Module

æä¾›é¢„æµ‹ç»“æœçš„æ ¼å¼åŒ–ã€åˆ†æã€è½¬æ¢å’Œå¯¼å‡ºåŠŸèƒ½ã€‚
"""

import os
import json
import csv
from typing import Dict, List, Any, Optional, Union, Tuple
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime
import numpy as np
import pandas as pd

from core_modules import logger_manager
from ..utils.exceptions import PredictionException
from .prediction_engine import PredictionResult, PredictionStatus


class ResultFormat(Enum):
    """ç»“æœæ ¼å¼æšä¸¾"""
    JSON = "json"
    CSV = "csv"
    EXCEL = "excel"
    XML = "xml"
    HTML = "html"
    NUMPY = "numpy"
    PANDAS = "pandas"


class AnalysisType(Enum):
    """åˆ†æç±»å‹æšä¸¾"""
    STATISTICAL = "statistical"
    DISTRIBUTION = "distribution"
    CORRELATION = "correlation"
    TREND = "trend"
    OUTLIER = "outlier"


@dataclass
class ProcessedResult:
    """å¤„ç†åçš„ç»“æœ"""
    original_result: PredictionResult
    formatted_data: Any
    format_type: ResultFormat
    analysis: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    processed_time: datetime = field(default_factory=datetime.now)


class ResultFormatter:
    """ç»“æœæ ¼å¼åŒ–å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç»“æœæ ¼å¼åŒ–å™¨"""
        logger_manager.info("ç»“æœæ ¼å¼åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def format_to_json(self, result: PredictionResult) -> Dict[str, Any]:
        """æ ¼å¼åŒ–ä¸ºJSON"""
        try:
            formatted = {
                'request_id': result.request_id,
                'status': result.status.value,
                'model_info': result.model_info,
                'execution_time': result.execution_time,
                'completed_time': result.completed_time.isoformat() if result.completed_time else None,
                'predictions': result.predictions.tolist() if result.predictions is not None else None,
                'confidence_scores': result.confidence_scores.tolist() if result.confidence_scores is not None else None,
                'error_message': result.error_message,
                'metadata': result.metadata
            }
            
            logger_manager.debug(f"ç»“æœæ ¼å¼åŒ–ä¸ºJSON: {result.request_id}")
            return formatted
            
        except Exception as e:
            logger_manager.error(f"JSONæ ¼å¼åŒ–å¤±è´¥: {e}")
            raise PredictionException(f"JSONæ ¼å¼åŒ–å¤±è´¥: {e}")
    
    def format_to_dataframe(self, result: PredictionResult) -> pd.DataFrame:
        """æ ¼å¼åŒ–ä¸ºDataFrame"""
        try:
            if result.predictions is None:
                raise PredictionException("é¢„æµ‹ç»“æœä¸ºç©ºï¼Œæ— æ³•è½¬æ¢ä¸ºDataFrame")
            
            # åˆ›å»ºåŸºç¡€æ•°æ®
            data = {
                'prediction': result.predictions.flatten() if result.predictions.ndim > 1 else result.predictions
            }
            
            # æ·»åŠ ç½®ä¿¡åº¦åˆ†æ•°
            if result.confidence_scores is not None:
                if result.confidence_scores.ndim > 1:
                    for i in range(result.confidence_scores.shape[1]):
                        data[f'confidence_{i}'] = result.confidence_scores[:, i]
                else:
                    data['confidence'] = result.confidence_scores
            
            # æ·»åŠ å…ƒæ•°æ®
            data['request_id'] = result.request_id
            data['status'] = result.status.value
            data['execution_time'] = result.execution_time
            
            df = pd.DataFrame(data)
            
            logger_manager.debug(f"ç»“æœæ ¼å¼åŒ–ä¸ºDataFrame: {result.request_id}, å½¢çŠ¶: {df.shape}")
            return df
            
        except Exception as e:
            logger_manager.error(f"DataFrameæ ¼å¼åŒ–å¤±è´¥: {e}")
            raise PredictionException(f"DataFrameæ ¼å¼åŒ–å¤±è´¥: {e}")
    
    def format_to_csv(self, result: PredictionResult) -> str:
        """æ ¼å¼åŒ–ä¸ºCSVå­—ç¬¦ä¸²"""
        try:
            df = self.format_to_dataframe(result)
            csv_string = df.to_csv(index=False)
            
            logger_manager.debug(f"ç»“æœæ ¼å¼åŒ–ä¸ºCSV: {result.request_id}")
            return csv_string
            
        except Exception as e:
            logger_manager.error(f"CSVæ ¼å¼åŒ–å¤±è´¥: {e}")
            raise PredictionException(f"CSVæ ¼å¼åŒ–å¤±è´¥: {e}")
    
    def format_to_html(self, result: PredictionResult) -> str:
        """æ ¼å¼åŒ–ä¸ºHTML"""
        try:
            html = f"""
            <div class="prediction-result">
                <h3>é¢„æµ‹ç»“æœ - {result.request_id}</h3>
                <div class="result-info">
                    <p><strong>çŠ¶æ€:</strong> {result.status.value}</p>
                    <p><strong>æ‰§è¡Œæ—¶é—´:</strong> {result.execution_time:.4f}ç§’</p>
                    <p><strong>å®Œæˆæ—¶é—´:</strong> {result.completed_time}</p>
                </div>
            """
            
            if result.predictions is not None:
                df = self.format_to_dataframe(result)
                html += f"""
                <div class="predictions">
                    <h4>é¢„æµ‹æ•°æ®</h4>
                    {df.to_html(classes='table table-striped', table_id='predictions-table')}
                </div>
                """
            
            if result.error_message:
                html += f"""
                <div class="error-message alert alert-danger">
                    <strong>é”™è¯¯ä¿¡æ¯:</strong> {result.error_message}
                </div>
                """
            
            html += "</div>"
            
            logger_manager.debug(f"ç»“æœæ ¼å¼åŒ–ä¸ºHTML: {result.request_id}")
            return html
            
        except Exception as e:
            logger_manager.error(f"HTMLæ ¼å¼åŒ–å¤±è´¥: {e}")
            raise PredictionException(f"HTMLæ ¼å¼åŒ–å¤±è´¥: {e}")


class ResultAnalyzer:
    """ç»“æœåˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç»“æœåˆ†æå™¨"""
        logger_manager.info("ç»“æœåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def analyze_statistical(self, predictions: np.ndarray) -> Dict[str, Any]:
        """ç»Ÿè®¡åˆ†æ"""
        try:
            if predictions is None or len(predictions) == 0:
                return {}
            
            # å±•å¹³å¤šç»´æ•°ç»„
            flat_predictions = predictions.flatten()
            
            analysis = {
                'count': len(flat_predictions),
                'mean': float(np.mean(flat_predictions)),
                'std': float(np.std(flat_predictions)),
                'min': float(np.min(flat_predictions)),
                'max': float(np.max(flat_predictions)),
                'median': float(np.median(flat_predictions)),
                'q25': float(np.percentile(flat_predictions, 25)),
                'q75': float(np.percentile(flat_predictions, 75)),
                'variance': float(np.var(flat_predictions)),
                'skewness': self._calculate_skewness(flat_predictions),
                'kurtosis': self._calculate_kurtosis(flat_predictions)
            }
            
            logger_manager.debug("ç»Ÿè®¡åˆ†æå®Œæˆ")
            return analysis
            
        except Exception as e:
            logger_manager.error(f"ç»Ÿè®¡åˆ†æå¤±è´¥: {e}")
            return {}
    
    def analyze_distribution(self, predictions: np.ndarray, bins: int = 20) -> Dict[str, Any]:
        """åˆ†å¸ƒåˆ†æ"""
        try:
            if predictions is None or len(predictions) == 0:
                return {}
            
            flat_predictions = predictions.flatten()
            
            # è®¡ç®—ç›´æ–¹å›¾
            hist, bin_edges = np.histogram(flat_predictions, bins=bins)
            
            analysis = {
                'histogram': {
                    'counts': hist.tolist(),
                    'bin_edges': bin_edges.tolist(),
                    'bins': bins
                },
                'distribution_stats': {
                    'range': float(np.max(flat_predictions) - np.min(flat_predictions)),
                    'iqr': float(np.percentile(flat_predictions, 75) - np.percentile(flat_predictions, 25)),
                    'mode_bin': int(np.argmax(hist)),
                    'entropy': self._calculate_entropy(hist)
                }
            }
            
            logger_manager.debug("åˆ†å¸ƒåˆ†æå®Œæˆ")
            return analysis
            
        except Exception as e:
            logger_manager.error(f"åˆ†å¸ƒåˆ†æå¤±è´¥: {e}")
            return {}
    
    def analyze_confidence(self, confidence_scores: np.ndarray) -> Dict[str, Any]:
        """ç½®ä¿¡åº¦åˆ†æ"""
        try:
            if confidence_scores is None or len(confidence_scores) == 0:
                return {}
            
            flat_confidence = confidence_scores.flatten()
            
            analysis = {
                'mean_confidence': float(np.mean(flat_confidence)),
                'min_confidence': float(np.min(flat_confidence)),
                'max_confidence': float(np.max(flat_confidence)),
                'std_confidence': float(np.std(flat_confidence)),
                'high_confidence_ratio': float(np.sum(flat_confidence > 0.8) / len(flat_confidence)),
                'low_confidence_ratio': float(np.sum(flat_confidence < 0.5) / len(flat_confidence)),
                'confidence_distribution': {
                    'very_high': float(np.sum(flat_confidence > 0.9) / len(flat_confidence)),
                    'high': float(np.sum((flat_confidence > 0.7) & (flat_confidence <= 0.9)) / len(flat_confidence)),
                    'medium': float(np.sum((flat_confidence > 0.5) & (flat_confidence <= 0.7)) / len(flat_confidence)),
                    'low': float(np.sum(flat_confidence <= 0.5) / len(flat_confidence))
                }
            }
            
            logger_manager.debug("ç½®ä¿¡åº¦åˆ†æå®Œæˆ")
            return analysis
            
        except Exception as e:
            logger_manager.error(f"ç½®ä¿¡åº¦åˆ†æå¤±è´¥: {e}")
            return {}
    
    def detect_outliers(self, predictions: np.ndarray, method: str = "iqr") -> Dict[str, Any]:
        """å¼‚å¸¸å€¼æ£€æµ‹"""
        try:
            if predictions is None or len(predictions) == 0:
                return {}
            
            flat_predictions = predictions.flatten()
            
            if method == "iqr":
                q1 = np.percentile(flat_predictions, 25)
                q3 = np.percentile(flat_predictions, 75)
                iqr = q3 - q1
                lower_bound = q1 - 1.5 * iqr
                upper_bound = q3 + 1.5 * iqr
                
                outliers = (flat_predictions < lower_bound) | (flat_predictions > upper_bound)
                
            elif method == "zscore":
                z_scores = np.abs((flat_predictions - np.mean(flat_predictions)) / np.std(flat_predictions))
                outliers = z_scores > 3
                
            else:
                raise PredictionException(f"ä¸æ”¯æŒçš„å¼‚å¸¸å€¼æ£€æµ‹æ–¹æ³•: {method}")
            
            outlier_indices = np.where(outliers)[0]
            outlier_values = flat_predictions[outliers]
            
            analysis = {
                'method': method,
                'outlier_count': len(outlier_indices),
                'outlier_ratio': float(len(outlier_indices) / len(flat_predictions)),
                'outlier_indices': outlier_indices.tolist(),
                'outlier_values': outlier_values.tolist(),
                'bounds': {
                    'lower': float(lower_bound) if method == "iqr" else None,
                    'upper': float(upper_bound) if method == "iqr" else None
                }
            }
            
            logger_manager.debug(f"å¼‚å¸¸å€¼æ£€æµ‹å®Œæˆï¼Œæ–¹æ³•: {method}")
            return analysis
            
        except Exception as e:
            logger_manager.error(f"å¼‚å¸¸å€¼æ£€æµ‹å¤±è´¥: {e}")
            return {}
    
    def _calculate_skewness(self, data: np.ndarray) -> float:
        """è®¡ç®—ååº¦"""
        try:
            mean = np.mean(data)
            std = np.std(data)
            if std == 0:
                return 0.0
            
            skewness = np.mean(((data - mean) / std) ** 3)
            return float(skewness)
            
        except Exception:
            return 0.0
    
    def _calculate_kurtosis(self, data: np.ndarray) -> float:
        """è®¡ç®—å³°åº¦"""
        try:
            mean = np.mean(data)
            std = np.std(data)
            if std == 0:
                return 0.0
            
            kurtosis = np.mean(((data - mean) / std) ** 4) - 3
            return float(kurtosis)
            
        except Exception:
            return 0.0
    
    def _calculate_entropy(self, hist: np.ndarray) -> float:
        """è®¡ç®—ç†µ"""
        try:
            # å½’ä¸€åŒ–ç›´æ–¹å›¾
            prob = hist / np.sum(hist)
            # é¿å…log(0)
            prob = prob[prob > 0]
            entropy = -np.sum(prob * np.log2(prob))
            return float(entropy)
            
        except Exception:
            return 0.0


class ResultProcessor:
    """ç»“æœå¤„ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç»“æœå¤„ç†å™¨"""
        self.formatter = ResultFormatter()
        self.analyzer = ResultAnalyzer()
        
        logger_manager.info("ç»“æœå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def process_result(self, result: PredictionResult, 
                      format_type: ResultFormat = ResultFormat.JSON,
                      include_analysis: bool = True,
                      analysis_types: List[AnalysisType] = None) -> ProcessedResult:
        """
        å¤„ç†é¢„æµ‹ç»“æœ
        
        Args:
            result: é¢„æµ‹ç»“æœ
            format_type: æ ¼å¼ç±»å‹
            include_analysis: æ˜¯å¦åŒ…å«åˆ†æ
            analysis_types: åˆ†æç±»å‹åˆ—è¡¨
            
        Returns:
            å¤„ç†åçš„ç»“æœ
        """
        try:
            # æ ¼å¼åŒ–æ•°æ®
            if format_type == ResultFormat.JSON:
                formatted_data = self.formatter.format_to_json(result)
            elif format_type == ResultFormat.PANDAS:
                formatted_data = self.formatter.format_to_dataframe(result)
            elif format_type == ResultFormat.CSV:
                formatted_data = self.formatter.format_to_csv(result)
            elif format_type == ResultFormat.HTML:
                formatted_data = self.formatter.format_to_html(result)
            elif format_type == ResultFormat.NUMPY:
                formatted_data = result.predictions
            else:
                formatted_data = result
            
            # åˆ›å»ºå¤„ç†ç»“æœ
            processed = ProcessedResult(
                original_result=result,
                formatted_data=formatted_data,
                format_type=format_type
            )
            
            # æ‰§è¡Œåˆ†æ
            if include_analysis and result.predictions is not None:
                analysis_types = analysis_types or [AnalysisType.STATISTICAL]
                
                for analysis_type in analysis_types:
                    if analysis_type == AnalysisType.STATISTICAL:
                        processed.analysis['statistical'] = self.analyzer.analyze_statistical(result.predictions)
                    elif analysis_type == AnalysisType.DISTRIBUTION:
                        processed.analysis['distribution'] = self.analyzer.analyze_distribution(result.predictions)
                    elif analysis_type == AnalysisType.OUTLIER:
                        processed.analysis['outliers'] = self.analyzer.detect_outliers(result.predictions)
                
                # ç½®ä¿¡åº¦åˆ†æ
                if result.confidence_scores is not None:
                    processed.analysis['confidence'] = self.analyzer.analyze_confidence(result.confidence_scores)
            
            logger_manager.info(f"ç»“æœå¤„ç†å®Œæˆ: {result.request_id}")
            return processed
            
        except Exception as e:
            logger_manager.error(f"ç»“æœå¤„ç†å¤±è´¥: {e}")
            raise PredictionException(f"ç»“æœå¤„ç†å¤±è´¥: {e}")
    
    def batch_process_results(self, results: List[PredictionResult],
                            format_type: ResultFormat = ResultFormat.JSON,
                            **kwargs) -> List[ProcessedResult]:
        """æ‰¹é‡å¤„ç†ç»“æœ"""
        try:
            processed_results = []
            
            for result in results:
                processed = self.process_result(result, format_type, **kwargs)
                processed_results.append(processed)
            
            logger_manager.info(f"æ‰¹é‡ç»“æœå¤„ç†å®Œæˆ: {len(processed_results)} ä¸ª")
            return processed_results
            
        except Exception as e:
            logger_manager.error(f"æ‰¹é‡ç»“æœå¤„ç†å¤±è´¥: {e}")
            raise PredictionException(f"æ‰¹é‡ç»“æœå¤„ç†å¤±è´¥: {e}")
    
    def export_result(self, processed_result: ProcessedResult, 
                     file_path: str) -> bool:
        """å¯¼å‡ºç»“æœåˆ°æ–‡ä»¶"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            
            if processed_result.format_type == ResultFormat.JSON:
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(processed_result.formatted_data, f, indent=2, ensure_ascii=False)
            
            elif processed_result.format_type == ResultFormat.CSV:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(processed_result.formatted_data)
            
            elif processed_result.format_type == ResultFormat.HTML:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(processed_result.formatted_data)
            
            elif processed_result.format_type == ResultFormat.PANDAS:
                if file_path.endswith('.csv'):
                    processed_result.formatted_data.to_csv(file_path, index=False)
                elif file_path.endswith('.xlsx'):
                    processed_result.formatted_data.to_excel(file_path, index=False)
                else:
                    processed_result.formatted_data.to_pickle(file_path)
            
            else:
                # é»˜è®¤ä¿å­˜ä¸ºpickle
                import pickle
                with open(file_path, 'wb') as f:
                    pickle.dump(processed_result.formatted_data, f)
            
            logger_manager.info(f"ç»“æœå·²å¯¼å‡º: {file_path}")
            return True
            
        except Exception as e:
            logger_manager.error(f"å¯¼å‡ºç»“æœå¤±è´¥: {e}")
            return False
    
    def create_summary_report(self, results: List[PredictionResult]) -> Dict[str, Any]:
        """åˆ›å»ºæ±‡æ€»æŠ¥å‘Š"""
        try:
            if not results:
                return {}
            
            # åŸºç¡€ç»Ÿè®¡
            total_results = len(results)
            successful_results = sum(1 for r in results if r.status == PredictionStatus.COMPLETED)
            failed_results = sum(1 for r in results if r.status == PredictionStatus.FAILED)
            
            # æ‰§è¡Œæ—¶é—´ç»Ÿè®¡
            execution_times = [r.execution_time for r in results if r.execution_time > 0]
            
            # é¢„æµ‹ç»“æœç»Ÿè®¡
            all_predictions = []
            all_confidences = []
            
            for result in results:
                if result.predictions is not None:
                    all_predictions.extend(result.predictions.flatten())
                if result.confidence_scores is not None:
                    all_confidences.extend(result.confidence_scores.flatten())
            
            summary = {
                'overview': {
                    'total_predictions': total_results,
                    'successful_predictions': successful_results,
                    'failed_predictions': failed_results,
                    'success_rate': successful_results / total_results if total_results > 0 else 0
                },
                'performance': {
                    'avg_execution_time': np.mean(execution_times) if execution_times else 0,
                    'min_execution_time': np.min(execution_times) if execution_times else 0,
                    'max_execution_time': np.max(execution_times) if execution_times else 0,
                    'total_execution_time': np.sum(execution_times) if execution_times else 0
                },
                'predictions_analysis': self.analyzer.analyze_statistical(np.array(all_predictions)) if all_predictions else {},
                'confidence_analysis': self.analyzer.analyze_confidence(np.array(all_confidences)) if all_confidences else {},
                'generated_time': datetime.now().isoformat()
            }
            
            logger_manager.info(f"æ±‡æ€»æŠ¥å‘Šåˆ›å»ºå®Œæˆ: {total_results} ä¸ªç»“æœ")
            return summary
            
        except Exception as e:
            logger_manager.error(f"åˆ›å»ºæ±‡æ€»æŠ¥å‘Šå¤±è´¥: {e}")
            return {}


# å…¨å±€ç»“æœå¤„ç†å™¨å®ä¾‹
result_processor = ResultProcessor()


if __name__ == "__main__":
    # æµ‹è¯•ç»“æœå¤„ç†å™¨åŠŸèƒ½
    print("ğŸ“Š æµ‹è¯•ç»“æœå¤„ç†å™¨åŠŸèƒ½...")
    
    try:
        from .prediction_engine import PredictionResult, PredictionStatus
        import numpy as np
        
        # åˆ›å»ºæµ‹è¯•ç»“æœ
        test_result = PredictionResult(
            request_id="test_request",
            status=PredictionStatus.COMPLETED,
            predictions=np.random.random((10, 2)),
            confidence_scores=np.random.random(10),
            execution_time=1.5
        )
        
        # æµ‹è¯•ç»“æœå¤„ç†
        processor = ResultProcessor()
        
        # JSONæ ¼å¼å¤„ç†
        json_result = processor.process_result(test_result, ResultFormat.JSON)
        print("âœ… JSONæ ¼å¼å¤„ç†æˆåŠŸ")
        
        # DataFrameæ ¼å¼å¤„ç†
        df_result = processor.process_result(test_result, ResultFormat.PANDAS)
        print(f"âœ… DataFrameæ ¼å¼å¤„ç†æˆåŠŸï¼Œå½¢çŠ¶: {df_result.formatted_data.shape}")
        
        # åŒ…å«åˆ†æçš„å¤„ç†
        analyzed_result = processor.process_result(
            test_result, 
            ResultFormat.JSON,
            include_analysis=True,
            analysis_types=[AnalysisType.STATISTICAL, AnalysisType.DISTRIBUTION]
        )
        print(f"âœ… åˆ†æå¤„ç†æˆåŠŸï¼Œåˆ†æé¡¹: {list(analyzed_result.analysis.keys())}")
        
        # æµ‹è¯•æ±‡æ€»æŠ¥å‘Š
        summary = processor.create_summary_report([test_result])
        print(f"âœ… æ±‡æ€»æŠ¥å‘Šåˆ›å»ºæˆåŠŸï¼ŒæˆåŠŸç‡: {summary['overview']['success_rate']}")
        
        print("âœ… ç»“æœå¤„ç†å™¨åŠŸèƒ½æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print("ç»“æœå¤„ç†å™¨åŠŸèƒ½æµ‹è¯•å®Œæˆ")
