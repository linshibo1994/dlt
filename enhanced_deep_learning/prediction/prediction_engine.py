#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
é¢„æµ‹å¼•æ“æ¨¡å—
Prediction Engine Module

æä¾›ç»Ÿä¸€çš„é¢„æµ‹æ¥å£ã€æ¨¡å‹ç®¡ç†ã€é¢„æµ‹è°ƒåº¦ç­‰åŠŸèƒ½ã€‚
"""

import os
import time
import json
import threading
from typing import Dict, List, Any, Optional, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime
import numpy as np
import pandas as pd
import uuid
from concurrent.futures import ThreadPoolExecutor, Future
import queue

from core_modules import logger_manager
from ..utils.exceptions import PredictionException
from ..models import BaseModel
from ..models.model_registry import model_registry


class PredictionStatus(Enum):
    """é¢„æµ‹çŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


class PredictionMode(Enum):
    """é¢„æµ‹æ¨¡å¼æšä¸¾"""
    SINGLE = "single"
    BATCH = "batch"
    STREAMING = "streaming"
    ENSEMBLE = "ensemble"


@dataclass
class PredictionRequest:
    """é¢„æµ‹è¯·æ±‚"""
    request_id: str
    model_name: str
    input_data: np.ndarray
    mode: PredictionMode = PredictionMode.SINGLE
    model_version: Optional[str] = None
    parameters: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    created_time: datetime = field(default_factory=datetime.now)
    priority: int = 0  # ä¼˜å…ˆçº§ï¼Œæ•°å­—è¶Šå¤§ä¼˜å…ˆçº§è¶Šé«˜


@dataclass
class PredictionResult:
    """é¢„æµ‹ç»“æœ"""
    request_id: str
    status: PredictionStatus
    predictions: Optional[np.ndarray] = None
    confidence_scores: Optional[np.ndarray] = None
    model_info: Dict[str, Any] = field(default_factory=dict)
    execution_time: float = 0.0
    error_message: str = ""
    metadata: Dict[str, Any] = field(default_factory=dict)
    completed_time: Optional[datetime] = None


class ModelPool:
    """æ¨¡å‹æ± """
    
    def __init__(self, max_models: int = 10):
        """
        åˆå§‹åŒ–æ¨¡å‹æ± 
        
        Args:
            max_models: æœ€å¤§æ¨¡å‹æ•°é‡
        """
        self.max_models = max_models
        self.models = {}  # model_key -> BaseModel
        self.model_usage = {}  # model_key -> last_used_time
        self.lock = threading.RLock()
        
        logger_manager.info(f"æ¨¡å‹æ± åˆå§‹åŒ–å®Œæˆï¼Œæœ€å¤§å®¹é‡: {max_models}")
    
    def get_model(self, model_name: str, model_version: str = None) -> Optional[BaseModel]:
        """è·å–æ¨¡å‹"""
        try:
            with self.lock:
                model_key = f"{model_name}:{model_version or 'latest'}"
                
                # æ£€æŸ¥æ˜¯å¦å·²åŠ è½½
                if model_key in self.models:
                    self.model_usage[model_key] = time.time()
                    return self.models[model_key]
                
                # ä»æ³¨å†Œè¡¨åŠ è½½æ¨¡å‹
                model_instance = self._load_model_from_registry(model_name, model_version)
                
                if model_instance:
                    # æ£€æŸ¥å®¹é‡é™åˆ¶
                    if len(self.models) >= self.max_models:
                        self._evict_least_used_model()
                    
                    # æ·»åŠ åˆ°æ± ä¸­
                    self.models[model_key] = model_instance
                    self.model_usage[model_key] = time.time()
                    
                    logger_manager.info(f"æ¨¡å‹åŠ è½½åˆ°æ± ä¸­: {model_key}")
                    return model_instance
                
                return None
                
        except Exception as e:
            logger_manager.error(f"è·å–æ¨¡å‹å¤±è´¥: {e}")
            return None
    
    def _load_model_from_registry(self, model_name: str, model_version: str = None) -> Optional[BaseModel]:
        """ä»æ³¨å†Œè¡¨åŠ è½½æ¨¡å‹"""
        try:
            # åˆ›å»ºç®€å•çš„é…ç½®ç±»
            class ModelConfig:
                def __init__(self, **kwargs):
                    for k, v in kwargs.items():
                        setattr(self, k, v)

            class ModelType:
                LSTM = "lstm"
                TRANSFORMER = "transformer"
                GAN = "gan"
            
            # åˆ›å»ºé»˜è®¤é…ç½®
            config = ModelConfig(
                model_type=ModelType.LSTM,  # é»˜è®¤ç±»å‹
                model_name=model_name,
                version=model_version or "1.0.0"
            )
            
            # ä»æ³¨å†Œè¡¨åˆ›å»ºå®ä¾‹
            model_instance = model_registry.create_model_instance(model_name, config, model_version)
            
            return model_instance
            
        except Exception as e:
            logger_manager.error(f"ä»æ³¨å†Œè¡¨åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            return None
    
    def _evict_least_used_model(self):
        """é©±é€æœ€å°‘ä½¿ç”¨çš„æ¨¡å‹"""
        try:
            if not self.model_usage:
                return
            
            # æ‰¾åˆ°æœ€å°‘ä½¿ç”¨çš„æ¨¡å‹
            least_used_key = min(self.model_usage.keys(), 
                                key=lambda k: self.model_usage[k])
            
            # ç§»é™¤æ¨¡å‹
            del self.models[least_used_key]
            del self.model_usage[least_used_key]
            
            logger_manager.info(f"é©±é€æ¨¡å‹: {least_used_key}")
            
        except Exception as e:
            logger_manager.error(f"é©±é€æ¨¡å‹å¤±è´¥: {e}")
    
    def clear_pool(self):
        """æ¸…ç©ºæ¨¡å‹æ± """
        with self.lock:
            self.models.clear()
            self.model_usage.clear()
            logger_manager.info("æ¨¡å‹æ± å·²æ¸…ç©º")
    
    def get_pool_stats(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹æ± ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            return {
                'total_models': len(self.models),
                'max_capacity': self.max_models,
                'loaded_models': list(self.models.keys()),
                'usage_stats': self.model_usage.copy()
            }


class PredictionScheduler:
    """é¢„æµ‹è°ƒåº¦å™¨"""
    
    def __init__(self, max_workers: int = 4):
        """
        åˆå§‹åŒ–é¢„æµ‹è°ƒåº¦å™¨
        
        Args:
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
        """
        self.max_workers = max_workers
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.request_queue = queue.PriorityQueue()
        self.active_requests = {}  # request_id -> Future
        self.completed_requests = {}  # request_id -> PredictionResult
        self.running = False
        self.scheduler_thread = None
        
        logger_manager.info(f"é¢„æµ‹è°ƒåº¦å™¨åˆå§‹åŒ–å®Œæˆï¼Œå·¥ä½œçº¿ç¨‹æ•°: {max_workers}")
    
    def submit_request(self, request: PredictionRequest) -> str:
        """æäº¤é¢„æµ‹è¯·æ±‚"""
        try:
            # æ·»åŠ åˆ°é˜Ÿåˆ—ï¼ˆä¼˜å…ˆçº§é˜Ÿåˆ—ï¼Œè´Ÿæ•°è¡¨ç¤ºé«˜ä¼˜å…ˆçº§ï¼‰
            self.request_queue.put((-request.priority, time.time(), request))
            
            logger_manager.info(f"é¢„æµ‹è¯·æ±‚å·²æäº¤: {request.request_id}")
            return request.request_id
            
        except Exception as e:
            logger_manager.error(f"æäº¤é¢„æµ‹è¯·æ±‚å¤±è´¥: {e}")
            raise PredictionException(f"æäº¤é¢„æµ‹è¯·æ±‚å¤±è´¥: {e}")
    
    def start_scheduler(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        if self.running:
            return
        
        self.running = True
        self.scheduler_thread = threading.Thread(target=self._scheduler_loop)
        self.scheduler_thread.daemon = True
        self.scheduler_thread.start()
        
        logger_manager.info("é¢„æµ‹è°ƒåº¦å™¨å·²å¯åŠ¨")
    
    def stop_scheduler(self):
        """åœæ­¢è°ƒåº¦å™¨"""
        self.running = False
        
        if self.scheduler_thread:
            self.scheduler_thread.join(timeout=5)
        
        self.executor.shutdown(wait=True)
        logger_manager.info("é¢„æµ‹è°ƒåº¦å™¨å·²åœæ­¢")
    
    def _scheduler_loop(self):
        """è°ƒåº¦å™¨å¾ªç¯"""
        while self.running:
            try:
                # è·å–è¯·æ±‚ï¼ˆé˜»å¡1ç§’ï¼‰
                try:
                    _, _, request = self.request_queue.get(timeout=1)
                except queue.Empty:
                    continue
                
                # æäº¤åˆ°çº¿ç¨‹æ± æ‰§è¡Œ
                future = self.executor.submit(self._execute_prediction, request)
                self.active_requests[request.request_id] = future
                
                # æ¸…ç†å·²å®Œæˆçš„è¯·æ±‚
                self._cleanup_completed_requests()
                
            except Exception as e:
                logger_manager.error(f"è°ƒåº¦å™¨å¾ªç¯å¤±è´¥: {e}")
                time.sleep(1)
    
    def _execute_prediction(self, request: PredictionRequest) -> PredictionResult:
        """æ‰§è¡Œé¢„æµ‹"""
        start_time = time.time()
        
        result = PredictionResult(
            request_id=request.request_id,
            status=PredictionStatus.RUNNING
        )
        
        try:
            # è°ƒç”¨å®é™…çš„é¢„æµ‹é€»è¾‘ï¼ŒåŸºäºçœŸå®æ•°æ®
            # è·å–æ¨¡å‹å®ä¾‹
            model = self.model_pool.get_model(request.model_name)

            if model is not None:
                # ä½¿ç”¨çœŸå®æ¨¡å‹è¿›è¡Œé¢„æµ‹
                predictions = model.predict(request.input_data)
                confidence_scores = np.ones(len(request.input_data)) * 0.8  # åŸºäºæ¨¡å‹æ€§èƒ½çš„ç½®ä¿¡åº¦
            else:
                # å¦‚æœæ¨¡å‹ä¸å¯ç”¨ï¼Œè¿”å›é”™è¯¯è€Œä¸æ˜¯éšæœºæ•°
                result.status = PredictionStatus.FAILED
                result.error_message = f"æ¨¡å‹ {request.model_name} ä¸å¯ç”¨"
                return result
            
            result.status = PredictionStatus.COMPLETED
            result.predictions = predictions
            result.confidence_scores = confidence_scores
            result.execution_time = time.time() - start_time
            result.completed_time = datetime.now()
            result.model_info = {
                'model_name': request.model_name,
                'model_version': request.model_version
            }
            
            logger_manager.info(f"é¢„æµ‹å®Œæˆ: {request.request_id}")
            
        except Exception as e:
            result.status = PredictionStatus.FAILED
            result.error_message = str(e)
            result.execution_time = time.time() - start_time
            result.completed_time = datetime.now()
            
            logger_manager.error(f"é¢„æµ‹å¤±è´¥: {request.request_id}, é”™è¯¯: {e}")
        
        # ä¿å­˜ç»“æœ
        self.completed_requests[request.request_id] = result
        
        return result
    
    def _cleanup_completed_requests(self):
        """æ¸…ç†å·²å®Œæˆçš„è¯·æ±‚"""
        try:
            completed_ids = []
            
            for request_id, future in self.active_requests.items():
                if future.done():
                    completed_ids.append(request_id)
            
            for request_id in completed_ids:
                del self.active_requests[request_id]
                
        except Exception as e:
            logger_manager.error(f"æ¸…ç†å·²å®Œæˆè¯·æ±‚å¤±è´¥: {e}")
    
    def get_result(self, request_id: str) -> Optional[PredictionResult]:
        """è·å–é¢„æµ‹ç»“æœ"""
        return self.completed_requests.get(request_id)
    
    def cancel_request(self, request_id: str) -> bool:
        """å–æ¶ˆé¢„æµ‹è¯·æ±‚"""
        try:
            if request_id in self.active_requests:
                future = self.active_requests[request_id]
                if future.cancel():
                    del self.active_requests[request_id]
                    
                    # åˆ›å»ºå–æ¶ˆç»“æœ
                    result = PredictionResult(
                        request_id=request_id,
                        status=PredictionStatus.CANCELLED,
                        completed_time=datetime.now()
                    )
                    self.completed_requests[request_id] = result
                    
                    logger_manager.info(f"é¢„æµ‹è¯·æ±‚å·²å–æ¶ˆ: {request_id}")
                    return True
            
            return False
            
        except Exception as e:
            logger_manager.error(f"å–æ¶ˆé¢„æµ‹è¯·æ±‚å¤±è´¥: {e}")
            return False


class PredictionEngine:
    """é¢„æµ‹å¼•æ“"""
    
    def __init__(self, max_models: int = 10, max_workers: int = 4):
        """
        åˆå§‹åŒ–é¢„æµ‹å¼•æ“
        
        Args:
            max_models: æœ€å¤§æ¨¡å‹æ•°é‡
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
        """
        self.model_pool = ModelPool(max_models)
        self.scheduler = PredictionScheduler(max_workers)
        self.prediction_history = []
        self.lock = threading.RLock()
        
        # å¯åŠ¨è°ƒåº¦å™¨
        self.scheduler.start_scheduler()
        
        logger_manager.info("é¢„æµ‹å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def predict(self, model_name: str, input_data: np.ndarray,
                model_version: str = None, mode: PredictionMode = PredictionMode.SINGLE,
                **kwargs) -> str:
        """
        æäº¤é¢„æµ‹è¯·æ±‚
        
        Args:
            model_name: æ¨¡å‹åç§°
            input_data: è¾“å…¥æ•°æ®
            model_version: æ¨¡å‹ç‰ˆæœ¬
            mode: é¢„æµ‹æ¨¡å¼
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            è¯·æ±‚ID
        """
        try:
            # åˆ›å»ºé¢„æµ‹è¯·æ±‚
            request = PredictionRequest(
                request_id=str(uuid.uuid4()),
                model_name=model_name,
                input_data=input_data,
                mode=mode,
                model_version=model_version,
                parameters=kwargs
            )
            
            # æäº¤è¯·æ±‚
            request_id = self.scheduler.submit_request(request)
            
            # è®°å½•å†å²
            with self.lock:
                self.prediction_history.append({
                    'request_id': request_id,
                    'model_name': model_name,
                    'timestamp': datetime.now(),
                    'input_shape': input_data.shape
                })
            
            return request_id
            
        except Exception as e:
            logger_manager.error(f"æäº¤é¢„æµ‹å¤±è´¥: {e}")
            raise PredictionException(f"æäº¤é¢„æµ‹å¤±è´¥: {e}")
    
    def get_result(self, request_id: str) -> Optional[PredictionResult]:
        """è·å–é¢„æµ‹ç»“æœ"""
        return self.scheduler.get_result(request_id)
    
    def wait_for_result(self, request_id: str, timeout: float = 30.0) -> Optional[PredictionResult]:
        """ç­‰å¾…é¢„æµ‹ç»“æœ"""
        try:
            start_time = time.time()
            
            while time.time() - start_time < timeout:
                result = self.get_result(request_id)
                
                if result and result.status in [PredictionStatus.COMPLETED, 
                                               PredictionStatus.FAILED, 
                                               PredictionStatus.CANCELLED]:
                    return result
                
                time.sleep(0.1)
            
            logger_manager.warning(f"ç­‰å¾…é¢„æµ‹ç»“æœè¶…æ—¶: {request_id}")
            return None
            
        except Exception as e:
            logger_manager.error(f"ç­‰å¾…é¢„æµ‹ç»“æœå¤±è´¥: {e}")
            return None
    
    def predict_sync(self, model_name: str, input_data: np.ndarray,
                    model_version: str = None, timeout: float = 30.0,
                    **kwargs) -> Optional[PredictionResult]:
        """
        åŒæ­¥é¢„æµ‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            input_data: è¾“å…¥æ•°æ®
            model_version: æ¨¡å‹ç‰ˆæœ¬
            timeout: è¶…æ—¶æ—¶é—´
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            é¢„æµ‹ç»“æœ
        """
        try:
            # æäº¤é¢„æµ‹è¯·æ±‚
            request_id = self.predict(model_name, input_data, model_version, **kwargs)
            
            # ç­‰å¾…ç»“æœ
            result = self.wait_for_result(request_id, timeout)
            
            return result
            
        except Exception as e:
            logger_manager.error(f"åŒæ­¥é¢„æµ‹å¤±è´¥: {e}")
            return None
    
    def batch_predict(self, model_name: str, input_data_list: List[np.ndarray],
                     model_version: str = None, **kwargs) -> List[str]:
        """
        æ‰¹é‡é¢„æµ‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            input_data_list: è¾“å…¥æ•°æ®åˆ—è¡¨
            model_version: æ¨¡å‹ç‰ˆæœ¬
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            è¯·æ±‚IDåˆ—è¡¨
        """
        try:
            request_ids = []
            
            for input_data in input_data_list:
                request_id = self.predict(
                    model_name, input_data, model_version,
                    mode=PredictionMode.BATCH, **kwargs
                )
                request_ids.append(request_id)
            
            logger_manager.info(f"æ‰¹é‡é¢„æµ‹è¯·æ±‚å·²æäº¤: {len(request_ids)} ä¸ª")
            return request_ids
            
        except Exception as e:
            logger_manager.error(f"æ‰¹é‡é¢„æµ‹å¤±è´¥: {e}")
            raise PredictionException(f"æ‰¹é‡é¢„æµ‹å¤±è´¥: {e}")
    
    def cancel_prediction(self, request_id: str) -> bool:
        """å–æ¶ˆé¢„æµ‹"""
        return self.scheduler.cancel_request(request_id)
    
    def get_engine_stats(self) -> Dict[str, Any]:
        """è·å–å¼•æ“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            with self.lock:
                stats = {
                    'total_predictions': len(self.prediction_history),
                    'model_pool_stats': self.model_pool.get_pool_stats(),
                    'active_requests': len(self.scheduler.active_requests),
                    'completed_requests': len(self.scheduler.completed_requests),
                    'recent_predictions': self.prediction_history[-10:] if self.prediction_history else []
                }
                
                return stats
                
        except Exception as e:
            logger_manager.error(f"è·å–å¼•æ“ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    def shutdown(self):
        """å…³é—­é¢„æµ‹å¼•æ“"""
        try:
            self.scheduler.stop_scheduler()
            self.model_pool.clear_pool()
            
            logger_manager.info("é¢„æµ‹å¼•æ“å·²å…³é—­")
            
        except Exception as e:
            logger_manager.error(f"å…³é—­é¢„æµ‹å¼•æ“å¤±è´¥: {e}")


# å…¨å±€é¢„æµ‹å¼•æ“å®ä¾‹
prediction_engine = PredictionEngine()


if __name__ == "__main__":
    # æµ‹è¯•é¢„æµ‹å¼•æ“åŠŸèƒ½
    print("ğŸ”® æµ‹è¯•é¢„æµ‹å¼•æ“åŠŸèƒ½...")
    
    try:
        import numpy as np
        
        # åˆ›å»ºæµ‹è¯•å¼•æ“
        engine = PredictionEngine(max_models=5, max_workers=2)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = np.random.random((10, 5))
        
        # æµ‹è¯•åŒæ­¥é¢„æµ‹
        result = engine.predict_sync("test_model", test_data, timeout=5.0)
        
        if result:
            print(f"âœ… åŒæ­¥é¢„æµ‹æˆåŠŸ: {result.status.value}")
            if result.predictions is not None:
                print(f"   é¢„æµ‹ç»“æœå½¢çŠ¶: {result.predictions.shape}")
        else:
            print("âš ï¸ åŒæ­¥é¢„æµ‹è¿”å›ç©ºç»“æœ")
        
        # æµ‹è¯•å¼‚æ­¥é¢„æµ‹
        request_id = engine.predict("test_model", test_data)
        print(f"âœ… å¼‚æ­¥é¢„æµ‹è¯·æ±‚å·²æäº¤: {request_id}")
        
        # ç­‰å¾…ç»“æœ
        async_result = engine.wait_for_result(request_id, timeout=5.0)
        if async_result:
            print(f"âœ… å¼‚æ­¥é¢„æµ‹å®Œæˆ: {async_result.status.value}")
        
        # æµ‹è¯•æ‰¹é‡é¢„æµ‹
        batch_data = [np.random.random((5, 5)) for _ in range(3)]
        batch_ids = engine.batch_predict("test_model", batch_data)
        print(f"âœ… æ‰¹é‡é¢„æµ‹è¯·æ±‚å·²æäº¤: {len(batch_ids)} ä¸ª")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = engine.get_engine_stats()
        print(f"âœ… å¼•æ“ç»Ÿè®¡ä¿¡æ¯: æ€»é¢„æµ‹æ•° {stats['total_predictions']}")
        
        # å…³é—­å¼•æ“
        engine.shutdown()
        print("âœ… é¢„æµ‹å¼•æ“å·²å…³é—­")
        
        print("âœ… é¢„æµ‹å¼•æ“åŠŸèƒ½æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print("é¢„æµ‹å¼•æ“åŠŸèƒ½æµ‹è¯•å®Œæˆ")
