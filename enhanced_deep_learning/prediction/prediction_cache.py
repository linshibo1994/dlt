#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
é¢„æµ‹ç¼“å­˜æ¨¡å—
Prediction Cache Module

æä¾›é¢„æµ‹ç»“æœç¼“å­˜ã€ç¼“å­˜ç­–ç•¥ã€ç¼“å­˜ç®¡ç†ç­‰åŠŸèƒ½ã€‚
"""

import os
import time
import json
import pickle
import hashlib
import threading
from typing import Dict, List, Any, Optional, Union, Tuple
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timedelta
from collections import OrderedDict
import numpy as np

from core_modules import logger_manager
from ..utils.exceptions import PredictionException
from .prediction_engine import PredictionResult, PredictionRequest


class CacheStrategy(Enum):
    """ç¼“å­˜ç­–ç•¥æšä¸¾"""
    LRU = "lru"  # æœ€è¿‘æœ€å°‘ä½¿ç”¨
    LFU = "lfu"  # æœ€å°‘ä½¿ç”¨é¢‘ç‡
    TTL = "ttl"  # ç”Ÿå­˜æ—¶é—´
    FIFO = "fifo"  # å…ˆè¿›å…ˆå‡º
    ADAPTIVE = "adaptive"  # è‡ªé€‚åº”


@dataclass
class CacheEntry:
    """ç¼“å­˜æ¡ç›®"""
    key: str
    value: PredictionResult
    created_time: datetime
    last_accessed: datetime
    access_count: int = 0
    ttl: Optional[int] = None  # ç”Ÿå­˜æ—¶é—´ï¼ˆç§’ï¼‰
    size: int = 0  # æ¡ç›®å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def is_expired(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¿‡æœŸ"""
        if self.ttl is None:
            return False
        
        return (datetime.now() - self.created_time).total_seconds() > self.ttl
    
    def update_access(self):
        """æ›´æ–°è®¿é—®ä¿¡æ¯"""
        self.last_accessed = datetime.now()
        self.access_count += 1


class CacheKeyGenerator:
    """ç¼“å­˜é”®ç”Ÿæˆå™¨"""
    
    @staticmethod
    def generate_key(request: PredictionRequest) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®
        
        Args:
            request: é¢„æµ‹è¯·æ±‚
            
        Returns:
            ç¼“å­˜é”®
        """
        try:
            # åˆ›å»ºé”®çš„ç»„æˆéƒ¨åˆ†
            key_parts = [
                request.model_name,
                request.model_version or "latest",
                request.mode.value
            ]
            
            # æ·»åŠ è¾“å…¥æ•°æ®çš„å“ˆå¸Œ
            input_hash = hashlib.md5(request.input_data.tobytes()).hexdigest()[:16]
            key_parts.append(input_hash)
            
            # æ·»åŠ å‚æ•°çš„å“ˆå¸Œ
            if request.parameters:
                params_str = json.dumps(request.parameters, sort_keys=True)
                params_hash = hashlib.md5(params_str.encode()).hexdigest()[:8]
                key_parts.append(params_hash)
            
            # ç»„åˆé”®
            cache_key = ":".join(key_parts)
            
            return cache_key
            
        except Exception as e:
            logger_manager.error(f"ç”Ÿæˆç¼“å­˜é”®å¤±è´¥: {e}")
            return f"fallback:{request.request_id}"
    
    @staticmethod
    def generate_key_from_data(model_name: str, input_data: np.ndarray,
                              model_version: str = None, **kwargs) -> str:
        """
        ä»æ•°æ®ç”Ÿæˆç¼“å­˜é”®
        
        Args:
            model_name: æ¨¡å‹åç§°
            input_data: è¾“å…¥æ•°æ®
            model_version: æ¨¡å‹ç‰ˆæœ¬
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ç¼“å­˜é”®
        """
        try:
            key_parts = [
                model_name,
                model_version or "latest"
            ]
            
            # æ·»åŠ è¾“å…¥æ•°æ®çš„å“ˆå¸Œ
            input_hash = hashlib.md5(input_data.tobytes()).hexdigest()[:16]
            key_parts.append(input_hash)
            
            # æ·»åŠ å‚æ•°çš„å“ˆå¸Œ
            if kwargs:
                params_str = json.dumps(kwargs, sort_keys=True)
                params_hash = hashlib.md5(params_str.encode()).hexdigest()[:8]
                key_parts.append(params_hash)
            
            return ":".join(key_parts)
            
        except Exception as e:
            logger_manager.error(f"ä»æ•°æ®ç”Ÿæˆç¼“å­˜é”®å¤±è´¥: {e}")
            return f"fallback:{int(time.time())}"


class LRUCache:
    """LRUç¼“å­˜å®ç°"""
    
    def __init__(self, max_size: int = 1000):
        """
        åˆå§‹åŒ–LRUç¼“å­˜
        
        Args:
            max_size: æœ€å¤§ç¼“å­˜å¤§å°
        """
        self.max_size = max_size
        self.cache = OrderedDict()
        self.lock = threading.RLock()
        
        logger_manager.debug(f"LRUç¼“å­˜åˆå§‹åŒ–ï¼Œæœ€å¤§å¤§å°: {max_size}")
    
    def get(self, key: str) -> Optional[CacheEntry]:
        """è·å–ç¼“å­˜é¡¹"""
        with self.lock:
            if key in self.cache:
                entry = self.cache.pop(key)
                entry.update_access()
                self.cache[key] = entry  # ç§»åŠ¨åˆ°æœ«å°¾
                return entry
            return None
    
    def put(self, key: str, entry: CacheEntry):
        """æ·»åŠ ç¼“å­˜é¡¹"""
        with self.lock:
            if key in self.cache:
                self.cache.pop(key)
            elif len(self.cache) >= self.max_size:
                # ç§»é™¤æœ€ä¹…æœªä½¿ç”¨çš„é¡¹
                self.cache.popitem(last=False)
            
            self.cache[key] = entry
    
    def remove(self, key: str) -> bool:
        """ç§»é™¤ç¼“å­˜é¡¹"""
        with self.lock:
            if key in self.cache:
                del self.cache[key]
                return True
            return False
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self.lock:
            self.cache.clear()
    
    def size(self) -> int:
        """è·å–ç¼“å­˜å¤§å°"""
        with self.lock:
            return len(self.cache)
    
    def keys(self) -> List[str]:
        """è·å–æ‰€æœ‰é”®"""
        with self.lock:
            return list(self.cache.keys())


class TTLCache:
    """TTLç¼“å­˜å®ç°"""
    
    def __init__(self, default_ttl: int = 3600):
        """
        åˆå§‹åŒ–TTLç¼“å­˜
        
        Args:
            default_ttl: é»˜è®¤ç”Ÿå­˜æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.default_ttl = default_ttl
        self.cache = {}
        self.lock = threading.RLock()
        
        logger_manager.debug(f"TTLç¼“å­˜åˆå§‹åŒ–ï¼Œé»˜è®¤TTL: {default_ttl}ç§’")
    
    def get(self, key: str) -> Optional[CacheEntry]:
        """è·å–ç¼“å­˜é¡¹"""
        with self.lock:
            if key in self.cache:
                entry = self.cache[key]
                if entry.is_expired():
                    del self.cache[key]
                    return None
                
                entry.update_access()
                return entry
            return None
    
    def put(self, key: str, entry: CacheEntry):
        """æ·»åŠ ç¼“å­˜é¡¹"""
        with self.lock:
            if entry.ttl is None:
                entry.ttl = self.default_ttl
            
            self.cache[key] = entry
    
    def remove(self, key: str) -> bool:
        """ç§»é™¤ç¼“å­˜é¡¹"""
        with self.lock:
            if key in self.cache:
                del self.cache[key]
                return True
            return False
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self.lock:
            self.cache.clear()
    
    def cleanup_expired(self) -> int:
        """æ¸…ç†è¿‡æœŸé¡¹"""
        with self.lock:
            expired_keys = []
            for key, entry in self.cache.items():
                if entry.is_expired():
                    expired_keys.append(key)
            
            for key in expired_keys:
                del self.cache[key]
            
            return len(expired_keys)
    
    def size(self) -> int:
        """è·å–ç¼“å­˜å¤§å°"""
        with self.lock:
            return len(self.cache)


class PredictionCache:
    """é¢„æµ‹ç¼“å­˜ä¸»ç±»"""
    
    def __init__(self, strategy: CacheStrategy = CacheStrategy.LRU,
                 max_size: int = 1000, default_ttl: int = 3600,
                 enable_persistence: bool = False, cache_dir: str = "cache"):
        """
        åˆå§‹åŒ–é¢„æµ‹ç¼“å­˜
        
        Args:
            strategy: ç¼“å­˜ç­–ç•¥
            max_size: æœ€å¤§ç¼“å­˜å¤§å°
            default_ttl: é»˜è®¤ç”Ÿå­˜æ—¶é—´
            enable_persistence: æ˜¯å¦å¯ç”¨æŒä¹…åŒ–
            cache_dir: ç¼“å­˜ç›®å½•
        """
        self.strategy = strategy
        self.max_size = max_size
        self.default_ttl = default_ttl
        self.enable_persistence = enable_persistence
        self.cache_dir = cache_dir
        
        # åˆ›å»ºç¼“å­˜å®ç°
        if strategy == CacheStrategy.LRU:
            self.cache_impl = LRUCache(max_size)
        elif strategy == CacheStrategy.TTL:
            self.cache_impl = TTLCache(default_ttl)
        else:
            # é»˜è®¤ä½¿ç”¨LRU
            self.cache_impl = LRUCache(max_size)
        
        self.key_generator = CacheKeyGenerator()
        self.stats = {
            'hits': 0,
            'misses': 0,
            'puts': 0,
            'removes': 0,
            'evictions': 0
        }
        self.lock = threading.RLock()
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        if enable_persistence:
            os.makedirs(cache_dir, exist_ok=True)
        
        # å¯åŠ¨æ¸…ç†çº¿ç¨‹
        self.cleanup_thread = None
        self.running = False
        if strategy == CacheStrategy.TTL:
            self.start_cleanup_thread()
        
        logger_manager.info(f"é¢„æµ‹ç¼“å­˜åˆå§‹åŒ–å®Œæˆï¼Œç­–ç•¥: {strategy.value}")
    
    def get(self, request: PredictionRequest) -> Optional[PredictionResult]:
        """
        è·å–ç¼“å­˜çš„é¢„æµ‹ç»“æœ
        
        Args:
            request: é¢„æµ‹è¯·æ±‚
            
        Returns:
            ç¼“å­˜çš„é¢„æµ‹ç»“æœ
        """
        try:
            cache_key = self.key_generator.generate_key(request)
            
            with self.lock:
                entry = self.cache_impl.get(cache_key)
                
                if entry:
                    self.stats['hits'] += 1
                    logger_manager.debug(f"ç¼“å­˜å‘½ä¸­: {cache_key}")
                    return entry.value
                else:
                    self.stats['misses'] += 1
                    logger_manager.debug(f"ç¼“å­˜æœªå‘½ä¸­: {cache_key}")
                    return None
                    
        except Exception as e:
            logger_manager.error(f"è·å–ç¼“å­˜å¤±è´¥: {e}")
            return None
    
    def put(self, request: PredictionRequest, result: PredictionResult,
            ttl: Optional[int] = None):
        """
        ç¼“å­˜é¢„æµ‹ç»“æœ
        
        Args:
            request: é¢„æµ‹è¯·æ±‚
            result: é¢„æµ‹ç»“æœ
            ttl: ç”Ÿå­˜æ—¶é—´
        """
        try:
            cache_key = self.key_generator.generate_key(request)
            
            # è®¡ç®—æ¡ç›®å¤§å°
            entry_size = self._calculate_size(result)
            
            # åˆ›å»ºç¼“å­˜æ¡ç›®
            entry = CacheEntry(
                key=cache_key,
                value=result,
                created_time=datetime.now(),
                last_accessed=datetime.now(),
                ttl=ttl or self.default_ttl,
                size=entry_size
            )
            
            with self.lock:
                self.cache_impl.put(cache_key, entry)
                self.stats['puts'] += 1
            
            # æŒä¹…åŒ–
            if self.enable_persistence:
                self._persist_entry(cache_key, entry)
            
            logger_manager.debug(f"ç¼“å­˜å·²æ·»åŠ : {cache_key}")
            
        except Exception as e:
            logger_manager.error(f"æ·»åŠ ç¼“å­˜å¤±è´¥: {e}")

    def set(self, key: str, value: Any, ttl: Optional[int] = None):
        """
        è®¾ç½®ç¼“å­˜å€¼ï¼ˆç®€åŒ–æ¥å£ï¼‰

        Args:
            key: ç¼“å­˜é”®
            value: ç¼“å­˜å€¼
            ttl: ç”Ÿå­˜æ—¶é—´ï¼ˆç§’ï¼‰
        """
        try:
            # ä½¿ç”¨åŸå§‹çš„CacheEntryç±»
            from datetime import datetime

            # åˆ›å»ºä¸€ä¸ªç®€å•çš„é¢„æµ‹ç»“æœåŒ…è£…å™¨
            class SimpleResult:
                def __init__(self, value):
                    self.value = value

            entry = CacheEntry(
                key=key,
                value=SimpleResult(value),
                created_time=datetime.now(),
                last_accessed=datetime.now(),
                ttl=ttl
            )

            with self.lock:
                self.cache_impl.put(key, entry)
                self.stats['puts'] += 1

            logger_manager.debug(f"ç¼“å­˜å€¼å·²è®¾ç½®: {key}")

        except Exception as e:
            logger_manager.error(f"è®¾ç½®ç¼“å­˜å€¼å¤±è´¥: {e}")

    def get_simple(self, key: str) -> Optional[Any]:
        """
        è·å–ç¼“å­˜å€¼ï¼ˆç®€åŒ–æ¥å£ï¼‰

        Args:
            key: ç¼“å­˜é”®

        Returns:
            ç¼“å­˜å€¼
        """
        try:
            with self.lock:
                entry = self.cache_impl.get(key)

                if entry:
                    # æ£€æŸ¥TTLï¼ˆä½¿ç”¨åŸå§‹çš„is_expiredæ–¹æ³•ï¼‰
                    if hasattr(entry, 'is_expired') and entry.is_expired():
                        self.cache_impl.remove(key)
                        self.stats['misses'] += 1
                        return None

                    self.stats['hits'] += 1

                    # è¿”å›åŒ…è£…çš„å€¼
                    if hasattr(entry.value, 'value'):
                        return entry.value.value
                    else:
                        return entry.value
                else:
                    self.stats['misses'] += 1
                    return None

        except Exception as e:
            logger_manager.error(f"è·å–ç¼“å­˜å€¼å¤±è´¥: {e}")
            return None

    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        try:
            with self.lock:
                total_requests = self.stats['hits'] + self.stats['misses']
                hit_rate = self.stats['hits'] / total_requests if total_requests > 0 else 0.0

                return {
                    'hits': self.stats['hits'],
                    'misses': self.stats['misses'],
                    'puts': self.stats['puts'],
                    'removes': self.stats['removes'],
                    'hit_rate': hit_rate,
                    'total_requests': total_requests,
                    'cache_size': len(self.cache_impl.cache) if hasattr(self.cache_impl, 'cache') else 0
                }
        except Exception as e:
            logger_manager.error(f"è·å–ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {e}")
            return {}

    def remove(self, request: PredictionRequest) -> bool:
        """
        ç§»é™¤ç¼“å­˜é¡¹
        
        Args:
            request: é¢„æµ‹è¯·æ±‚
            
        Returns:
            æ˜¯å¦ç§»é™¤æˆåŠŸ
        """
        try:
            cache_key = self.key_generator.generate_key(request)
            
            with self.lock:
                success = self.cache_impl.remove(cache_key)
            
            # åˆ é™¤æŒä¹…åŒ–æ–‡ä»¶
            if self.enable_persistence and success:
                self._remove_persisted_entry(cache_key)
            
            if success:
                logger_manager.debug(f"ç¼“å­˜å·²ç§»é™¤: {cache_key}")
            
            return success
            
        except Exception as e:
            logger_manager.error(f"ç§»é™¤ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        try:
            with self.lock:
                self.cache_impl.clear()
                self.stats = {
                    'hits': 0,
                    'misses': 0,
                    'puts': 0,
                    'evictions': 0
                }
            
            # æ¸…ç©ºæŒä¹…åŒ–æ–‡ä»¶
            if self.enable_persistence:
                self._clear_persisted_cache()
            
            logger_manager.info("ç¼“å­˜å·²æ¸…ç©º")
            
        except Exception as e:
            logger_manager.error(f"æ¸…ç©ºç¼“å­˜å¤±è´¥: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            total_requests = self.stats['hits'] + self.stats['misses']
            hit_rate = self.stats['hits'] / total_requests if total_requests > 0 else 0
            
            return {
                'strategy': self.strategy.value,
                'size': self.cache_impl.size(),
                'max_size': self.max_size,
                'hit_rate': hit_rate,
                'stats': self.stats.copy(),
                'memory_usage': self._calculate_memory_usage()
            }
    
    def start_cleanup_thread(self):
        """å¯åŠ¨æ¸…ç†çº¿ç¨‹"""
        if self.running:
            return
        
        self.running = True
        self.cleanup_thread = threading.Thread(target=self._cleanup_loop)
        self.cleanup_thread.daemon = True
        self.cleanup_thread.start()
        
        logger_manager.debug("ç¼“å­˜æ¸…ç†çº¿ç¨‹å·²å¯åŠ¨")
    
    def stop_cleanup_thread(self):
        """åœæ­¢æ¸…ç†çº¿ç¨‹"""
        try:
            self.running = False
            if self.cleanup_thread:
                self.cleanup_thread.join(timeout=5)

            logger_manager.debug("ç¼“å­˜æ¸…ç†çº¿ç¨‹å·²åœæ­¢")
        except Exception:
            # åœ¨Pythonå…³é—­æ—¶ï¼ŒæŸäº›æ¨¡å—å¯èƒ½å·²ç»è¢«æ¸…ç†ï¼Œå¿½ç•¥é”™è¯¯
            pass
    
    def _cleanup_loop(self):
        """æ¸…ç†å¾ªç¯"""
        while self.running:
            try:
                if hasattr(self.cache_impl, 'cleanup_expired'):
                    expired_count = self.cache_impl.cleanup_expired()
                    if expired_count > 0:
                        logger_manager.debug(f"æ¸…ç†è¿‡æœŸç¼“å­˜é¡¹: {expired_count} ä¸ª")
                
                time.sleep(300)  # æ¯5åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
                
            except Exception as e:
                logger_manager.error(f"ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")
                time.sleep(60)
    
    def _calculate_size(self, result: PredictionResult) -> int:
        """è®¡ç®—ç»“æœå¤§å°"""
        try:
            size = 0
            
            if result.predictions is not None:
                size += result.predictions.nbytes
            
            if result.confidence_scores is not None:
                size += result.confidence_scores.nbytes
            
            # ä¼°ç®—å…¶ä»–å­—æ®µå¤§å°
            size += len(str(result.request_id)) * 2
            size += len(str(result.error_message)) * 2
            size += len(json.dumps(result.model_info)) * 2
            size += len(json.dumps(result.metadata)) * 2
            
            return size
            
        except Exception:
            return 1024  # é»˜è®¤1KB
    
    def _calculate_memory_usage(self) -> int:
        """è®¡ç®—å†…å­˜ä½¿ç”¨é‡"""
        try:
            total_size = 0
            
            if hasattr(self.cache_impl, 'cache'):
                for entry in self.cache_impl.cache.values():
                    total_size += entry.size
            
            return total_size
            
        except Exception:
            return 0
    
    def _persist_entry(self, key: str, entry: CacheEntry):
        """æŒä¹…åŒ–ç¼“å­˜æ¡ç›®"""
        try:
            if not self.enable_persistence:
                return
            
            file_path = os.path.join(self.cache_dir, f"{key}.pkl")
            
            with open(file_path, 'wb') as f:
                pickle.dump(entry, f)
                
        except Exception as e:
            logger_manager.error(f"æŒä¹…åŒ–ç¼“å­˜æ¡ç›®å¤±è´¥: {e}")
    
    def _remove_persisted_entry(self, key: str):
        """åˆ é™¤æŒä¹…åŒ–çš„ç¼“å­˜æ¡ç›®"""
        try:
            file_path = os.path.join(self.cache_dir, f"{key}.pkl")
            
            if os.path.exists(file_path):
                os.remove(file_path)
                
        except Exception as e:
            logger_manager.error(f"åˆ é™¤æŒä¹…åŒ–ç¼“å­˜æ¡ç›®å¤±è´¥: {e}")
    
    def _clear_persisted_cache(self):
        """æ¸…ç©ºæŒä¹…åŒ–ç¼“å­˜"""
        try:
            if not os.path.exists(self.cache_dir):
                return
            
            for filename in os.listdir(self.cache_dir):
                if filename.endswith('.pkl'):
                    file_path = os.path.join(self.cache_dir, filename)
                    os.remove(file_path)
                    
        except Exception as e:
            logger_manager.error(f"æ¸…ç©ºæŒä¹…åŒ–ç¼“å­˜å¤±è´¥: {e}")
    
    def __del__(self):
        """ææ„å‡½æ•°"""
        try:
            self.stop_cleanup_thread()
        except Exception:
            # åœ¨Pythonå…³é—­æ—¶ï¼ŒæŸäº›æ¨¡å—å¯èƒ½å·²ç»è¢«æ¸…ç†ï¼Œå¿½ç•¥é”™è¯¯
            pass


# å…¨å±€é¢„æµ‹ç¼“å­˜å®ä¾‹
prediction_cache = PredictionCache()


if __name__ == "__main__":
    # æµ‹è¯•é¢„æµ‹ç¼“å­˜åŠŸèƒ½
    print("ğŸ’¾ æµ‹è¯•é¢„æµ‹ç¼“å­˜åŠŸèƒ½...")
    
    try:
        from .prediction_engine import PredictionRequest, PredictionResult, PredictionMode, PredictionStatus
        import numpy as np
        
        # åˆ›å»ºæµ‹è¯•ç¼“å­˜
        cache = PredictionCache(
            strategy=CacheStrategy.LRU,
            max_size=100,
            enable_persistence=False
        )
        
        # åˆ›å»ºæµ‹è¯•è¯·æ±‚å’Œç»“æœ
        test_request = PredictionRequest(
            request_id="test_001",
            model_name="test_model",
            input_data=np.random.random((10, 5)),
            mode=PredictionMode.SINGLE
        )
        
        test_result = PredictionResult(
            request_id="test_001",
            status=PredictionStatus.COMPLETED,
            predictions=np.random.random((10, 2)),
            confidence_scores=np.random.random(10),
            execution_time=1.5
        )
        
        # æµ‹è¯•ç¼“å­˜æ·»åŠ 
        cache.put(test_request, test_result)
        print("âœ… ç¼“å­˜æ·»åŠ æˆåŠŸ")
        
        # æµ‹è¯•ç¼“å­˜è·å–
        cached_result = cache.get(test_request)
        if cached_result:
            print("âœ… ç¼“å­˜è·å–æˆåŠŸ")
        else:
            print("âŒ ç¼“å­˜è·å–å¤±è´¥")
        
        # æµ‹è¯•ç¼“å­˜ç»Ÿè®¡
        stats = cache.get_stats()
        print(f"âœ… ç¼“å­˜ç»Ÿè®¡è·å–æˆåŠŸï¼Œå‘½ä¸­ç‡: {stats['hit_rate']:.2f}")
        
        # æµ‹è¯•ç¼“å­˜ç§»é™¤
        if cache.remove(test_request):
            print("âœ… ç¼“å­˜ç§»é™¤æˆåŠŸ")
        
        # æµ‹è¯•ç¼“å­˜æ¸…ç©º
        cache.clear()
        print("âœ… ç¼“å­˜æ¸…ç©ºæˆåŠŸ")
        
        print("âœ… é¢„æµ‹ç¼“å­˜åŠŸèƒ½æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print("é¢„æµ‹ç¼“å­˜åŠŸèƒ½æµ‹è¯•å®Œæˆ")
