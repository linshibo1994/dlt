#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å·¥ä½œæµç®¡ç†å™¨æ¨¡å—
Workflow Manager Module

æä¾›å·¥ä½œæµå®šä¹‰ã€æ‰§è¡Œã€ç›‘æ§å’Œç®¡ç†åŠŸèƒ½ã€‚
"""

import os
import json
import threading
import time
from typing import Dict, List, Any, Optional, Callable, Union
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timedelta
import uuid

from core_modules import logger_manager
from ..utils.exceptions import DeepLearningException


class WorkflowStatus(Enum):
    """å·¥ä½œæµçŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"
    PAUSED = "paused"


class TaskType(Enum):
    """ä»»åŠ¡ç±»å‹æšä¸¾"""
    DATA_PROCESSING = "data_processing"
    MODEL_TRAINING = "model_training"
    MODEL_EVALUATION = "model_evaluation"
    PREDICTION = "prediction"
    VISUALIZATION = "visualization"
    NOTIFICATION = "notification"
    CUSTOM = "custom"


@dataclass
class WorkflowTask:
    """å·¥ä½œæµä»»åŠ¡"""
    task_id: str
    name: str
    task_type: TaskType
    handler: Callable
    dependencies: List[str] = field(default_factory=list)
    parameters: Dict[str, Any] = field(default_factory=dict)
    timeout: Optional[int] = None
    retry_count: int = 0
    max_retries: int = 3
    status: WorkflowStatus = WorkflowStatus.PENDING
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    result: Any = None
    error_message: str = ""


@dataclass
class WorkflowConfig:
    """å·¥ä½œæµé…ç½®"""
    workflow_id: str
    name: str
    description: str = ""
    tasks: List[WorkflowTask] = field(default_factory=list)
    parallel_execution: bool = False
    max_parallel_tasks: int = 4
    timeout: Optional[int] = None
    auto_retry: bool = True
    notification_enabled: bool = False
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class WorkflowExecution:
    """å·¥ä½œæµæ‰§è¡Œè®°å½•"""
    execution_id: str
    workflow_id: str
    status: WorkflowStatus
    start_time: datetime
    end_time: Optional[datetime] = None
    completed_tasks: int = 0
    failed_tasks: int = 0
    total_tasks: int = 0
    progress: float = 0.0
    error_message: str = ""
    results: Dict[str, Any] = field(default_factory=dict)


class WorkflowExecutor:
    """å·¥ä½œæµæ‰§è¡Œå™¨"""
    
    def __init__(self, max_workers: int = 4):
        """
        åˆå§‹åŒ–å·¥ä½œæµæ‰§è¡Œå™¨
        
        Args:
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
        """
        self.max_workers = max_workers
        self.active_executions = {}
        self.execution_history = []
        self.lock = threading.RLock()
        
        logger_manager.info(f"å·¥ä½œæµæ‰§è¡Œå™¨åˆå§‹åŒ–å®Œæˆï¼Œæœ€å¤§å·¥ä½œçº¿ç¨‹: {max_workers}")
    
    def execute_workflow(self, config: WorkflowConfig) -> str:
        """
        æ‰§è¡Œå·¥ä½œæµ
        
        Args:
            config: å·¥ä½œæµé…ç½®
            
        Returns:
            æ‰§è¡ŒID
        """
        try:
            execution_id = str(uuid.uuid4())
            
            # åˆ›å»ºæ‰§è¡Œè®°å½•
            execution = WorkflowExecution(
                execution_id=execution_id,
                workflow_id=config.workflow_id,
                status=WorkflowStatus.RUNNING,
                start_time=datetime.now(),
                total_tasks=len(config.tasks)
            )
            
            with self.lock:
                self.active_executions[execution_id] = execution
            
            # å¯åŠ¨æ‰§è¡Œçº¿ç¨‹
            execution_thread = threading.Thread(
                target=self._execute_workflow_thread,
                args=(config, execution)
            )
            execution_thread.daemon = True
            execution_thread.start()
            
            logger_manager.info(f"å·¥ä½œæµæ‰§è¡Œå·²å¯åŠ¨: {execution_id}")
            return execution_id
            
        except Exception as e:
            logger_manager.error(f"å¯åŠ¨å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
            raise DeepLearningException(f"å¯åŠ¨å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
    
    def _execute_workflow_thread(self, config: WorkflowConfig, execution: WorkflowExecution):
        """å·¥ä½œæµæ‰§è¡Œçº¿ç¨‹"""
        try:
            logger_manager.info(f"å¼€å§‹æ‰§è¡Œå·¥ä½œæµ: {config.name}")
            
            # æ„å»ºä»»åŠ¡ä¾èµ–å›¾
            task_graph = self._build_task_graph(config.tasks)
            
            # æ‰§è¡Œä»»åŠ¡
            if config.parallel_execution:
                self._execute_parallel(task_graph, execution)
            else:
                self._execute_sequential(task_graph, execution)
            
            # æ›´æ–°æ‰§è¡ŒçŠ¶æ€
            execution.status = WorkflowStatus.COMPLETED
            execution.end_time = datetime.now()
            execution.progress = 100.0
            
            logger_manager.info(f"å·¥ä½œæµæ‰§è¡Œå®Œæˆ: {config.name}")
            
        except Exception as e:
            execution.status = WorkflowStatus.FAILED
            execution.error_message = str(e)
            execution.end_time = datetime.now()
            
            logger_manager.error(f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
        
        finally:
            # ç§»åŠ¨åˆ°å†å²è®°å½•
            with self.lock:
                if execution.execution_id in self.active_executions:
                    del self.active_executions[execution.execution_id]
                self.execution_history.append(execution)
    
    def _build_task_graph(self, tasks: List[WorkflowTask]) -> Dict[str, WorkflowTask]:
        """æ„å»ºä»»åŠ¡ä¾èµ–å›¾"""
        task_graph = {}
        
        for task in tasks:
            task_graph[task.task_id] = task
        
        return task_graph
    
    def _execute_sequential(self, task_graph: Dict[str, WorkflowTask], 
                          execution: WorkflowExecution):
        """é¡ºåºæ‰§è¡Œä»»åŠ¡"""
        try:
            executed_tasks = set()
            
            while len(executed_tasks) < len(task_graph):
                # æ‰¾åˆ°å¯ä»¥æ‰§è¡Œçš„ä»»åŠ¡ï¼ˆä¾èµ–å·²æ»¡è¶³ï¼‰
                ready_tasks = []
                
                for task_id, task in task_graph.items():
                    if task_id not in executed_tasks:
                        dependencies_met = all(dep in executed_tasks for dep in task.dependencies)
                        if dependencies_met:
                            ready_tasks.append(task)
                
                if not ready_tasks:
                    raise DeepLearningException("æ£€æµ‹åˆ°å¾ªç¯ä¾èµ–æˆ–æ— æ³•æ»¡è¶³çš„ä¾èµ–")
                
                # æ‰§è¡Œç¬¬ä¸€ä¸ªå°±ç»ªä»»åŠ¡
                task = ready_tasks[0]
                success = self._execute_task(task, execution)
                
                if success:
                    executed_tasks.add(task.task_id)
                    execution.completed_tasks += 1
                else:
                    execution.failed_tasks += 1
                    if not task.handler:  # å¦‚æœä»»åŠ¡å¤±è´¥ä¸”ä¸å…è®¸ç»§ç»­
                        raise DeepLearningException(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {task.name}")
                
                # æ›´æ–°è¿›åº¦
                execution.progress = (len(executed_tasks) / len(task_graph)) * 100
                
        except Exception as e:
            logger_manager.error(f"é¡ºåºæ‰§è¡Œä»»åŠ¡å¤±è´¥: {e}")
            raise
    
    def _execute_parallel(self, task_graph: Dict[str, WorkflowTask], 
                         execution: WorkflowExecution):
        """å¹¶è¡Œæ‰§è¡Œä»»åŠ¡"""
        try:
            executed_tasks = set()
            running_tasks = {}
            
            while len(executed_tasks) < len(task_graph):
                # å¯åŠ¨æ–°ä»»åŠ¡
                ready_tasks = []
                
                for task_id, task in task_graph.items():
                    if (task_id not in executed_tasks and 
                        task_id not in running_tasks and
                        len(running_tasks) < self.max_workers):
                        
                        dependencies_met = all(dep in executed_tasks for dep in task.dependencies)
                        if dependencies_met:
                            ready_tasks.append(task)
                
                # å¯åŠ¨å°±ç»ªä»»åŠ¡
                for task in ready_tasks[:self.max_workers - len(running_tasks)]:
                    task_thread = threading.Thread(
                        target=self._execute_task_thread,
                        args=(task, execution, running_tasks, executed_tasks)
                    )
                    task_thread.daemon = True
                    task_thread.start()
                    
                    running_tasks[task.task_id] = task_thread
                
                # ç­‰å¾…ä»»åŠ¡å®Œæˆ
                time.sleep(0.1)
                
                # æ¸…ç†å®Œæˆçš„ä»»åŠ¡
                completed_task_ids = []
                for task_id, thread in running_tasks.items():
                    if not thread.is_alive():
                        completed_task_ids.append(task_id)
                
                for task_id in completed_task_ids:
                    del running_tasks[task_id]
                
                # æ›´æ–°è¿›åº¦
                execution.progress = (len(executed_tasks) / len(task_graph)) * 100
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for thread in running_tasks.values():
                thread.join()
                
        except Exception as e:
            logger_manager.error(f"å¹¶è¡Œæ‰§è¡Œä»»åŠ¡å¤±è´¥: {e}")
            raise
    
    def _execute_task_thread(self, task: WorkflowTask, execution: WorkflowExecution,
                           running_tasks: Dict, executed_tasks: set):
        """ä»»åŠ¡æ‰§è¡Œçº¿ç¨‹"""
        try:
            success = self._execute_task(task, execution)
            
            if success:
                executed_tasks.add(task.task_id)
                execution.completed_tasks += 1
            else:
                execution.failed_tasks += 1
                
        except Exception as e:
            logger_manager.error(f"ä»»åŠ¡çº¿ç¨‹æ‰§è¡Œå¤±è´¥: {e}")
            execution.failed_tasks += 1
    
    def _execute_task(self, task: WorkflowTask, execution: WorkflowExecution) -> bool:
        """æ‰§è¡Œå•ä¸ªä»»åŠ¡"""
        try:
            task.status = WorkflowStatus.RUNNING
            task.start_time = datetime.now()
            
            logger_manager.info(f"å¼€å§‹æ‰§è¡Œä»»åŠ¡: {task.name}")
            
            # æ‰§è¡Œä»»åŠ¡å¤„ç†å™¨
            if task.handler:
                result = task.handler(**task.parameters)
                task.result = result
                execution.results[task.task_id] = result
            
            task.status = WorkflowStatus.COMPLETED
            task.end_time = datetime.now()
            
            logger_manager.info(f"ä»»åŠ¡æ‰§è¡Œå®Œæˆ: {task.name}")
            return True
            
        except Exception as e:
            task.status = WorkflowStatus.FAILED
            task.error_message = str(e)
            task.end_time = datetime.now()
            
            logger_manager.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {task.name}, é”™è¯¯: {e}")
            
            # é‡è¯•é€»è¾‘
            if task.retry_count < task.max_retries:
                task.retry_count += 1
                logger_manager.info(f"ä»»åŠ¡é‡è¯•: {task.name}, ç¬¬ {task.retry_count} æ¬¡")
                return self._execute_task(task, execution)
            
            return False
    
    def get_execution_status(self, execution_id: str) -> Optional[WorkflowExecution]:
        """è·å–æ‰§è¡ŒçŠ¶æ€"""
        with self.lock:
            # æ£€æŸ¥æ´»è·ƒæ‰§è¡Œ
            if execution_id in self.active_executions:
                return self.active_executions[execution_id]
            
            # æ£€æŸ¥å†å²è®°å½•
            for execution in self.execution_history:
                if execution.execution_id == execution_id:
                    return execution
            
            return None
    
    def cancel_execution(self, execution_id: str) -> bool:
        """å–æ¶ˆæ‰§è¡Œ"""
        try:
            with self.lock:
                if execution_id in self.active_executions:
                    execution = self.active_executions[execution_id]
                    execution.status = WorkflowStatus.CANCELLED
                    execution.end_time = datetime.now()
                    
                    logger_manager.info(f"å·¥ä½œæµæ‰§è¡Œå·²å–æ¶ˆ: {execution_id}")
                    return True
            
            return False
            
        except Exception as e:
            logger_manager.error(f"å–æ¶ˆå·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
            return False


class WorkflowManager:
    """å·¥ä½œæµç®¡ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å·¥ä½œæµç®¡ç†å™¨"""
        self.workflows = {}
        self.executor = WorkflowExecutor()
        self.lock = threading.RLock()
        
        logger_manager.info("å·¥ä½œæµç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def register_workflow(self, config: WorkflowConfig) -> bool:
        """
        æ³¨å†Œå·¥ä½œæµ
        
        Args:
            config: å·¥ä½œæµé…ç½®
            
        Returns:
            æ˜¯å¦æ³¨å†ŒæˆåŠŸ
        """
        try:
            with self.lock:
                self.workflows[config.workflow_id] = config
                
            logger_manager.info(f"å·¥ä½œæµæ³¨å†ŒæˆåŠŸ: {config.name}")
            return True
            
        except Exception as e:
            logger_manager.error(f"æ³¨å†Œå·¥ä½œæµå¤±è´¥: {e}")
            return False
    
    def execute_workflow(self, workflow_id: str) -> Optional[str]:
        """
        æ‰§è¡Œå·¥ä½œæµ
        
        Args:
            workflow_id: å·¥ä½œæµID
            
        Returns:
            æ‰§è¡ŒID
        """
        try:
            with self.lock:
                if workflow_id not in self.workflows:
                    raise DeepLearningException(f"å·¥ä½œæµä¸å­˜åœ¨: {workflow_id}")
                
                config = self.workflows[workflow_id]
            
            execution_id = self.executor.execute_workflow(config)
            return execution_id
            
        except Exception as e:
            logger_manager.error(f"æ‰§è¡Œå·¥ä½œæµå¤±è´¥: {e}")
            return None
    
    def get_workflow_status(self, execution_id: str) -> Optional[WorkflowExecution]:
        """è·å–å·¥ä½œæµçŠ¶æ€"""
        return self.executor.get_execution_status(execution_id)
    
    def cancel_workflow(self, execution_id: str) -> bool:
        """å–æ¶ˆå·¥ä½œæµ"""
        return self.executor.cancel_execution(execution_id)
    
    def list_workflows(self) -> List[WorkflowConfig]:
        """åˆ—å‡ºæ‰€æœ‰å·¥ä½œæµ"""
        with self.lock:
            return list(self.workflows.values())
    
    def get_execution_history(self) -> List[WorkflowExecution]:
        """è·å–æ‰§è¡Œå†å²"""
        return self.executor.execution_history.copy()


# å…¨å±€å·¥ä½œæµç®¡ç†å™¨å®ä¾‹
workflow_manager = WorkflowManager()


if __name__ == "__main__":
    # æµ‹è¯•å·¥ä½œæµç®¡ç†å™¨åŠŸèƒ½
    print("ğŸ”„ æµ‹è¯•å·¥ä½œæµç®¡ç†å™¨åŠŸèƒ½...")
    
    try:
        manager = WorkflowManager()
        
        # åˆ›å»ºæµ‹è¯•ä»»åŠ¡
        def task1():
            print("æ‰§è¡Œä»»åŠ¡1")
            return "task1_result"
        
        def task2():
            print("æ‰§è¡Œä»»åŠ¡2")
            return "task2_result"
        
        def task3():
            print("æ‰§è¡Œä»»åŠ¡3")
            return "task3_result"
        
        # åˆ›å»ºå·¥ä½œæµé…ç½®
        tasks = [
            WorkflowTask(
                task_id="task1",
                name="æ•°æ®å¤„ç†ä»»åŠ¡",
                task_type=TaskType.DATA_PROCESSING,
                handler=task1
            ),
            WorkflowTask(
                task_id="task2",
                name="æ¨¡å‹è®­ç»ƒä»»åŠ¡",
                task_type=TaskType.MODEL_TRAINING,
                handler=task2,
                dependencies=["task1"]
            ),
            WorkflowTask(
                task_id="task3",
                name="æ¨¡å‹è¯„ä¼°ä»»åŠ¡",
                task_type=TaskType.MODEL_EVALUATION,
                handler=task3,
                dependencies=["task2"]
            )
        ]
        
        config = WorkflowConfig(
            workflow_id="test_workflow",
            name="æµ‹è¯•å·¥ä½œæµ",
            description="ç”¨äºæµ‹è¯•çš„å·¥ä½œæµ",
            tasks=tasks
        )
        
        # æ³¨å†Œå·¥ä½œæµ
        if manager.register_workflow(config):
            print("âœ… å·¥ä½œæµæ³¨å†ŒæˆåŠŸ")
        
        # æ‰§è¡Œå·¥ä½œæµ
        execution_id = manager.execute_workflow("test_workflow")
        if execution_id:
            print(f"âœ… å·¥ä½œæµæ‰§è¡Œå·²å¯åŠ¨: {execution_id}")
            
            # ç­‰å¾…æ‰§è¡Œå®Œæˆ
            import time
            time.sleep(2)
            
            # æ£€æŸ¥çŠ¶æ€
            status = manager.get_workflow_status(execution_id)
            if status:
                print(f"âœ… å·¥ä½œæµçŠ¶æ€: {status.status.value}")
                print(f"   å®Œæˆä»»åŠ¡: {status.completed_tasks}/{status.total_tasks}")
        
        # åˆ—å‡ºå·¥ä½œæµ
        workflows = manager.list_workflows()
        print(f"âœ… å·¥ä½œæµåˆ—è¡¨è·å–æˆåŠŸ: {len(workflows)} ä¸ªå·¥ä½œæµ")
        
        print("âœ… å·¥ä½œæµç®¡ç†å™¨åŠŸèƒ½æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print("å·¥ä½œæµç®¡ç†å™¨åŠŸèƒ½æµ‹è¯•å®Œæˆ")
