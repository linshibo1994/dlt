#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ€§èƒ½è·Ÿè¸ªç³»ç»Ÿ
è·Ÿè¸ªå’Œç®¡ç†æ¨¡å‹æ€§èƒ½æŒ‡æ ‡
"""

import os
import json
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Any, Optional, Union
from collections import defaultdict, deque
from datetime import datetime
import matplotlib.pyplot as plt

from core_modules import logger_manager, cache_manager


class ModelPerformanceMetrics:
    """æ¨¡å‹æ€§èƒ½æŒ‡æ ‡æ•°æ®ç»“æ„"""
    
    def __init__(self, model_id: str):
        """
        åˆå§‹åŒ–æ€§èƒ½æŒ‡æ ‡
        
        Args:
            model_id: æ¨¡å‹ID
        """
        self.model_id = model_id
        self.hit_rates = []  # å‘½ä¸­ç‡å†å²
        self.accuracies = []  # å‡†ç¡®ç‡å†å²
        self.confidence_scores = []  # ç½®ä¿¡åº¦å†å²
        self.resource_usage = []  # èµ„æºä½¿ç”¨å†å²
        self.prediction_times = []  # é¢„æµ‹æ—¶é—´å†å²
        self.timestamps = []  # æ—¶é—´æˆ³å†å²
        self.prize_rates = []  # ä¸­å¥–ç‡å†å²
        self.front_matches = []  # å‰åŒºåŒ¹é…æ•°å†å²
        self.back_matches = []  # ååŒºåŒ¹é…æ•°å†å²
        self.overall_scores = []  # ç»¼åˆå¾—åˆ†å†å²
    
    def add_metrics(self, metrics: Dict[str, float], timestamp: Optional[str] = None) -> None:
        """
        æ·»åŠ æ€§èƒ½æŒ‡æ ‡
        
        Args:
            metrics: æ€§èƒ½æŒ‡æ ‡å­—å…¸
            timestamp: æ—¶é—´æˆ³ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰æ—¶é—´
        """
        if timestamp is None:
            timestamp = datetime.now().isoformat()
        
        # æ·»åŠ å„é¡¹æŒ‡æ ‡
        if 'hit_rate' in metrics:
            self.hit_rates.append(metrics['hit_rate'])
        
        if 'accuracy' in metrics:
            self.accuracies.append(metrics['accuracy'])
        
        if 'confidence' in metrics:
            self.confidence_scores.append(metrics['confidence'])
        
        if 'resource_usage' in metrics:
            self.resource_usage.append(metrics['resource_usage'])
        
        if 'prediction_time' in metrics:
            self.prediction_times.append(metrics['prediction_time'])
        
        if 'prize_rate' in metrics:
            self.prize_rates.append(metrics['prize_rate'])
        
        if 'front_matches' in metrics:
            self.front_matches.append(metrics['front_matches'])
        
        if 'back_matches' in metrics:
            self.back_matches.append(metrics['back_matches'])
        
        # è®¡ç®—ç»¼åˆå¾—åˆ†
        overall_score = self._calculate_overall_score(metrics)
        self.overall_scores.append(overall_score)
        
        # æ·»åŠ æ—¶é—´æˆ³
        self.timestamps.append(timestamp)
    
    def _calculate_overall_score(self, metrics: Dict[str, float]) -> float:
        """
        è®¡ç®—ç»¼åˆå¾—åˆ†
        
        Args:
            metrics: æ€§èƒ½æŒ‡æ ‡å­—å…¸
            
        Returns:
            ç»¼åˆå¾—åˆ†
        """
        # æƒé‡é…ç½®
        weights = {
            'hit_rate': 0.2,
            'accuracy': 0.3,
            'confidence': 0.1,
            'prize_rate': 0.3,
            'front_matches': 0.05,
            'back_matches': 0.05
        }
        
        # è®¡ç®—åŠ æƒå¾—åˆ†
        score = 0.0
        total_weight = 0.0
        
        for metric, weight in weights.items():
            if metric in metrics:
                score += metrics[metric] * weight
                total_weight += weight
        
        # å½’ä¸€åŒ–å¾—åˆ†
        if total_weight > 0:
            score /= total_weight
        
        return score
    
    def get_average_metrics(self, window: Optional[int] = None) -> Dict[str, float]:
        """
        è·å–å¹³å‡æ€§èƒ½æŒ‡æ ‡
        
        Args:
            window: çª—å£å¤§å°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å…¨éƒ¨å†å²
            
        Returns:
            å¹³å‡æ€§èƒ½æŒ‡æ ‡å­—å…¸
        """
        result = {}
        
        # è®¡ç®—å„é¡¹æŒ‡æ ‡çš„å¹³å‡å€¼
        if self.hit_rates:
            result['hit_rate'] = self._get_average(self.hit_rates, window)
        
        if self.accuracies:
            result['accuracy'] = self._get_average(self.accuracies, window)
        
        if self.confidence_scores:
            result['confidence'] = self._get_average(self.confidence_scores, window)
        
        if self.resource_usage:
            result['resource_usage'] = self._get_average(self.resource_usage, window)
        
        if self.prediction_times:
            result['prediction_time'] = self._get_average(self.prediction_times, window)
        
        if self.prize_rates:
            result['prize_rate'] = self._get_average(self.prize_rates, window)
        
        if self.front_matches:
            result['front_matches'] = self._get_average(self.front_matches, window)
        
        if self.back_matches:
            result['back_matches'] = self._get_average(self.back_matches, window)
        
        if self.overall_scores:
            result['overall_score'] = self._get_average(self.overall_scores, window)
        
        return result
    
    def _get_average(self, values: List[float], window: Optional[int] = None) -> float:
        """
        è®¡ç®—å¹³å‡å€¼
        
        Args:
            values: å€¼åˆ—è¡¨
            window: çª—å£å¤§å°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å…¨éƒ¨å†å²
            
        Returns:
            å¹³å‡å€¼
        """
        if not values:
            return 0.0
        
        if window is not None and window > 0:
            values = values[-window:]
        
        return sum(values) / len(values)
    
    def get_trend(self, metric_name: str, window: Optional[int] = None) -> List[float]:
        """
        è·å–æŒ‡æ ‡è¶‹åŠ¿
        
        Args:
            metric_name: æŒ‡æ ‡åç§°
            window: çª—å£å¤§å°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å…¨éƒ¨å†å²
            
        Returns:
            æŒ‡æ ‡è¶‹åŠ¿åˆ—è¡¨
        """
        # è·å–æŒ‡æ ‡å€¼åˆ—è¡¨
        if metric_name == 'hit_rate':
            values = self.hit_rates
        elif metric_name == 'accuracy':
            values = self.accuracies
        elif metric_name == 'confidence':
            values = self.confidence_scores
        elif metric_name == 'resource_usage':
            values = self.resource_usage
        elif metric_name == 'prediction_time':
            values = self.prediction_times
        elif metric_name == 'prize_rate':
            values = self.prize_rates
        elif metric_name == 'front_matches':
            values = self.front_matches
        elif metric_name == 'back_matches':
            values = self.back_matches
        elif metric_name == 'overall_score':
            values = self.overall_scores
        else:
            return []
        
        # åº”ç”¨çª—å£
        if window is not None and window > 0 and len(values) > window:
            values = values[-window:]
        
        return values
    
    def to_dict(self) -> Dict[str, Any]:
        """
        è½¬æ¢ä¸ºå­—å…¸
        
        Returns:
            å­—å…¸è¡¨ç¤º
        """
        return {
            'model_id': self.model_id,
            'hit_rates': self.hit_rates,
            'accuracies': self.accuracies,
            'confidence_scores': self.confidence_scores,
            'resource_usage': self.resource_usage,
            'prediction_times': self.prediction_times,
            'timestamps': self.timestamps,
            'prize_rates': self.prize_rates,
            'front_matches': self.front_matches,
            'back_matches': self.back_matches,
            'overall_scores': self.overall_scores
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'ModelPerformanceMetrics':
        """
        ä»å­—å…¸åˆ›å»ºå®ä¾‹
        
        Args:
            data: å­—å…¸æ•°æ®
            
        Returns:
            ModelPerformanceMetricså®ä¾‹
        """
        metrics = cls(data['model_id'])
        
        metrics.hit_rates = data.get('hit_rates', [])
        metrics.accuracies = data.get('accuracies', [])
        metrics.confidence_scores = data.get('confidence_scores', [])
        metrics.resource_usage = data.get('resource_usage', [])
        metrics.prediction_times = data.get('prediction_times', [])
        metrics.timestamps = data.get('timestamps', [])
        metrics.prize_rates = data.get('prize_rates', [])
        metrics.front_matches = data.get('front_matches', [])
        metrics.back_matches = data.get('back_matches', [])
        metrics.overall_scores = data.get('overall_scores', [])
        
        return metrics


class PerformanceTracker:
    """æ€§èƒ½è·Ÿè¸ªå™¨"""
    
    def __init__(self, history_size: int = 100):
        """
        åˆå§‹åŒ–æ€§èƒ½è·Ÿè¸ªå™¨
        
        Args:
            history_size: å†å²è®°å½•å¤§å°
        """
        self.history_size = history_size
        self.model_metrics = {}  # æ¨¡å‹æ€§èƒ½æŒ‡æ ‡å­—å…¸
        self.cache_manager = cache_manager
        
        # åŠ è½½å†å²è®°å½•
        self._load_history()
        
        logger_manager.info(f"åˆå§‹åŒ–æ€§èƒ½è·Ÿè¸ªå™¨ï¼Œå†å²è®°å½•å¤§å°: {history_size}")
    
    def track_performance(self, model_id: str, metrics: Dict[str, float]) -> None:
        """
        è·Ÿè¸ªæ¨¡å‹æ€§èƒ½
        
        Args:
            model_id: æ¨¡å‹ID
            metrics: æ€§èƒ½æŒ‡æ ‡å­—å…¸
        """
        # å¦‚æœæ¨¡å‹ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„æ€§èƒ½æŒ‡æ ‡
        if model_id not in self.model_metrics:
            self.model_metrics[model_id] = ModelPerformanceMetrics(model_id)
        
        # æ·»åŠ æ€§èƒ½æŒ‡æ ‡
        self.model_metrics[model_id].add_metrics(metrics)
        
        # é™åˆ¶å†å²è®°å½•å¤§å°
        self._limit_history_size(model_id)
        
        # ä¿å­˜å†å²è®°å½•
        self._save_history()
        
        logger_manager.info(f"è·Ÿè¸ªæ¨¡å‹ {model_id} æ€§èƒ½: {metrics}")
    
    def _limit_history_size(self, model_id: str) -> None:
        """
        é™åˆ¶å†å²è®°å½•å¤§å°
        
        Args:
            model_id: æ¨¡å‹ID
        """
        if model_id not in self.model_metrics:
            return
        
        metrics = self.model_metrics[model_id]
        
        # é™åˆ¶å„é¡¹æŒ‡æ ‡çš„å†å²è®°å½•å¤§å°
        if len(metrics.hit_rates) > self.history_size:
            metrics.hit_rates = metrics.hit_rates[-self.history_size:]
        
        if len(metrics.accuracies) > self.history_size:
            metrics.accuracies = metrics.accuracies[-self.history_size:]
        
        if len(metrics.confidence_scores) > self.history_size:
            metrics.confidence_scores = metrics.confidence_scores[-self.history_size:]
        
        if len(metrics.resource_usage) > self.history_size:
            metrics.resource_usage = metrics.resource_usage[-self.history_size:]
        
        if len(metrics.prediction_times) > self.history_size:
            metrics.prediction_times = metrics.prediction_times[-self.history_size:]
        
        if len(metrics.timestamps) > self.history_size:
            metrics.timestamps = metrics.timestamps[-self.history_size:]
        
        if len(metrics.prize_rates) > self.history_size:
            metrics.prize_rates = metrics.prize_rates[-self.history_size:]
        
        if len(metrics.front_matches) > self.history_size:
            metrics.front_matches = metrics.front_matches[-self.history_size:]
        
        if len(metrics.back_matches) > self.history_size:
            metrics.back_matches = metrics.back_matches[-self.history_size:]
        
        if len(metrics.overall_scores) > self.history_size:
            metrics.overall_scores = metrics.overall_scores[-self.history_size:]
    
    def get_model_metrics(self, model_id: str) -> Optional[ModelPerformanceMetrics]:
        """
        è·å–æ¨¡å‹æ€§èƒ½æŒ‡æ ‡
        
        Args:
            model_id: æ¨¡å‹ID
            
        Returns:
            æ¨¡å‹æ€§èƒ½æŒ‡æ ‡ï¼Œå¦‚æœæ¨¡å‹ä¸å­˜åœ¨åˆ™è¿”å›None
        """
        return self.model_metrics.get(model_id)
    
    def get_all_model_metrics(self) -> Dict[str, ModelPerformanceMetrics]:
        """
        è·å–æ‰€æœ‰æ¨¡å‹æ€§èƒ½æŒ‡æ ‡
        
        Returns:
            æ‰€æœ‰æ¨¡å‹æ€§èƒ½æŒ‡æ ‡å­—å…¸
        """
        return self.model_metrics
    
    def get_best_model(self, metric_name: str = 'overall_score', window: Optional[int] = None) -> Optional[str]:
        """
        è·å–æœ€ä½³æ¨¡å‹
        
        Args:
            metric_name: æŒ‡æ ‡åç§°
            window: çª—å£å¤§å°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å…¨éƒ¨å†å²
            
        Returns:
            æœ€ä½³æ¨¡å‹IDï¼Œå¦‚æœæ²¡æœ‰æ¨¡å‹åˆ™è¿”å›None
        """
        if not self.model_metrics:
            return None
        
        # è®¡ç®—å„æ¨¡å‹çš„å¹³å‡æŒ‡æ ‡
        model_scores = {}
        for model_id, metrics in self.model_metrics.items():
            avg_metrics = metrics.get_average_metrics(window)
            if metric_name in avg_metrics:
                model_scores[model_id] = avg_metrics[metric_name]
        
        if not model_scores:
            return None
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„æ¨¡å‹
        return max(model_scores.items(), key=lambda x: x[1])[0]
    
    def compare_models(self, model_ids: List[str], metric_name: str = 'overall_score', 
                      window: Optional[int] = None) -> Dict[str, float]:
        """
        æ¯”è¾ƒå¤šä¸ªæ¨¡å‹
        
        Args:
            model_ids: æ¨¡å‹IDåˆ—è¡¨
            metric_name: æŒ‡æ ‡åç§°
            window: çª—å£å¤§å°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å…¨éƒ¨å†å²
            
        Returns:
            æ¨¡å‹å¾—åˆ†å­—å…¸
        """
        result = {}
        
        for model_id in model_ids:
            if model_id in self.model_metrics:
                avg_metrics = self.model_metrics[model_id].get_average_metrics(window)
                if metric_name in avg_metrics:
                    result[model_id] = avg_metrics[metric_name]
        
        return result
    
    def detect_performance_degradation(self, model_id: str, threshold: float = 0.1, 
                                     window: int = 10) -> bool:
        """
        æ£€æµ‹æ€§èƒ½é€€åŒ–
        
        Args:
            model_id: æ¨¡å‹ID
            threshold: é€€åŒ–é˜ˆå€¼
            window: çª—å£å¤§å°
            
        Returns:
            æ˜¯å¦æ£€æµ‹åˆ°æ€§èƒ½é€€åŒ–
        """
        if model_id not in self.model_metrics:
            return False
        
        metrics = self.model_metrics[model_id]
        
        # è·å–æ•´ä½“å¾—åˆ†è¶‹åŠ¿
        scores = metrics.get_trend('overall_score')
        
        if len(scores) < window * 2:
            return False
        
        # è®¡ç®—æœ€è¿‘çª—å£çš„å¹³å‡å¾—åˆ†
        recent_avg = sum(scores[-window:]) / window
        
        # è®¡ç®—å‰ä¸€ä¸ªçª—å£çš„å¹³å‡å¾—åˆ†
        previous_avg = sum(scores[-2*window:-window]) / window
        
        # æ£€æµ‹é€€åŒ–
        if previous_avg - recent_avg > threshold:
            logger_manager.warning(f"æ£€æµ‹åˆ°æ¨¡å‹ {model_id} æ€§èƒ½é€€åŒ–: {previous_avg:.4f} -> {recent_avg:.4f}")
            return True
        
        return False
    
    def _save_history(self) -> None:
        """ä¿å­˜å†å²è®°å½•"""
        try:
            # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
            data = {}
            for model_id, metrics in self.model_metrics.items():
                data[model_id] = metrics.to_dict()
            
            # ä¿å­˜åˆ°ç¼“å­˜
            self.cache_manager.save_cache("data", "performance_history", data)
            
            logger_manager.debug("æ€§èƒ½å†å²è®°å½•å·²ä¿å­˜")
        except Exception as e:
            logger_manager.error(f"ä¿å­˜æ€§èƒ½å†å²è®°å½•å¤±è´¥: {e}")
    
    def _load_history(self) -> None:
        """åŠ è½½å†å²è®°å½•"""
        try:
            # ä»ç¼“å­˜åŠ è½½
            data = self.cache_manager.load_cache("data", "performance_history")
            
            if data:
                # è½¬æ¢ä¸ºModelPerformanceMetricså¯¹è±¡
                for model_id, metrics_data in data.items():
                    self.model_metrics[model_id] = ModelPerformanceMetrics.from_dict(metrics_data)
                
                logger_manager.info(f"åŠ è½½äº† {len(self.model_metrics)} ä¸ªæ¨¡å‹çš„æ€§èƒ½å†å²è®°å½•")
        except Exception as e:
            logger_manager.error(f"åŠ è½½æ€§èƒ½å†å²è®°å½•å¤±è´¥: {e}")
    
    def plot_performance_trend(self, model_id: str, metric_name: str = 'overall_score', 
                             save_path: Optional[str] = None) -> None:
        """
        ç»˜åˆ¶æ€§èƒ½è¶‹åŠ¿å›¾
        
        Args:
            model_id: æ¨¡å‹ID
            metric_name: æŒ‡æ ‡åç§°
            save_path: ä¿å­˜è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™æ˜¾ç¤ºå›¾è¡¨
        """
        if model_id not in self.model_metrics:
            logger_manager.warning(f"æ¨¡å‹ {model_id} ä¸å­˜åœ¨")
            return
        
        metrics = self.model_metrics[model_id]
        values = metrics.get_trend(metric_name)
        timestamps = metrics.timestamps[-len(values):] if len(metrics.timestamps) >= len(values) else None
        
        if not values:
            logger_manager.warning(f"æ¨¡å‹ {model_id} æ²¡æœ‰ {metric_name} æŒ‡æ ‡æ•°æ®")
            return
        
        try:
            plt.figure(figsize=(10, 6))
            
            if timestamps:
                # è½¬æ¢æ—¶é—´æˆ³ä¸ºæ—¥æœŸæ—¶é—´å¯¹è±¡
                dates = [datetime.fromisoformat(ts) for ts in timestamps]
                plt.plot(dates, values)
                plt.gcf().autofmt_xdate()  # è‡ªåŠ¨æ ¼å¼åŒ–æ—¥æœŸæ ‡ç­¾
            else:
                plt.plot(values)
            
            plt.title(f"{model_id} - {metric_name} è¶‹åŠ¿")
            plt.xlabel("æ—¶é—´")
            plt.ylabel(metric_name)
            plt.grid(True)
            
            if save_path:
                plt.savefig(save_path)
                logger_manager.info(f"æ€§èƒ½è¶‹åŠ¿å›¾å·²ä¿å­˜åˆ° {save_path}")
            else:
                plt.show()
        except Exception as e:
            logger_manager.error(f"ç»˜åˆ¶æ€§èƒ½è¶‹åŠ¿å›¾å¤±è´¥: {e}")


if __name__ == "__main__":
    # æµ‹è¯•æ€§èƒ½è·Ÿè¸ªå™¨
    print("ğŸ“Š æµ‹è¯•æ€§èƒ½è·Ÿè¸ªå™¨...")
    
    # åˆ›å»ºæ€§èƒ½è·Ÿè¸ªå™¨
    tracker = PerformanceTracker()
    
    # è·Ÿè¸ªæ¨¡å‹æ€§èƒ½
    tracker.track_performance("model1", {
        'hit_rate': 0.6,
        'accuracy': 0.7,
        'confidence': 0.8,
        'prize_rate': 0.2,
        'front_matches': 2.5,
        'back_matches': 1.0
    })
    
    tracker.track_performance("model2", {
        'hit_rate': 0.7,
        'accuracy': 0.8,
        'confidence': 0.9,
        'prize_rate': 0.3,
        'front_matches': 3.0,
        'back_matches': 1.2
    })
    
    # è·å–æ¨¡å‹æ€§èƒ½æŒ‡æ ‡
    metrics1 = tracker.get_model_metrics("model1")
    metrics2 = tracker.get_model_metrics("model2")
    
    print(f"æ¨¡å‹1å¹³å‡æŒ‡æ ‡: {metrics1.get_average_metrics()}")
    print(f"æ¨¡å‹2å¹³å‡æŒ‡æ ‡: {metrics2.get_average_metrics()}")
    
    # è·å–æœ€ä½³æ¨¡å‹
    best_model = tracker.get_best_model()
    print(f"æœ€ä½³æ¨¡å‹: {best_model}")
    
    # æ¯”è¾ƒæ¨¡å‹
    comparison = tracker.compare_models(["model1", "model2"])
    print(f"æ¨¡å‹æ¯”è¾ƒ: {comparison}")
    
    print("æ€§èƒ½è·Ÿè¸ªå™¨æµ‹è¯•å®Œæˆ")