#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å†³ç­–è§£é‡Šå™¨
è§£é‡Šæ¨¡å‹é€‰æ‹©å’Œæƒé‡è°ƒæ•´å†³ç­–
"""

import os
import json
import numpy as np
from typing import List, Dict, Tuple, Any, Optional, Union
from datetime import datetime

from .performance_tracker import PerformanceTracker
from .weight_optimizer import WeightOptimizer
from core_modules import logger_manager


class DecisionExplainer:
    """å†³ç­–è§£é‡Šå™¨"""
    
    def __init__(self, performance_tracker: Optional[PerformanceTracker] = None,
               weight_optimizer: Optional[WeightOptimizer] = None):
        """
        åˆå§‹åŒ–å†³ç­–è§£é‡Šå™¨
        
        Args:
            performance_tracker: æ€§èƒ½è·Ÿè¸ªå™¨ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºæ–°çš„
            weight_optimizer: æƒé‡ä¼˜åŒ–å™¨ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºæ–°çš„
        """
        self.performance_tracker = performance_tracker or PerformanceTracker()
        self.weight_optimizer = weight_optimizer or WeightOptimizer(self.performance_tracker)
        
        logger_manager.info("åˆå§‹åŒ–å†³ç­–è§£é‡Šå™¨")
    
    def explain_model_selection(self, model_id: str, metric_name: str = 'overall_score',
                              window: Optional[int] = None) -> Dict[str, Any]:
        """
        è§£é‡Šæ¨¡å‹é€‰æ‹©
        
        Args:
            model_id: æ¨¡å‹ID
            metric_name: æŒ‡æ ‡åç§°
            window: çª—å£å¤§å°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å…¨éƒ¨å†å²
            
        Returns:
            è§£é‡Šå­—å…¸
        """
        # è·å–æ¨¡å‹æ€§èƒ½æŒ‡æ ‡
        metrics = self.performance_tracker.get_model_metrics(model_id)
        
        if not metrics:
            return {
                'model_id': model_id,
                'explanation': f"æ¨¡å‹ {model_id} æ²¡æœ‰æ€§èƒ½è®°å½•"
            }
        
        # è·å–å¹³å‡æŒ‡æ ‡
        avg_metrics = metrics.get_average_metrics(window)
        
        # è·å–æ‰€æœ‰æ¨¡å‹çš„æ¯”è¾ƒ
        all_models = list(self.performance_tracker.get_all_model_metrics().keys())
        comparison = self.performance_tracker.compare_models(all_models, metric_name, window)
        
        # è·å–æœ€ä½³æ¨¡å‹
        best_model = self.performance_tracker.get_best_model(metric_name, window)
        
        # è·å–æ¨¡å‹æ’å
        ranking = sorted(comparison.items(), key=lambda x: x[1], reverse=True)
        rank = next((i + 1 for i, (mid, _) in enumerate(ranking) if mid == model_id), -1)
        
        # è·å–æ¨¡å‹æƒé‡
        weights = self.weight_optimizer.get_weights()
        weight = weights.get(model_id, 0.0)
        
        # ç”Ÿæˆè§£é‡Š
        explanation = {
            'model_id': model_id,
            'metrics': avg_metrics,
            'comparison': comparison,
            'best_model': best_model,
            'rank': rank,
            'weight': weight,
            'explanation': self._generate_model_explanation(model_id, avg_metrics, comparison, best_model, rank, weight)
        }
        
        return explanation
    
    def _generate_model_explanation(self, model_id: str, metrics: Dict[str, float],
                                  comparison: Dict[str, float], best_model: str,
                                  rank: int, weight: float) -> str:
        """
        ç”Ÿæˆæ¨¡å‹è§£é‡Š
        
        Args:
            model_id: æ¨¡å‹ID
            metrics: å¹³å‡æŒ‡æ ‡
            comparison: æ¨¡å‹æ¯”è¾ƒ
            best_model: æœ€ä½³æ¨¡å‹
            rank: æ¨¡å‹æ’å
            weight: æ¨¡å‹æƒé‡
            
        Returns:
            è§£é‡Šæ–‡æœ¬
        """
        explanation = f"æ¨¡å‹ {model_id} çš„æ€§èƒ½åˆ†æ:\n\n"
        
        # æ·»åŠ æ€§èƒ½æŒ‡æ ‡
        explanation += "æ€§èƒ½æŒ‡æ ‡:\n"
        for metric, value in metrics.items():
            explanation += f"- {metric}: {value:.4f}\n"
        
        explanation += "\n"
        
        # æ·»åŠ æ’åä¿¡æ¯
        if rank > 0:
            explanation += f"åœ¨æ‰€æœ‰æ¨¡å‹ä¸­æ’åç¬¬ {rank}\n"
            
            if model_id == best_model:
                explanation += "è¿™æ˜¯å½“å‰æ€§èƒ½æœ€å¥½çš„æ¨¡å‹\n"
            else:
                best_score = comparison.get(best_model, 0.0)
                model_score = comparison.get(model_id, 0.0)
                diff = best_score - model_score
                explanation += f"ä¸æœ€ä½³æ¨¡å‹ {best_model} çš„å·®è·: {diff:.4f}\n"
        
        explanation += "\n"
        
        # æ·»åŠ æƒé‡ä¿¡æ¯
        explanation += f"åœ¨é›†æˆä¸­çš„æƒé‡: {weight:.4f}\n"
        
        # æ·»åŠ å»ºè®®
        explanation += "\nå»ºè®®:\n"
        
        if model_id == best_model:
            explanation += "- ä¿æŒå½“å‰é…ç½®ï¼Œè¯¥æ¨¡å‹è¡¨ç°è‰¯å¥½\n"
        elif rank <= 3:
            explanation += "- è€ƒè™‘å¢åŠ è¯¥æ¨¡å‹åœ¨é›†æˆä¸­çš„æƒé‡\n"
            explanation += "- å¯ä»¥å°è¯•å¾®è°ƒæ¨¡å‹å‚æ•°ä»¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½\n"
        else:
            explanation += "- è€ƒè™‘å‡å°‘è¯¥æ¨¡å‹åœ¨é›†æˆä¸­çš„æƒé‡\n"
            explanation += "- å¯èƒ½éœ€è¦é‡æ–°è®­ç»ƒæˆ–è°ƒæ•´æ¨¡å‹å‚æ•°\n"
        
        return explanation
    
    def explain_weight_adjustment(self) -> Dict[str, Any]:
        """
        è§£é‡Šæƒé‡è°ƒæ•´
        
        Returns:
            è§£é‡Šå­—å…¸
        """
        # è·å–æƒé‡è§£é‡Š
        weight_explanation = self.weight_optimizer.get_weight_explanation()
        
        # è·å–æ›´æ–°å†å²
        update_history = self.weight_optimizer.get_update_history()
        
        if not update_history:
            return {
                'explanation': "å°šæœªè¿›è¡Œæƒé‡è°ƒæ•´",
                'weights': self.weight_optimizer.get_weights()
            }
        
        # è·å–æƒé‡å†å²è¶‹åŠ¿
        weight_trends = {}
        for model_id in self.weight_optimizer.get_weights():
            history = self.weight_optimizer.get_weight_history(model_id)
            if len(history) >= 2:
                initial = history[0]
                current = history[-1]
                change = current - initial
                trend = "ä¸Šå‡" if change > 0.01 else "ä¸‹é™" if change < -0.01 else "ç¨³å®š"
                weight_trends[model_id] = {
                    'initial': initial,
                    'current': current,
                    'change': change,
                    'trend': trend
                }
        
        # ç”Ÿæˆè§£é‡Š
        explanation = {
            'weights': self.weight_optimizer.get_weights(),
            'weight_explanation': weight_explanation,
            'weight_trends': weight_trends,
            'update_count': len(update_history),
            'explanation': self._generate_weight_explanation(weight_explanation, weight_trends, update_history)
        }
        
        return explanation
    
    def _generate_weight_explanation(self, weight_explanation: Dict[str, Any],
                                   weight_trends: Dict[str, Dict[str, Any]],
                                   update_history: List[Dict[str, Any]]) -> str:
        """
        ç”Ÿæˆæƒé‡è§£é‡Š
        
        Args:
            weight_explanation: æƒé‡è§£é‡Š
            weight_trends: æƒé‡è¶‹åŠ¿
            update_history: æ›´æ–°å†å²
            
        Returns:
            è§£é‡Šæ–‡æœ¬
        """
        explanation = "æƒé‡è°ƒæ•´åˆ†æ:\n\n"
        
        # æ·»åŠ å½“å‰æƒé‡
        explanation += "å½“å‰æƒé‡:\n"
        for model_id, weight in weight_explanation['weights'].items():
            explanation += f"- {model_id}: {weight:.4f}\n"
        
        explanation += "\n"
        
        # æ·»åŠ æƒé‡è¶‹åŠ¿
        explanation += "æƒé‡è¶‹åŠ¿:\n"
        for model_id, trend in weight_trends.items():
            explanation += f"- {model_id}: {trend['initial']:.4f} -> {trend['current']:.4f} ({trend['trend']})\n"
        
        explanation += "\n"
        
        # æ·»åŠ è°ƒæ•´åŸå› 
        explanation += "è°ƒæ•´åŸå› :\n"
        explanation += weight_explanation['explanation']
        
        explanation += "\n"
        
        # æ·»åŠ å»ºè®®
        explanation += "å»ºè®®:\n"
        
        # æ‰¾å‡ºæƒé‡å¢åŠ æœ€å¤šå’Œå‡å°‘æœ€å¤šçš„æ¨¡å‹
        if weight_trends:
            max_increase = max(weight_trends.items(), key=lambda x: x[1]['change'])
            max_decrease = min(weight_trends.items(), key=lambda x: x[1]['change'])
            
            if max_increase[1]['change'] > 0.05:
                explanation += f"- æ¨¡å‹ {max_increase[0]} çš„æƒé‡æ˜¾è‘—å¢åŠ ï¼Œè¡¨æ˜å…¶æ€§èƒ½æŒç»­æå‡\n"
                explanation += "  å¯ä»¥è€ƒè™‘è¿›ä¸€æ­¥ä¼˜åŒ–è¯¥æ¨¡å‹æˆ–å¢åŠ ç±»ä¼¼æ¨¡å‹\n"
            
            if max_decrease[1]['change'] < -0.05:
                explanation += f"- æ¨¡å‹ {max_decrease[0]} çš„æƒé‡æ˜¾è‘—å‡å°‘ï¼Œè¡¨æ˜å…¶æ€§èƒ½å¯èƒ½å­˜åœ¨é—®é¢˜\n"
                explanation += "  å»ºè®®æ£€æŸ¥è¯¥æ¨¡å‹æˆ–è€ƒè™‘æ›¿æ¢\n"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æƒé‡è¿‡äºé›†ä¸­çš„æƒ…å†µ
        weights = weight_explanation['weights']
        if weights and max(weights.values()) > 0.7:
            dominant_model = max(weights.items(), key=lambda x: x[1])[0]
            explanation += f"- æ¨¡å‹ {dominant_model} çš„æƒé‡è¿‡é«˜ ({weights[dominant_model]:.4f})ï¼Œå¯èƒ½å¯¼è‡´è¿‡åº¦ä¾èµ–\n"
            explanation += "  å»ºè®®å¼•å…¥æ›´å¤šå¤šæ ·åŒ–çš„æ¨¡å‹ä»¥æé«˜é›†æˆçš„é²æ£’æ€§\n"
        
        return explanation
    
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """
        ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        
        Returns:
            æŠ¥å‘Šå­—å…¸
        """
        # è·å–æ‰€æœ‰æ¨¡å‹
        all_models = list(self.performance_tracker.get_all_model_metrics().keys())
        
        if not all_models:
            return {
                'explanation': "æ²¡æœ‰æ¨¡å‹æ•°æ®ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š"
            }
        
        # è·å–å„æ¨¡å‹çš„è§£é‡Š
        model_explanations = {}
        for model_id in all_models:
            model_explanations[model_id] = self.explain_model_selection(model_id)
        
        # è·å–æƒé‡è°ƒæ•´è§£é‡Š
        weight_explanation = self.explain_weight_adjustment()
        
        # è·å–æœ€ä½³æ¨¡å‹
        best_model = self.performance_tracker.get_best_model()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            'timestamp': datetime.now().isoformat(),
            'models': all_models,
            'best_model': best_model,
            'model_explanations': model_explanations,
            'weight_explanation': weight_explanation,
            'explanation': self._generate_report_explanation(all_models, best_model, model_explanations, weight_explanation)
        }
        
        return report
    
    def _generate_report_explanation(self, all_models: List[str], best_model: str,
                                   model_explanations: Dict[str, Dict[str, Any]],
                                   weight_explanation: Dict[str, Any]) -> str:
        """
        ç”ŸæˆæŠ¥å‘Šè§£é‡Š
        
        Args:
            all_models: æ‰€æœ‰æ¨¡å‹
            best_model: æœ€ä½³æ¨¡å‹
            model_explanations: æ¨¡å‹è§£é‡Š
            weight_explanation: æƒé‡è§£é‡Š
            
        Returns:
            è§£é‡Šæ–‡æœ¬
        """
        explanation = "æ¨¡å‹æ€§èƒ½ä¸æƒé‡ç»¼åˆæŠ¥å‘Š\n"
        explanation += "=" * 50 + "\n\n"
        
        # æ·»åŠ æ¦‚è¿°
        explanation += f"æ¨¡å‹æ•°é‡: {len(all_models)}\n"
        explanation += f"æœ€ä½³æ¨¡å‹: {best_model}\n\n"
        
        # æ·»åŠ æ¨¡å‹æ€§èƒ½æ‘˜è¦
        explanation += "æ¨¡å‹æ€§èƒ½æ‘˜è¦:\n"
        explanation += "-" * 30 + "\n"
        
        for model_id in all_models:
            model_expl = model_explanations[model_id]
            metrics = model_expl.get('metrics', {})
            overall_score = metrics.get('overall_score', 0.0)
            rank = model_expl.get('rank', -1)
            weight = model_expl.get('weight', 0.0)
            
            explanation += f"æ¨¡å‹: {model_id}\n"
            explanation += f"- ç»¼åˆå¾—åˆ†: {overall_score:.4f}\n"
            explanation += f"- æ’å: {rank}\n"
            explanation += f"- æƒé‡: {weight:.4f}\n\n"
        
        # æ·»åŠ æƒé‡è°ƒæ•´æ‘˜è¦
        explanation += "æƒé‡è°ƒæ•´æ‘˜è¦:\n"
        explanation += "-" * 30 + "\n"
        explanation += weight_explanation.get('explanation', "æ— æƒé‡è°ƒæ•´ä¿¡æ¯") + "\n\n"
        
        # æ·»åŠ å»ºè®®
        explanation += "ç»¼åˆå»ºè®®:\n"
        explanation += "-" * 30 + "\n"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ€§èƒ½å·®è·è¿‡å¤§çš„æƒ…å†µ
        if best_model and len(all_models) > 1:
            best_score = model_explanations[best_model]['metrics'].get('overall_score', 0.0)
            worst_model = min(all_models, key=lambda m: model_explanations[m]['metrics'].get('overall_score', 0.0))
            worst_score = model_explanations[worst_model]['metrics'].get('overall_score', 0.0)
            
            if best_score - worst_score > 0.3:
                explanation += f"- æ¨¡å‹æ€§èƒ½å·®è·è¾ƒå¤§ï¼Œæœ€ä½³æ¨¡å‹ {best_model} å’Œæœ€å·®æ¨¡å‹ {worst_model} çš„å¾—åˆ†å·®è·ä¸º {best_score - worst_score:.4f}\n"
                explanation += "  å»ºè®®è€ƒè™‘ç§»é™¤æˆ–é‡æ–°è®­ç»ƒæ€§èƒ½è¾ƒå·®çš„æ¨¡å‹\n\n"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æƒé‡åˆ†å¸ƒä¸å‡çš„æƒ…å†µ
        weights = weight_explanation.get('weights', {})
        if weights and len(weights) > 1:
            max_weight = max(weights.values())
            min_weight = min(weights.values())
            
            if max_weight / (min_weight + 1e-10) > 5:
                explanation += "- æƒé‡åˆ†å¸ƒä¸å‡ï¼Œå¯èƒ½å¯¼è‡´é›†æˆæ•ˆæœä¸ä½³\n"
                explanation += "  å»ºè®®è°ƒæ•´å­¦ä¹ ç‡æˆ–æ‰‹åŠ¨å¹³è¡¡æƒé‡\n\n"
        
        # æ·»åŠ æ€»ç»“
        explanation += "æ€»ç»“:\n"
        if best_model:
            explanation += f"- å½“å‰æœ€ä½³æ¨¡å‹æ˜¯ {best_model}ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨å’Œä¼˜åŒ–è¯¥æ¨¡å‹\n"
        else:
            explanation += "- æš‚æ— æ˜ç¡®çš„æœ€ä½³æ¨¡å‹ï¼Œå»ºè®®ç»§ç»­æ”¶é›†æ€§èƒ½æ•°æ®\n"
        
        explanation += "- å®šæœŸæ£€æŸ¥æ¨¡å‹æ€§èƒ½å¹¶æ›´æ–°æƒé‡ï¼Œä»¥ä¿æŒé›†æˆæ•ˆæœ\n"
        explanation += "- è€ƒè™‘å¼•å…¥æ–°çš„æ¨¡å‹æˆ–ä¼˜åŒ–ç°æœ‰æ¨¡å‹ï¼Œä»¥æé«˜æ•´ä½“é¢„æµ‹å‡†ç¡®æ€§\n"
        
        return explanation


if __name__ == "__main__":
    # æµ‹è¯•å†³ç­–è§£é‡Šå™¨
    print("ğŸ” æµ‹è¯•å†³ç­–è§£é‡Šå™¨...")
    
    # åˆ›å»ºæ€§èƒ½è·Ÿè¸ªå™¨
    tracker = PerformanceTracker()
    
    # è·Ÿè¸ªæ¨¡å‹æ€§èƒ½
    tracker.track_performance("model1", {
        'overall_score': 0.6,
        'accuracy': 0.7,
        'hit_rate': 0.5
    })
    
    tracker.track_performance("model2", {
        'overall_score': 0.8,
        'accuracy': 0.9,
        'hit_rate': 0.7
    })
    
    # åˆ›å»ºæƒé‡ä¼˜åŒ–å™¨
    optimizer = WeightOptimizer(tracker)
    optimizer.initialize_weights(["model1", "model2"])
    optimizer.update_weights()
    
    # åˆ›å»ºå†³ç­–è§£é‡Šå™¨
    explainer = DecisionExplainer(tracker, optimizer)
    
    # è§£é‡Šæ¨¡å‹é€‰æ‹©
    model_explanation = explainer.explain_model_selection("model1")
    print(f"æ¨¡å‹è§£é‡Š:\n{model_explanation['explanation']}")
    
    # è§£é‡Šæƒé‡è°ƒæ•´
    weight_explanation = explainer.explain_weight_adjustment()
    print(f"\næƒé‡è°ƒæ•´è§£é‡Š:\n{weight_explanation['explanation']}")
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    report = explainer.generate_comprehensive_report()
    print(f"\nç»¼åˆæŠ¥å‘Š:\n{report['explanation']}")
    
    print("å†³ç­–è§£é‡Šå™¨æµ‹è¯•å®Œæˆ")