#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è¿ç§»å·¥å…·æ¨¡å—
Migration Tools Module

æä¾›æ•°æ®è¿ç§»ã€æ ¼å¼è½¬æ¢ã€ç‰ˆæœ¬å‡çº§ç­‰åŠŸèƒ½ã€‚
"""

import os
import json
import csv
import pickle
import shutil
from typing import Dict, List, Any, Optional, Union, Callable, Tuple
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime
import pandas as pd
import numpy as np
from pathlib import Path

from core_modules import logger_manager
from ..utils.exceptions import DeepLearningException


class DataFormat(Enum):
    """æ•°æ®æ ¼å¼æšä¸¾"""
    JSON = "json"
    CSV = "csv"
    EXCEL = "excel"
    PICKLE = "pickle"
    NUMPY = "numpy"
    PARQUET = "parquet"
    HDF5 = "hdf5"
    XML = "xml"
    YAML = "yaml"


class MigrationStatus(Enum):
    """è¿ç§»çŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


@dataclass
class MigrationConfig:
    """è¿ç§»é…ç½®"""
    name: str
    source_path: str
    target_path: str
    source_format: DataFormat
    target_format: DataFormat
    description: str = ""
    backup_enabled: bool = True
    validation_enabled: bool = True
    chunk_size: int = 10000
    parallel_processing: bool = False
    custom_transformer: Optional[Callable] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class MigrationResult:
    """è¿ç§»ç»“æœ"""
    migration_id: str
    config: MigrationConfig
    status: MigrationStatus
    start_time: datetime
    end_time: Optional[datetime] = None
    records_processed: int = 0
    records_migrated: int = 0
    records_failed: int = 0
    error_message: str = ""
    validation_results: Dict[str, Any] = field(default_factory=dict)
    backup_path: str = ""


class DataFormatDetector:
    """æ•°æ®æ ¼å¼æ£€æµ‹å™¨"""
    
    @staticmethod
    def detect_format(file_path: str) -> Optional[DataFormat]:
        """
        æ£€æµ‹æ–‡ä»¶æ ¼å¼
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ£€æµ‹åˆ°çš„æ•°æ®æ ¼å¼
        """
        try:
            if not os.path.exists(file_path):
                return None
            
            # æ ¹æ®æ–‡ä»¶æ‰©å±•åæ£€æµ‹
            file_ext = Path(file_path).suffix.lower()
            
            format_mapping = {
                '.json': DataFormat.JSON,
                '.csv': DataFormat.CSV,
                '.xlsx': DataFormat.EXCEL,
                '.xls': DataFormat.EXCEL,
                '.pkl': DataFormat.PICKLE,
                '.pickle': DataFormat.PICKLE,
                '.npy': DataFormat.NUMPY,
                '.npz': DataFormat.NUMPY,
                '.parquet': DataFormat.PARQUET,
                '.h5': DataFormat.HDF5,
                '.hdf5': DataFormat.HDF5,
                '.xml': DataFormat.XML,
                '.yaml': DataFormat.YAML,
                '.yml': DataFormat.YAML
            }
            
            detected_format = format_mapping.get(file_ext)
            
            if detected_format:
                logger_manager.debug(f"æ£€æµ‹åˆ°æ–‡ä»¶æ ¼å¼: {file_path} -> {detected_format.value}")
                return detected_format
            
            # å°è¯•å†…å®¹æ£€æµ‹
            return DataFormatDetector._detect_by_content(file_path)
            
        except Exception as e:
            logger_manager.error(f"æ ¼å¼æ£€æµ‹å¤±è´¥: {e}")
            return None
    
    @staticmethod
    def _detect_by_content(file_path: str) -> Optional[DataFormat]:
        """æ ¹æ®æ–‡ä»¶å†…å®¹æ£€æµ‹æ ¼å¼"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                first_line = f.readline().strip()
            
            # JSONæ£€æµ‹
            if first_line.startswith('{') or first_line.startswith('['):
                try:
                    json.loads(first_line)
                    return DataFormat.JSON
                except:
                    pass
            
            # CSVæ£€æµ‹
            if ',' in first_line:
                return DataFormat.CSV
            
            # XMLæ£€æµ‹
            if first_line.startswith('<?xml') or first_line.startswith('<'):
                return DataFormat.XML
            
            return None
            
        except Exception:
            return None


class DataConverter:
    """æ•°æ®è½¬æ¢å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®è½¬æ¢å™¨"""
        self.converters = {
            (DataFormat.CSV, DataFormat.JSON): self._csv_to_json,
            (DataFormat.JSON, DataFormat.CSV): self._json_to_csv,
            (DataFormat.CSV, DataFormat.EXCEL): self._csv_to_excel,
            (DataFormat.EXCEL, DataFormat.CSV): self._excel_to_csv,
            (DataFormat.JSON, DataFormat.EXCEL): self._json_to_excel,
            (DataFormat.EXCEL, DataFormat.JSON): self._excel_to_json,
            (DataFormat.CSV, DataFormat.PICKLE): self._csv_to_pickle,
            (DataFormat.PICKLE, DataFormat.CSV): self._pickle_to_csv,
            (DataFormat.NUMPY, DataFormat.CSV): self._numpy_to_csv,
            (DataFormat.CSV, DataFormat.NUMPY): self._csv_to_numpy
        }
        
        logger_manager.info("æ•°æ®è½¬æ¢å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def convert(self, source_path: str, target_path: str,
                source_format: DataFormat, target_format: DataFormat,
                chunk_size: int = 10000, custom_transformer: Callable = None) -> bool:
        """
        è½¬æ¢æ•°æ®æ ¼å¼
        
        Args:
            source_path: æºæ–‡ä»¶è·¯å¾„
            target_path: ç›®æ ‡æ–‡ä»¶è·¯å¾„
            source_format: æºæ ¼å¼
            target_format: ç›®æ ‡æ ¼å¼
            chunk_size: å—å¤§å°
            custom_transformer: è‡ªå®šä¹‰è½¬æ¢å™¨
            
        Returns:
            æ˜¯å¦è½¬æ¢æˆåŠŸ
        """
        try:
            # æ£€æŸ¥è½¬æ¢å™¨æ˜¯å¦å­˜åœ¨
            converter_key = (source_format, target_format)
            
            if custom_transformer:
                converter = custom_transformer
            elif converter_key in self.converters:
                converter = self.converters[converter_key]
            else:
                raise DeepLearningException(f"ä¸æ”¯æŒçš„æ ¼å¼è½¬æ¢: {source_format.value} -> {target_format.value}")
            
            # åˆ›å»ºç›®æ ‡ç›®å½•
            os.makedirs(os.path.dirname(target_path), exist_ok=True)
            
            # æ‰§è¡Œè½¬æ¢
            success = converter(source_path, target_path, chunk_size)
            
            if success:
                logger_manager.info(f"æ•°æ®è½¬æ¢æˆåŠŸ: {source_path} -> {target_path}")
            else:
                logger_manager.error(f"æ•°æ®è½¬æ¢å¤±è´¥: {source_path} -> {target_path}")
            
            return success
            
        except Exception as e:
            logger_manager.error(f"æ•°æ®è½¬æ¢å¼‚å¸¸: {e}")
            return False
    
    def _csv_to_json(self, source_path: str, target_path: str, chunk_size: int) -> bool:
        """CSVè½¬JSON"""
        try:
            df = pd.read_csv(source_path)
            data = df.to_dict('records')
            
            with open(target_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            
            return True
        except Exception as e:
            logger_manager.error(f"CSVè½¬JSONå¤±è´¥: {e}")
            return False
    
    def _json_to_csv(self, source_path: str, target_path: str, chunk_size: int) -> bool:
        """JSONè½¬CSV"""
        try:
            with open(source_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if isinstance(data, list):
                df = pd.DataFrame(data)
            else:
                df = pd.DataFrame([data])
            
            df.to_csv(target_path, index=False)
            return True
        except Exception as e:
            logger_manager.error(f"JSONè½¬CSVå¤±è´¥: {e}")
            return False
    
    def _csv_to_excel(self, source_path: str, target_path: str, chunk_size: int) -> bool:
        """CSVè½¬Excel"""
        try:
            df = pd.read_csv(source_path)
            df.to_excel(target_path, index=False)
            return True
        except Exception as e:
            logger_manager.error(f"CSVè½¬Excelå¤±è´¥: {e}")
            return False
    
    def _excel_to_csv(self, source_path: str, target_path: str, chunk_size: int) -> bool:
        """Excelè½¬CSV"""
        try:
            df = pd.read_excel(source_path)
            df.to_csv(target_path, index=False)
            return True
        except Exception as e:
            logger_manager.error(f"Excelè½¬CSVå¤±è´¥: {e}")
            return False
    
    def _json_to_excel(self, source_path: str, target_path: str, chunk_size: int) -> bool:
        """JSONè½¬Excel"""
        try:
            with open(source_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if isinstance(data, list):
                df = pd.DataFrame(data)
            else:
                df = pd.DataFrame([data])
            
            df.to_excel(target_path, index=False)
            return True
        except Exception as e:
            logger_manager.error(f"JSONè½¬Excelå¤±è´¥: {e}")
            return False
    
    def _excel_to_json(self, source_path: str, target_path: str, chunk_size: int) -> bool:
        """Excelè½¬JSON"""
        try:
            df = pd.read_excel(source_path)
            data = df.to_dict('records')
            
            with open(target_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            
            return True
        except Exception as e:
            logger_manager.error(f"Excelè½¬JSONå¤±è´¥: {e}")
            return False
    
    def _csv_to_pickle(self, source_path: str, target_path: str, chunk_size: int) -> bool:
        """CSVè½¬Pickle"""
        try:
            df = pd.read_csv(source_path)
            df.to_pickle(target_path)
            return True
        except Exception as e:
            logger_manager.error(f"CSVè½¬Pickleå¤±è´¥: {e}")
            return False
    
    def _pickle_to_csv(self, source_path: str, target_path: str, chunk_size: int) -> bool:
        """Pickleè½¬CSV"""
        try:
            df = pd.read_pickle(source_path)
            df.to_csv(target_path, index=False)
            return True
        except Exception as e:
            logger_manager.error(f"Pickleè½¬CSVå¤±è´¥: {e}")
            return False
    
    def _numpy_to_csv(self, source_path: str, target_path: str, chunk_size: int) -> bool:
        """Numpyè½¬CSV"""
        try:
            data = np.load(source_path)
            df = pd.DataFrame(data)
            df.to_csv(target_path, index=False)
            return True
        except Exception as e:
            logger_manager.error(f"Numpyè½¬CSVå¤±è´¥: {e}")
            return False
    
    def _csv_to_numpy(self, source_path: str, target_path: str, chunk_size: int) -> bool:
        """CSVè½¬Numpy"""
        try:
            df = pd.read_csv(source_path)
            data = df.values
            np.save(target_path, data)
            return True
        except Exception as e:
            logger_manager.error(f"CSVè½¬Numpyå¤±è´¥: {e}")
            return False


class DataValidator:
    """æ•°æ®éªŒè¯å™¨"""
    
    @staticmethod
    def validate_migration(source_path: str, target_path: str,
                          source_format: DataFormat, target_format: DataFormat) -> Dict[str, Any]:
        """
        éªŒè¯è¿ç§»ç»“æœ
        
        Args:
            source_path: æºæ–‡ä»¶è·¯å¾„
            target_path: ç›®æ ‡æ–‡ä»¶è·¯å¾„
            source_format: æºæ ¼å¼
            target_format: ç›®æ ‡æ ¼å¼
            
        Returns:
            éªŒè¯ç»“æœ
        """
        try:
            validation_result = {
                'valid': False,
                'source_exists': os.path.exists(source_path),
                'target_exists': os.path.exists(target_path),
                'source_size': 0,
                'target_size': 0,
                'record_count_match': False,
                'data_integrity': False,
                'errors': []
            }
            
            if not validation_result['source_exists']:
                validation_result['errors'].append("æºæ–‡ä»¶ä¸å­˜åœ¨")
                return validation_result
            
            if not validation_result['target_exists']:
                validation_result['errors'].append("ç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨")
                return validation_result
            
            # æ–‡ä»¶å¤§å°æ£€æŸ¥
            validation_result['source_size'] = os.path.getsize(source_path)
            validation_result['target_size'] = os.path.getsize(target_path)
            
            # è®°å½•æ•°é‡æ£€æŸ¥
            try:
                source_count = DataValidator._count_records(source_path, source_format)
                target_count = DataValidator._count_records(target_path, target_format)
                
                validation_result['source_record_count'] = source_count
                validation_result['target_record_count'] = target_count
                validation_result['record_count_match'] = source_count == target_count
                
                if not validation_result['record_count_match']:
                    validation_result['errors'].append(f"è®°å½•æ•°é‡ä¸åŒ¹é…: æº{source_count} vs ç›®æ ‡{target_count}")
                
            except Exception as e:
                validation_result['errors'].append(f"è®°å½•æ•°é‡æ£€æŸ¥å¤±è´¥: {e}")
            
            # æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
            try:
                validation_result['data_integrity'] = DataValidator._check_data_integrity(
                    source_path, target_path, source_format, target_format
                )
                
                if not validation_result['data_integrity']:
                    validation_result['errors'].append("æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥")
                
            except Exception as e:
                validation_result['errors'].append(f"æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å¼‚å¸¸: {e}")
            
            # æ€»ä½“éªŒè¯ç»“æœ
            validation_result['valid'] = (
                validation_result['source_exists'] and
                validation_result['target_exists'] and
                validation_result['record_count_match'] and
                validation_result['data_integrity'] and
                len(validation_result['errors']) == 0
            )
            
            return validation_result
            
        except Exception as e:
            logger_manager.error(f"éªŒè¯è¿ç§»ç»“æœå¤±è´¥: {e}")
            return {
                'valid': False,
                'errors': [f"éªŒè¯å¼‚å¸¸: {e}"]
            }
    
    @staticmethod
    def _count_records(file_path: str, data_format: DataFormat) -> int:
        """è®¡ç®—è®°å½•æ•°é‡"""
        try:
            if data_format == DataFormat.CSV:
                with open(file_path, 'r', encoding='utf-8') as f:
                    return sum(1 for line in f) - 1  # å‡å»æ ‡é¢˜è¡Œ
            
            elif data_format == DataFormat.JSON:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        return len(data)
                    else:
                        return 1
            
            elif data_format in [DataFormat.EXCEL]:
                df = pd.read_excel(file_path)
                return len(df)
            
            elif data_format == DataFormat.PICKLE:
                df = pd.read_pickle(file_path)
                return len(df)
            
            else:
                return 0
                
        except Exception as e:
            logger_manager.error(f"è®¡ç®—è®°å½•æ•°é‡å¤±è´¥: {e}")
            return 0
    
    @staticmethod
    def _check_data_integrity(source_path: str, target_path: str,
                             source_format: DataFormat, target_format: DataFormat) -> bool:
        """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§"""
        try:
            # ç®€åŒ–çš„å®Œæ•´æ€§æ£€æŸ¥ï¼šæ¯”è¾ƒç¬¬ä¸€è¡Œå’Œæœ€åä¸€è¡Œæ•°æ®
            source_sample = DataValidator._get_data_sample(source_path, source_format)
            target_sample = DataValidator._get_data_sample(target_path, target_format)
            
            if source_sample and target_sample:
                # æ¯”è¾ƒæ ·æœ¬æ•°æ®çš„å…³é”®å­—æ®µ
                return len(source_sample) == len(target_sample)
            
            return False
            
        except Exception as e:
            logger_manager.error(f"æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    @staticmethod
    def _get_data_sample(file_path: str, data_format: DataFormat) -> Optional[List]:
        """è·å–æ•°æ®æ ·æœ¬"""
        try:
            if data_format == DataFormat.CSV:
                df = pd.read_csv(file_path, nrows=2)
                return df.values.tolist()
            
            elif data_format == DataFormat.JSON:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        return data[:2] if len(data) >= 2 else data
                    else:
                        return [data]
            
            elif data_format == DataFormat.EXCEL:
                df = pd.read_excel(file_path, nrows=2)
                return df.values.tolist()
            
            return None
            
        except Exception as e:
            logger_manager.error(f"è·å–æ•°æ®æ ·æœ¬å¤±è´¥: {e}")
            return None


class MigrationTool:
    """è¿ç§»å·¥å…·ä¸»ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–è¿ç§»å·¥å…·"""
        self.format_detector = DataFormatDetector()
        self.data_converter = DataConverter()
        self.data_validator = DataValidator()
        self.migration_history = []
        
        logger_manager.info("è¿ç§»å·¥å…·åˆå§‹åŒ–å®Œæˆ")
    
    def migrate_data(self, config: MigrationConfig) -> MigrationResult:
        """
        æ‰§è¡Œæ•°æ®è¿ç§»
        
        Args:
            config: è¿ç§»é…ç½®
            
        Returns:
            è¿ç§»ç»“æœ
        """
        migration_id = f"migration_{int(datetime.now().timestamp())}"
        
        result = MigrationResult(
            migration_id=migration_id,
            config=config,
            status=MigrationStatus.RUNNING,
            start_time=datetime.now()
        )
        
        try:
            logger_manager.info(f"å¼€å§‹æ•°æ®è¿ç§»: {config.name}")
            
            # æ£€æŸ¥æºæ–‡ä»¶
            if not os.path.exists(config.source_path):
                raise DeepLearningException(f"æºæ–‡ä»¶ä¸å­˜åœ¨: {config.source_path}")
            
            # è‡ªåŠ¨æ£€æµ‹æºæ ¼å¼ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
            if config.source_format is None:
                detected_format = self.format_detector.detect_format(config.source_path)
                if detected_format:
                    config.source_format = detected_format
                else:
                    raise DeepLearningException("æ— æ³•æ£€æµ‹æºæ–‡ä»¶æ ¼å¼")
            
            # åˆ›å»ºå¤‡ä»½
            if config.backup_enabled:
                result.backup_path = self._create_backup(config.source_path)
            
            # æ‰§è¡Œæ•°æ®è½¬æ¢
            success = self.data_converter.convert(
                config.source_path,
                config.target_path,
                config.source_format,
                config.target_format,
                config.chunk_size,
                config.custom_transformer
            )
            
            if not success:
                raise DeepLearningException("æ•°æ®è½¬æ¢å¤±è´¥")
            
            # éªŒè¯è¿ç§»ç»“æœ
            if config.validation_enabled:
                validation_result = self.data_validator.validate_migration(
                    config.source_path,
                    config.target_path,
                    config.source_format,
                    config.target_format
                )
                
                result.validation_results = validation_result
                
                if not validation_result['valid']:
                    raise DeepLearningException(f"è¿ç§»éªŒè¯å¤±è´¥: {validation_result['errors']}")
            
            # æ›´æ–°ç»“æœ
            result.status = MigrationStatus.COMPLETED
            result.records_migrated = result.validation_results.get('target_record_count', 0)
            result.records_processed = result.records_migrated
            
            logger_manager.info(f"æ•°æ®è¿ç§»å®Œæˆ: {config.name}")
            
        except Exception as e:
            result.status = MigrationStatus.FAILED
            result.error_message = str(e)
            logger_manager.error(f"æ•°æ®è¿ç§»å¤±è´¥: {e}")
        
        finally:
            result.end_time = datetime.now()
            self.migration_history.append(result)
        
        return result
    
    def _create_backup(self, source_path: str) -> str:
        """åˆ›å»ºå¤‡ä»½"""
        try:
            backup_dir = os.path.join(os.path.dirname(source_path), "backups")
            os.makedirs(backup_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = os.path.basename(source_path)
            backup_filename = f"{timestamp}_{filename}"
            backup_path = os.path.join(backup_dir, backup_filename)
            
            shutil.copy2(source_path, backup_path)
            
            logger_manager.info(f"å¤‡ä»½åˆ›å»ºæˆåŠŸ: {backup_path}")
            return backup_path
            
        except Exception as e:
            logger_manager.error(f"åˆ›å»ºå¤‡ä»½å¤±è´¥: {e}")
            return ""
    
    def get_migration_history(self) -> List[MigrationResult]:
        """è·å–è¿ç§»å†å²"""
        return self.migration_history.copy()
    
    def get_supported_formats(self) -> List[DataFormat]:
        """è·å–æ”¯æŒçš„æ ¼å¼"""
        return list(DataFormat)
    
    def get_supported_conversions(self) -> List[Tuple[DataFormat, DataFormat]]:
        """è·å–æ”¯æŒçš„è½¬æ¢"""
        return list(self.data_converter.converters.keys())


# å…¨å±€è¿ç§»å·¥å…·å®ä¾‹
migration_tool = MigrationTool()


if __name__ == "__main__":
    # æµ‹è¯•è¿ç§»å·¥å…·åŠŸèƒ½
    print("ğŸ”„ æµ‹è¯•è¿ç§»å·¥å…·åŠŸèƒ½...")
    
    try:
        import tempfile
        
        # åˆ›å»ºä¸´æ—¶æµ‹è¯•æ–‡ä»¶
        with tempfile.TemporaryDirectory() as temp_dir:
            # åˆ›å»ºæµ‹è¯•CSVæ–‡ä»¶
            csv_file = os.path.join(temp_dir, "test.csv")
            test_data = pd.DataFrame({
                'id': [1, 2, 3],
                'name': ['Alice', 'Bob', 'Charlie'],
                'age': [25, 30, 35]
            })
            test_data.to_csv(csv_file, index=False)
            
            # æµ‹è¯•æ ¼å¼æ£€æµ‹
            detected_format = DataFormatDetector.detect_format(csv_file)
            if detected_format == DataFormat.CSV:
                print("âœ… æ ¼å¼æ£€æµ‹æˆåŠŸ")
            
            # æµ‹è¯•æ•°æ®è¿ç§»
            json_file = os.path.join(temp_dir, "test.json")
            
            config = MigrationConfig(
                name="test_migration",
                source_path=csv_file,
                target_path=json_file,
                source_format=DataFormat.CSV,
                target_format=DataFormat.JSON,
                description="æµ‹è¯•è¿ç§»"
            )
            
            tool = MigrationTool()
            result = tool.migrate_data(config)
            
            if result.status == MigrationStatus.COMPLETED:
                print(f"âœ… æ•°æ®è¿ç§»æˆåŠŸ: {result.records_migrated} æ¡è®°å½•")
            else:
                print(f"âŒ æ•°æ®è¿ç§»å¤±è´¥: {result.error_message}")
            
            # æµ‹è¯•æ”¯æŒçš„æ ¼å¼
            formats = tool.get_supported_formats()
            print(f"âœ… æ”¯æŒçš„æ ¼å¼: {len(formats)} ç§")
            
            # æµ‹è¯•æ”¯æŒçš„è½¬æ¢
            conversions = tool.get_supported_conversions()
            print(f"âœ… æ”¯æŒçš„è½¬æ¢: {len(conversions)} ç§")
        
        print("âœ… è¿ç§»å·¥å…·åŠŸèƒ½æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print("è¿ç§»å·¥å…·åŠŸèƒ½æµ‹è¯•å®Œæˆ")
