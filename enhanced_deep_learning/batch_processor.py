#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ‰¹å¤„ç†ç³»ç»Ÿ
æä¾›æ‰¹é‡é¢„æµ‹å’ŒåŠ¨æ€æ‰¹å¤§å°è°ƒæ•´åŠŸèƒ½
"""

import os
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Any, Optional, Union, Callable
import multiprocessing
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import time

from core_modules import logger_manager, task_manager, with_progress
from .gpu_accelerator import GPUAccelerator, GPUMemoryManager


class BatchProcessor:
    """æ‰¹å¤„ç†ç³»ç»Ÿ"""
    
    def __init__(self, initial_batch_size: int = 32, 
               memory_manager: Optional[GPUMemoryManager] = None,
               max_workers: Optional[int] = None):
        """
        åˆå§‹åŒ–æ‰¹å¤„ç†ç³»ç»Ÿ
        
        Args:
            initial_batch_size: åˆå§‹æ‰¹å¤„ç†å¤§å°
            memory_manager: GPUå†…å­˜ç®¡ç†å™¨ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºæ–°çš„
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨CPUæ ¸å¿ƒæ•°
        """
        self.initial_batch_size = initial_batch_size
        self.current_batch_size = initial_batch_size
        self.memory_manager = memory_manager or GPUMemoryManager()
        self.max_workers = max_workers or multiprocessing.cpu_count()
        self.adaptive_batch_size = True
        
        logger_manager.info(f"æ‰¹å¤„ç†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œåˆå§‹æ‰¹å¤§å°: {initial_batch_size}ï¼Œæœ€å¤§å·¥ä½œçº¿ç¨‹: {self.max_workers}")
    
    def set_batch_size(self, batch_size: int) -> None:
        """
        è®¾ç½®æ‰¹å¤„ç†å¤§å°
        
        Args:
            batch_size: æ‰¹å¤„ç†å¤§å°
        """
        self.current_batch_size = max(1, batch_size)
        logger_manager.info(f"æ‰¹å¤„ç†å¤§å°å·²è®¾ç½®ä¸º {self.current_batch_size}")
    
    def enable_adaptive_batch_size(self, enabled: bool = True) -> None:
        """
        å¯ç”¨è‡ªé€‚åº”æ‰¹å¤„ç†å¤§å°
        
        Args:
            enabled: æ˜¯å¦å¯ç”¨
        """
        self.adaptive_batch_size = enabled
        logger_manager.info(f"è‡ªé€‚åº”æ‰¹å¤„ç†å¤§å°å·²{'å¯ç”¨' if enabled else 'ç¦ç”¨'}")
    
    def optimize_batch_size(self, model_size_mb: int) -> int:
        """
        ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°
        
        Args:
            model_size_mb: æ¨¡å‹å¤§å°ï¼ˆMBï¼‰
            
        Returns:
            ä¼˜åŒ–åçš„æ‰¹å¤„ç†å¤§å°
        """
        if not self.adaptive_batch_size:
            return self.current_batch_size
        
        # ä½¿ç”¨å†…å­˜ç®¡ç†å™¨ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°
        optimized_batch_size = self.memory_manager.optimize_batch_size(
            self.current_batch_size, model_size_mb)
        
        # æ›´æ–°å½“å‰æ‰¹å¤„ç†å¤§å°
        self.current_batch_size = optimized_batch_size
        
        return optimized_batch_size
    
    @with_progress(100, "æ‰¹é‡å¤„ç†")
    def batch_process(self, progress_bar, data: np.ndarray, 
                    process_func: Callable[[np.ndarray], np.ndarray],
                    model_size_mb: int = 500) -> np.ndarray:
        """
        æ‰¹é‡å¤„ç†æ•°æ®
        
        Args:
            progress_bar: è¿›åº¦æ¡
            data: è¾“å…¥æ•°æ®
            process_func: å¤„ç†å‡½æ•°ï¼Œæ¥å—ä¸€ä¸ªæ‰¹æ¬¡æ•°æ®ï¼Œè¿”å›å¤„ç†ç»“æœ
            model_size_mb: æ¨¡å‹å¤§å°ï¼ˆMBï¼‰ï¼Œç”¨äºä¼˜åŒ–æ‰¹å¤„ç†å¤§å°
            
        Returns:
            å¤„ç†ç»“æœ
        """
        if data is None or len(data) == 0:
            logger_manager.warning("æ²¡æœ‰æ•°æ®å¯å¤„ç†")
            return np.array([])
        
        # ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°
        if self.adaptive_batch_size:
            self.optimize_batch_size(model_size_mb)
        
        # è®¡ç®—æ‰¹æ¬¡æ•°
        n_samples = len(data)
        batch_size = min(self.current_batch_size, n_samples)
        n_batches = (n_samples + batch_size - 1) // batch_size  # å‘ä¸Šå–æ•´
        
        # æ›´æ–°è¿›åº¦æ¡æ€»æ•°
        progress_bar.total = n_batches
        
        # æ‰¹é‡å¤„ç†
        results = []
        
        for i in range(0, n_samples, batch_size):
            # è·å–å½“å‰æ‰¹æ¬¡
            batch_end = min(i + batch_size, n_samples)
            batch_data = data[i:batch_end]
            
            # å¤„ç†å½“å‰æ‰¹æ¬¡
            try:
                batch_result = process_func(batch_data)
                results.append(batch_result)
            except Exception as e:
                logger_manager.error(f"æ‰¹å¤„ç†å¤±è´¥: {e}")
                
                # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªæ‰¹æ¬¡å°±å¤±è´¥ï¼Œå°è¯•å‡å°æ‰¹å¤„ç†å¤§å°
                if i == 0 and self.adaptive_batch_size:
                    reduced_batch_size = self.current_batch_size // 2
                    if reduced_batch_size >= 1:
                        logger_manager.info(f"å‡å°æ‰¹å¤„ç†å¤§å°å¹¶é‡è¯•: {self.current_batch_size} -> {reduced_batch_size}")
                        self.current_batch_size = reduced_batch_size
                        
                        # é€’å½’è°ƒç”¨è‡ªèº«é‡è¯•
                        return self.batch_process(progress_bar, data, process_func, model_size_mb)
            
            # æ¸…ç†GPUå†…å­˜ï¼ˆå¦‚æœéœ€è¦ï¼‰
            self.memory_manager.clear_if_needed()
            
            # æ›´æ–°è¿›åº¦
            progress_bar.update(1)
        
        # åˆå¹¶ç»“æœ
        if results:
            try:
                combined_results = np.vstack(results)
            except:
                # å¦‚æœæ— æ³•å‚ç›´å †å ï¼Œå°è¯•æ°´å¹³è¿æ¥
                try:
                    combined_results = np.concatenate(results)
                except:
                    # å¦‚æœä»ç„¶å¤±è´¥ï¼Œè¿”å›åˆ—è¡¨
                    combined_results = results
        else:
            combined_results = np.array([])
        
        logger_manager.info(f"æ‰¹å¤„ç†å®Œæˆï¼Œå¤„ç† {n_samples} ä¸ªæ ·æœ¬ï¼Œæ‰¹å¤§å°: {batch_size}ï¼Œæ‰¹æ¬¡æ•°: {n_batches}")
        
        return combined_results
    
    def parallel_batch_process(self, data: np.ndarray, 
                             process_func: Callable[[np.ndarray], np.ndarray],
                             use_processes: bool = False) -> np.ndarray:
        """
        å¹¶è¡Œæ‰¹é‡å¤„ç†æ•°æ®
        
        Args:
            data: è¾“å…¥æ•°æ®
            process_func: å¤„ç†å‡½æ•°ï¼Œæ¥å—ä¸€ä¸ªæ‰¹æ¬¡æ•°æ®ï¼Œè¿”å›å¤„ç†ç»“æœ
            use_processes: æ˜¯å¦ä½¿ç”¨è¿›ç¨‹æ± ï¼ˆTrueï¼‰æˆ–çº¿ç¨‹æ± ï¼ˆFalseï¼‰
            
        Returns:
            å¤„ç†ç»“æœ
        """
        if data is None or len(data) == 0:
            logger_manager.warning("æ²¡æœ‰æ•°æ®å¯å¤„ç†")
            return np.array([])
        
        # è®¡ç®—æ¯ä¸ªå·¥ä½œçº¿ç¨‹çš„æ•°æ®é‡
        n_samples = len(data)
        n_workers = min(self.max_workers, n_samples)
        chunk_size = (n_samples + n_workers - 1) // n_workers  # å‘ä¸Šå–æ•´
        
        # å‡†å¤‡æ•°æ®å—
        data_chunks = [data[i:min(i + chunk_size, n_samples)] 
                      for i in range(0, n_samples, chunk_size)]
        
        # é€‰æ‹©æ‰§è¡Œå™¨
        executor_class = ProcessPoolExecutor if use_processes else ThreadPoolExecutor
        
        # å¹¶è¡Œå¤„ç†
        results = []
        start_time = time.time()
        
        with executor_class(max_workers=n_workers) as executor:
            # æäº¤ä»»åŠ¡
            futures = [executor.submit(process_func, chunk) for chunk in data_chunks]
            
            # æ”¶é›†ç»“æœ
            for future in futures:
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    logger_manager.error(f"å¹¶è¡Œå¤„ç†å¤±è´¥: {e}")
        
        # è®¡ç®—å¤„ç†æ—¶é—´
        elapsed_time = time.time() - start_time
        
        # åˆå¹¶ç»“æœ
        if results:
            try:
                combined_results = np.vstack(results)
            except:
                # å¦‚æœæ— æ³•å‚ç›´å †å ï¼Œå°è¯•æ°´å¹³è¿æ¥
                try:
                    combined_results = np.concatenate(results)
                except:
                    # å¦‚æœä»ç„¶å¤±è´¥ï¼Œè¿”å›åˆ—è¡¨
                    combined_results = results
        else:
            combined_results = np.array([])
        
        logger_manager.info(f"å¹¶è¡Œå¤„ç†å®Œæˆï¼Œå¤„ç† {n_samples} ä¸ªæ ·æœ¬ï¼Œå·¥ä½œçº¿ç¨‹: {n_workers}ï¼Œç”¨æ—¶: {elapsed_time:.2f}ç§’")
        
        return combined_results
    
    def dynamic_batch_processing(self, data: np.ndarray, 
                               process_func: Callable[[np.ndarray], np.ndarray],
                               initial_batch_size: Optional[int] = None,
                               target_time_per_batch: float = 1.0) -> np.ndarray:
        """
        åŠ¨æ€æ‰¹å¤„ç†
        
        Args:
            data: è¾“å…¥æ•°æ®
            process_func: å¤„ç†å‡½æ•°ï¼Œæ¥å—ä¸€ä¸ªæ‰¹æ¬¡æ•°æ®ï¼Œè¿”å›å¤„ç†ç»“æœ
            initial_batch_size: åˆå§‹æ‰¹å¤„ç†å¤§å°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰æ‰¹å¤„ç†å¤§å°
            target_time_per_batch: æ¯æ‰¹æ¬¡ç›®æ ‡å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            å¤„ç†ç»“æœ
        """
        if data is None or len(data) == 0:
            logger_manager.warning("æ²¡æœ‰æ•°æ®å¯å¤„ç†")
            return np.array([])
        
        # è®¾ç½®åˆå§‹æ‰¹å¤„ç†å¤§å°
        batch_size = initial_batch_size or self.current_batch_size
        
        # è®¡ç®—æ‰¹æ¬¡æ•°
        n_samples = len(data)
        n_batches = (n_samples + batch_size - 1) // batch_size  # å‘ä¸Šå–æ•´
        
        # æ‰¹é‡å¤„ç†
        results = []
        processed_samples = 0
        
        with task_manager.progress_bar(total=n_samples, desc="åŠ¨æ€æ‰¹å¤„ç†") as progress_bar:
            while processed_samples < n_samples:
                # è·å–å½“å‰æ‰¹æ¬¡
                batch_end = min(processed_samples + batch_size, n_samples)
                batch_data = data[processed_samples:batch_end]
                batch_size_actual = len(batch_data)
                
                # å¤„ç†å½“å‰æ‰¹æ¬¡å¹¶è®¡æ—¶
                start_time = time.time()
                try:
                    batch_result = process_func(batch_data)
                    results.append(batch_result)
                except Exception as e:
                    logger_manager.error(f"æ‰¹å¤„ç†å¤±è´¥: {e}")
                    
                    # å‡å°æ‰¹å¤„ç†å¤§å°å¹¶é‡è¯•
                    if batch_size > 1:
                        batch_size = max(1, batch_size // 2)
                        logger_manager.info(f"å‡å°æ‰¹å¤„ç†å¤§å°å¹¶é‡è¯•: {batch_size}")
                        continue
                
                # è®¡ç®—å¤„ç†æ—¶é—´
                elapsed_time = time.time() - start_time
                
                # åŠ¨æ€è°ƒæ•´æ‰¹å¤„ç†å¤§å°
                if elapsed_time > 0:
                    # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å¤„ç†æ—¶é—´
                    time_per_sample = elapsed_time / batch_size_actual
                    
                    # è®¡ç®—æ–°çš„æ‰¹å¤„ç†å¤§å°ï¼Œä½¿æ¯æ‰¹æ¬¡å¤„ç†æ—¶é—´æ¥è¿‘ç›®æ ‡æ—¶é—´
                    new_batch_size = int(target_time_per_batch / time_per_sample)
                    
                    # é™åˆ¶æ‰¹å¤„ç†å¤§å°å˜åŒ–å¹…åº¦
                    new_batch_size = max(batch_size // 2, min(batch_size * 2, new_batch_size))
                    
                    # ç¡®ä¿æ‰¹å¤„ç†å¤§å°è‡³å°‘ä¸º1
                    new_batch_size = max(1, new_batch_size)
                    
                    if new_batch_size != batch_size:
                        logger_manager.debug(f"åŠ¨æ€è°ƒæ•´æ‰¹å¤„ç†å¤§å°: {batch_size} -> {new_batch_size}")
                        batch_size = new_batch_size
                
                # æ¸…ç†GPUå†…å­˜ï¼ˆå¦‚æœéœ€è¦ï¼‰
                self.memory_manager.clear_if_needed()
                
                # æ›´æ–°è¿›åº¦
                processed_samples = batch_end
                progress_bar.update(batch_size_actual)
        
        # åˆå¹¶ç»“æœ
        if results:
            try:
                combined_results = np.vstack(results)
            except:
                # å¦‚æœæ— æ³•å‚ç›´å †å ï¼Œå°è¯•æ°´å¹³è¿æ¥
                try:
                    combined_results = np.concatenate(results)
                except:
                    # å¦‚æœä»ç„¶å¤±è´¥ï¼Œè¿”å›åˆ—è¡¨
                    combined_results = results
        else:
            combined_results = np.array([])
        
        logger_manager.info(f"åŠ¨æ€æ‰¹å¤„ç†å®Œæˆï¼Œå¤„ç† {n_samples} ä¸ªæ ·æœ¬ï¼Œæœ€ç»ˆæ‰¹å¤§å°: {batch_size}")
        
        return combined_results


if __name__ == "__main__":
    # æµ‹è¯•æ‰¹å¤„ç†ç³»ç»Ÿ
    print("ğŸš€ æµ‹è¯•æ‰¹å¤„ç†ç³»ç»Ÿ...")
    
    # åˆ›å»ºæ‰¹å¤„ç†ç³»ç»Ÿ
    processor = BatchProcessor()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = np.random.rand(1000, 10)
    
    # å®šä¹‰å¤„ç†å‡½æ•°
    def process_function(batch):
        # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        time.sleep(0.01)
        return batch * 2
    
    # æµ‹è¯•æ‰¹é‡å¤„ç†
    print("\næµ‹è¯•æ‰¹é‡å¤„ç†...")
    result1 = processor.batch_process(test_data, process_function)
    print(f"æ‰¹é‡å¤„ç†ç»“æœå½¢çŠ¶: {result1.shape}")
    
    # æµ‹è¯•å¹¶è¡Œå¤„ç†
    print("\næµ‹è¯•å¹¶è¡Œå¤„ç†...")
    result2 = processor.parallel_batch_process(test_data, process_function)
    print(f"å¹¶è¡Œå¤„ç†ç»“æœå½¢çŠ¶: {result2.shape}")
    
    # æµ‹è¯•åŠ¨æ€æ‰¹å¤„ç†
    print("\næµ‹è¯•åŠ¨æ€æ‰¹å¤„ç†...")
    result3 = processor.dynamic_batch_processing(test_data, process_function)
    print(f"åŠ¨æ€æ‰¹å¤„ç†ç»“æœå½¢çŠ¶: {result3.shape}")
    
    print("æ‰¹å¤„ç†ç³»ç»Ÿæµ‹è¯•å®Œæˆ")