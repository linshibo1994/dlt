#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
GANé¢„æµ‹å™¨
åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„æ·±åº¦å­¦ä¹ é¢„æµ‹æ¨¡å‹
"""

import os
import numpy as np
import tensorflow as tf
from typing import List, Tuple, Dict, Any
from tensorflow.keras import layers, Model, optimizers
from datetime import datetime

from .base import BaseDeepPredictor
from .config import DEFAULT_GAN_CONFIG
from .exceptions import ModelInitializationError, handle_model_error
from core_modules import logger_manager


class GANPredictor(BaseDeepPredictor):
    """åŸºäºGANçš„å½©ç¥¨é¢„æµ‹æ¨¡å‹"""
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–GANé¢„æµ‹å™¨
        
        Args:
            config: é…ç½®å‚æ•°å­—å…¸
        """
        # åˆå¹¶é»˜è®¤é…ç½®å’Œç”¨æˆ·é…ç½®
        merged_config = DEFAULT_GAN_CONFIG.copy()
        if config:
            merged_config.update(config)
        
        super().__init__(name="GAN", config=merged_config)
        
        # ä»é…ç½®ä¸­æå–å‚æ•°
        self.latent_dim = self.config.get('latent_dim', 100)
        self.generator_layers = self.config.get('generator_layers', [128, 256, 128])
        self.discriminator_layers = self.config.get('discriminator_layers', [128, 64, 32])
        self.learning_rate = self.config.get('learning_rate', 0.0002)
        self.beta1 = self.config.get('beta1', 0.5)
        
        # GANç‰¹æœ‰å±æ€§
        self.generator = None
        self.discriminator = None
        self.gan = None
        
        logger_manager.info(f"åˆå§‹åŒ–GANé¢„æµ‹å™¨: latent_dim={self.latent_dim}, generator_layers={self.generator_layers}")
    
    def _build_model(self):
        """æ„å»ºGANæ¨¡å‹"""
        try:
            # æ„å»ºç”Ÿæˆå™¨
            self.generator = self._build_generator()
            
            # æ„å»ºåˆ¤åˆ«å™¨
            self.discriminator = self._build_discriminator()
            
            # æ„å»ºGAN
            self.gan = self._build_gan()
            
            # è¿”å›ç”Ÿæˆå™¨ä½œä¸ºä¸»æ¨¡å‹
            return self.generator
        except Exception as e:
            raise ModelInitializationError("GAN", str(e))
    
    def _build_generator(self):
        """æ„å»ºç”Ÿæˆå™¨"""
        model = tf.keras.Sequential(name="Generator")
        
        # è¾“å…¥å±‚
        model.add(layers.Input(shape=(self.latent_dim,)))
        
        # éšè—å±‚
        for i, units in enumerate(self.generator_layers):
            model.add(layers.Dense(units, name=f"generator_dense_{i}"))
            model.add(layers.BatchNormalization(name=f"generator_bn_{i}"))
            model.add(layers.LeakyReLU(alpha=0.2, name=f"generator_leaky_{i}"))
        
        # è¾“å‡ºå±‚ - 7ä¸ªè¾“å‡ºï¼ˆ5å‰åŒº+2ååŒºï¼‰
        model.add(layers.Dense(7, activation='sigmoid', name="generator_output"))
        
        # ç¼–è¯‘æ¨¡å‹
        model.compile(
            optimizer=optimizers.Adam(learning_rate=self.learning_rate, beta_1=self.beta1),
            loss='binary_crossentropy'
        )
        
        # æ‰“å°æ¨¡å‹æ‘˜è¦
        model.summary()
        
        return model
    
    def _build_discriminator(self):
        """æ„å»ºåˆ¤åˆ«å™¨"""
        model = tf.keras.Sequential(name="Discriminator")
        
        # è¾“å…¥å±‚
        model.add(layers.Input(shape=(7,)))
        
        # éšè—å±‚
        for i, units in enumerate(self.discriminator_layers):
            model.add(layers.Dense(units, name=f"discriminator_dense_{i}"))
            model.add(layers.LeakyReLU(alpha=0.2, name=f"discriminator_leaky_{i}"))
            model.add(layers.Dropout(0.3, name=f"discriminator_dropout_{i}"))
        
        # è¾“å‡ºå±‚ - å•ä¸€è¾“å‡ºï¼ˆçœŸ/å‡ï¼‰
        model.add(layers.Dense(1, activation='sigmoid', name="discriminator_output"))
        
        # ç¼–è¯‘æ¨¡å‹
        model.compile(
            optimizer=optimizers.Adam(learning_rate=self.learning_rate, beta_1=self.beta1),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        # æ‰“å°æ¨¡å‹æ‘˜è¦
        model.summary()
        
        return model
    
    def _build_gan(self):
        """æ„å»ºGAN"""
        # å†»ç»“åˆ¤åˆ«å™¨æƒé‡
        self.discriminator.trainable = False
        
        # åˆ›å»ºGANæ¨¡å‹
        gan_input = layers.Input(shape=(self.latent_dim,))
        generated = self.generator(gan_input)
        validity = self.discriminator(generated)
        
        model = Model(gan_input, validity, name="GAN")
        
        # ç¼–è¯‘æ¨¡å‹
        model.compile(
            optimizer=optimizers.Adam(learning_rate=self.learning_rate, beta_1=self.beta1),
            loss='binary_crossentropy'
        )
        
        # æ‰“å°æ¨¡å‹æ‘˜è¦
        model.summary()
        
        return model
    
    def _prepare_training_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        from .data_manager import DeepLearningDataManager
        
        # åˆ›å»ºæ•°æ®ç®¡ç†å™¨
        data_manager = DeepLearningDataManager()
        
        # æå–å·ç æ•°æ®
        real_samples = []
        
        for _, row in self.df.iterrows():
            front_balls, back_balls = data_manager.parse_balls(row)
            
            # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
            normalized_front = [(x - 1) / 34 for x in front_balls]  # 1-35 -> 0-1
            normalized_back = [(x - 1) / 11 for x in back_balls]    # 1-12 -> 0-1
            
            real_samples.append(normalized_front + normalized_back)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        real_samples = np.array(real_samples)
        
        # æ•°æ®å¢å¼º
        if self.config.get('data_augmentation', True):
            real_samples = data_manager.augment_data(real_samples, factor=1.5)
        
        # æ£€æµ‹å’Œå¤„ç†å¼‚å¸¸æ•°æ®
        normal_samples, anomaly_samples = data_manager.detect_anomalies(real_samples)
        
        if len(anomaly_samples) > 0:
            logger_manager.info(f"æ£€æµ‹åˆ° {len(anomaly_samples)} ä¸ªå¼‚å¸¸æ ·æœ¬ï¼Œå·²æ’é™¤")
            real_samples = normal_samples
        
        return real_samples
    
    def _save_model(self):
        """ä¿å­˜æ¨¡å‹"""
        try:
            # ä¿å­˜ç”Ÿæˆå™¨
            generator_path = os.path.join(self.model_dir, f"{self.name.lower()}_generator.h5")
            self.generator.save(generator_path)
            
            # ä¿å­˜åˆ¤åˆ«å™¨
            discriminator_path = os.path.join(self.model_dir, f"{self.name.lower()}_discriminator.h5")
            self.discriminator.save(discriminator_path)
            
            logger_manager.info(f"{self.name}æ¨¡å‹å·²ä¿å­˜")
        except Exception as e:
            logger_manager.error(f"ä¿å­˜{self.name}æ¨¡å‹å¤±è´¥: {e}")
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹"""
        try:
            # æ¨¡å‹è·¯å¾„
            generator_path = os.path.join(self.model_dir, f"{self.name.lower()}_generator.h5")
            discriminator_path = os.path.join(self.model_dir, f"{self.name.lower()}_discriminator.h5")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(generator_path) or not os.path.exists(discriminator_path):
                logger_manager.warning(f"{self.name}æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦é‡æ–°è®­ç»ƒ")
                return False
            
            # åŠ è½½æ¨¡å‹
            self.generator = tf.keras.models.load_model(generator_path)
            self.discriminator = tf.keras.models.load_model(discriminator_path)
            
            # é‡å»ºGAN
            self.gan = self._build_gan()
            
            self.is_trained = True
            logger_manager.info(f"{self.name}æ¨¡å‹åŠ è½½æˆåŠŸ")
            return True
        except Exception as e:
            logger_manager.error(f"åŠ è½½{self.name}æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def get_confidence(self) -> float:
        """
        è·å–é¢„æµ‹ç½®ä¿¡åº¦
        
        Returns:
            ç½®ä¿¡åº¦åˆ†æ•° (0.0-1.0)
        """
        if not self.is_trained:
            return 0.0
        
        # GANçš„ç½®ä¿¡åº¦é€šå¸¸ä½äºå…¶ä»–æ¨¡å‹
        base_confidence = 0.6
        
        # æ ¹æ®ç”Ÿæˆå™¨å¤æ‚åº¦è°ƒæ•´
        complexity_factor = min(1.0, len(self.generator_layers) / 4)
        
        # æ ¹æ®è®­ç»ƒæ•°æ®é‡è°ƒæ•´
        data_factor = min(1.0, len(self.df) / 1000)
        
        confidence = base_confidence * complexity_factor * data_factor
        
        return min(0.85, confidence)  # GANçš„æœ€é«˜ç½®ä¿¡åº¦é™åˆ¶åœ¨0.85
    
    def use_fallback_config(self):
        """ä½¿ç”¨å¤‡ç”¨é…ç½®"""
        logger_manager.info("ä½¿ç”¨GANå¤‡ç”¨é…ç½®")
        
        # ç®€åŒ–æ¨¡å‹é…ç½®
        self.latent_dim = 50
        self.generator_layers = [64, 128, 64]
        self.discriminator_layers = [64, 32]
        
        # æ›´æ–°é…ç½®å­—å…¸
        self.config.update({
            'latent_dim': self.latent_dim,
            'generator_layers': self.generator_layers,
            'discriminator_layers': self.discriminator_layers
        })
    
    def use_simple_model(self):
        """ä½¿ç”¨ç®€å•æ¨¡å‹"""
        logger_manager.info("ä½¿ç”¨ç®€å•GANæ¨¡å‹")
        
        # æç®€é…ç½®
        self.latent_dim = 20
        self.generator_layers = [32, 64]
        self.discriminator_layers = [32]
        
        # æ›´æ–°é…ç½®å­—å…¸
        self.config.update({
            'latent_dim': self.latent_dim,
            'generator_layers': self.generator_layers,
            'discriminator_layers': self.discriminator_layers
        })


    @handle_model_error
    def train(self, epochs=None, batch_size=None, sample_interval=100):
        """
        è®­ç»ƒGANæ¨¡å‹
        
        Args:
            epochs: è®­ç»ƒè½®æ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„å€¼
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„å€¼
            sample_interval: é‡‡æ ·é—´éš”ï¼Œæ¯éš”å¤šå°‘è½®è¾“å‡ºä¸€æ¬¡çŠ¶æ€
            
        Returns:
            è®­ç»ƒæ˜¯å¦æˆåŠŸ
        """
        from .training_utils import TrainingProgressCallback, TrainingVisualizer
        
        if epochs is None:
            epochs = self.config.get('epochs', 200)
        
        if batch_size is None:
            batch_size = self.config.get('batch_size', 64)
        
        logger_manager.info(f"å¼€å§‹è®­ç»ƒGANæ¨¡å‹: epochs={epochs}, batch_size={batch_size}")
        
        try:
            # æ„å»ºæ¨¡å‹
            if self.generator is None or self.discriminator is None or self.gan is None:
                self._build_model()
            
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            real_samples = self._prepare_training_data()
            
            if len(real_samples) == 0:
                logger_manager.error("è®­ç»ƒæ•°æ®ä¸è¶³ï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹")
                return False
            
            # åˆ›å»ºè¿›åº¦å›è°ƒ
            progress_callback = TrainingProgressCallback(self.name, epochs)
            progress_callback.on_train_begin()
            
            # åˆ›å»ºå¯è§†åŒ–å™¨
            visualizer = TrainingVisualizer(self.name)
            
            # è®­ç»ƒå†å²è®°å½•
            history = {
                'd_loss': [],
                'd_accuracy': [],
                'g_loss': []
            }
            
            # åˆ›å»ºçœŸå‡æ ‡ç­¾
            valid = np.ones((batch_size, 1))
            fake = np.zeros((batch_size, 1))
            
            # è®­ç»ƒGAN
            for epoch in range(epochs):
                # è®­ç»ƒåˆ¤åˆ«å™¨
                
                # éšæœºé€‰æ‹©çœŸå®æ ·æœ¬
                idx = np.random.randint(0, real_samples.shape[0], batch_size)
                real_batch = real_samples[idx]
                
                # ç”Ÿæˆå‡æ ·æœ¬
                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))
                gen_samples = self.generator.predict(noise, verbose=0)
                
                # æ·»åŠ å™ªå£°åˆ°æ ‡ç­¾ï¼ˆæ ‡ç­¾å¹³æ»‘åŒ–ï¼‰
                valid_smooth = valid - 0.1 * np.random.random(valid.shape)
                fake_smooth = fake + 0.1 * np.random.random(fake.shape)
                
                # è®­ç»ƒåˆ¤åˆ«å™¨
                d_loss_real = self.discriminator.train_on_batch(real_batch, valid_smooth)
                d_loss_fake = self.discriminator.train_on_batch(gen_samples, fake_smooth)
                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
                
                # è®­ç»ƒç”Ÿæˆå™¨
                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))
                g_loss = self.gan.train_on_batch(noise, valid)
                
                # è®°å½•å†å²
                history['d_loss'].append(d_loss[0])
                history['d_accuracy'].append(d_loss[1])
                history['g_loss'].append(g_loss)
                
                # æ›´æ–°è¿›åº¦
                if epoch % sample_interval == 0 or epoch == epochs - 1:
                    metrics_str = f"D loss: {d_loss[0]:.4f}, acc: {100*d_loss[1]:.2f}%, G loss: {g_loss:.4f}"
                    progress_callback.on_epoch_end(epoch, {
                        'd_loss': d_loss[0],
                        'd_accuracy': d_loss[1],
                        'g_loss': g_loss
                    })
                
                # å®ç°æ—©åœ
                if epoch > 50 and np.mean(history['d_accuracy'][-20:]) > 0.95:
                    logger_manager.info("åˆ¤åˆ«å™¨å‡†ç¡®ç‡è¿‡é«˜ï¼Œæå‰åœæ­¢è®­ç»ƒ")
                    break
                
                # åˆ¤åˆ«å™¨é‡ç½®ï¼ˆå¦‚æœåˆ¤åˆ«å™¨å¤ªå¼ºï¼‰
                if epoch > 10 and np.mean(history['d_accuracy'][-10:]) > 0.9:
                    logger_manager.info("åˆ¤åˆ«å™¨è¿‡å¼ºï¼Œé‡ç½®åˆ¤åˆ«å™¨æƒé‡")
                    self.discriminator = self._build_discriminator()
                    self.gan = self._build_gan()
            
            # å®Œæˆè®­ç»ƒ
            progress_callback.on_train_end({
                'd_loss': np.mean(history['d_loss'][-10:]),
                'd_accuracy': np.mean(history['d_accuracy'][-10:]),
                'g_loss': np.mean(history['g_loss'][-10:])
            })
            
            # å¯è§†åŒ–è®­ç»ƒå†å²
            visualizer.plot_training_history(history)
            
            self.is_trained = True
            
            # ä¿å­˜æ¨¡å‹
            self._save_model()
            
            logger_manager.info(f"{self.name}æ¨¡å‹è®­ç»ƒå®Œæˆ")
            
            return True
        
        except Exception as e:
            logger_manager.error(f"{self.name}æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return False
    
    def _generate_samples(self, count=1, noise_std=None):
        """
        ç”Ÿæˆæ ·æœ¬
        
        Args:
            count: ç”Ÿæˆæ ·æœ¬æ•°é‡
            noise_std: å™ªå£°æ ‡å‡†å·®ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„å€¼
            
        Returns:
            ç”Ÿæˆçš„æ ·æœ¬æ•°ç»„
        """
        if noise_std is None:
            noise_std = self.config.get('noise_std', 0.1)
        
        # ç”Ÿæˆéšæœºå™ªå£°
        noise = np.random.normal(0, noise_std, (count, self.latent_dim))
        
        # ç”Ÿæˆæ ·æœ¬
        return self.generator.predict(noise, verbose=0)
    
    def _select_best_samples(self, samples, count=1):
        """
        é€‰æ‹©æœ€ä½³æ ·æœ¬
        
        Args:
            samples: ç”Ÿæˆçš„æ ·æœ¬æ•°ç»„
            count: é€‰æ‹©æ•°é‡
            
        Returns:
            é€‰æ‹©çš„æœ€ä½³æ ·æœ¬
        """
        # å¦‚æœæ ·æœ¬æ•°é‡ä¸è¶³ï¼Œç›´æ¥è¿”å›
        if len(samples) <= count:
            return samples
        
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„è´¨é‡åˆ†æ•°
        scores = []
        
        for sample in samples:
            # å‰åŒºå·ç 
            front = sample[:5]
            
            # ååŒºå·ç 
            back = sample[5:7]
            
            # è®¡ç®—åˆ†å¸ƒå‡åŒ€æ€§ï¼ˆç†æƒ³æƒ…å†µä¸‹ï¼Œå·ç åº”è¯¥åˆ†å¸ƒå‡åŒ€ï¼‰
            front_std = np.std(front)
            back_std = np.std(back)
            
            # è®¡ç®—é‡å¤æ€§ï¼ˆç†æƒ³æƒ…å†µä¸‹ï¼Œå·ç ä¸åº”è¯¥é‡å¤ï¼‰
            front_unique = len(np.unique(np.round(front * 34 + 1)))
            back_unique = len(np.unique(np.round(back * 11 + 1)))
            
            # è®¡ç®—æ€»åˆ†æ•°ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
            score = (front_std * 0.3 + back_std * 0.2) + (front_unique * 0.3 + back_unique * 0.2)
            scores.append(score)
        
        # é€‰æ‹©åˆ†æ•°æœ€é«˜çš„æ ·æœ¬
        best_indices = np.argsort(scores)[-count:]
        return samples[best_indices]
    
    def _apply_gradient_penalty(self, real_samples, fake_samples):
        """
        åº”ç”¨æ¢¯åº¦æƒ©ç½šï¼ˆWasserstein GAN-GPï¼‰
        
        Args:
            real_samples: çœŸå®æ ·æœ¬
            fake_samples: ç”Ÿæˆçš„æ ·æœ¬
            
        Returns:
            æ¢¯åº¦æƒ©ç½šæŸå¤±
        """
        batch_size = real_samples.shape[0]
        
        # åˆ›å»ºéšæœºæ’å€¼
        alpha = np.random.random((batch_size, 1))
        interpolated = alpha * real_samples + (1 - alpha) * fake_samples
        
        with tf.GradientTape() as tape:
            tape.watch(interpolated)
            predictions = self.discriminator(interpolated)
        
        # è®¡ç®—æ¢¯åº¦
        gradients = tape.gradient(predictions, interpolated)
        gradients_norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=1))
        gradient_penalty = tf.reduce_mean((gradients_norm - 1.0) ** 2)
        
        return gradient_penalty


    @handle_model_error
    def predict(self, count=1, verbose=True) -> List[Tuple[List[int], List[int]]]:
        """
        ç”Ÿæˆé¢„æµ‹ç»“æœ
        
        Args:
            count: é¢„æµ‹æ³¨æ•°
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        Returns:
            é¢„æµ‹ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º(å‰åŒºå·ç åˆ—è¡¨, ååŒºå·ç åˆ—è¡¨)
        """
        from .prediction_utils import PredictionProcessor
        
        # å°è¯•åŠ è½½å·²æœ‰æ¨¡å‹
        if not self.is_trained:
            if not self._load_model():
                logger_manager.info(f"{self.name}æ¨¡å‹æœªè®­ç»ƒï¼Œå¼€å§‹è®­ç»ƒ...")
                if not self.train():
                    logger_manager.error(f"{self.name}æ¨¡å‹è®­ç»ƒå¤±è´¥")
                    return []
        
        # åˆ›å»ºé¢„æµ‹å¤„ç†å™¨
        processor = PredictionProcessor()
        
        if verbose:
            logger_manager.info(f"ä½¿ç”¨{self.name}æ¨¡å‹ç”Ÿæˆ{count}æ³¨é¢„æµ‹...")
        
        # ç”Ÿæˆæ›´å¤šæ ·æœ¬ï¼Œç„¶åé€‰æ‹©æœ€ä½³çš„
        gen_count = max(count * 3, 10)  # ç”Ÿæˆ3å€æ•°é‡çš„æ ·æœ¬
        
        # ç”Ÿæˆæ ·æœ¬
        raw_samples = self._generate_samples(gen_count)
        
        # é€‰æ‹©æœ€ä½³æ ·æœ¬
        best_samples = self._select_best_samples(raw_samples, count)
        
        # å¤„ç†é¢„æµ‹ç»“æœ
        predictions = []
        
        for i, sample in enumerate(best_samples):
            # å¤„ç†é¢„æµ‹ç»“æœ
            front_balls, back_balls = processor.process_raw_prediction(sample)
            predictions.append((front_balls, back_balls))
            
            if verbose:
                formatted = processor.format_prediction((front_balls, back_balls))
                logger_manager.info(f"é¢„æµ‹ {i+1}/{count}: {formatted}")
        
        # è®¡ç®—é¢„æµ‹ç½®ä¿¡åº¦
        confidence = processor.calculate_confidence(predictions)
        
        if verbose:
            logger_manager.info(f"{self.name}é¢„æµ‹å®Œæˆï¼Œç½®ä¿¡åº¦: {confidence:.2f}")
        
        return predictions
    
    def predict_with_details(self, count=1) -> Dict[str, Any]:
        """
        ç”Ÿæˆå¸¦è¯¦ç»†ä¿¡æ¯çš„é¢„æµ‹ç»“æœ
        
        Args:
            count: é¢„æµ‹æ³¨æ•°
            
        Returns:
            åŒ…å«é¢„æµ‹ç»“æœå’Œè¯¦ç»†ä¿¡æ¯çš„å­—å…¸
        """
        from .prediction_utils import PredictionProcessor
        
        # æ‰§è¡Œé¢„æµ‹
        predictions = self.predict(count, verbose=False)
        
        # åˆ›å»ºé¢„æµ‹å¤„ç†å™¨
        processor = PredictionProcessor()
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = processor.calculate_confidence(predictions)
        
        # æ ¼å¼åŒ–é¢„æµ‹ç»“æœ
        formatted_predictions = []
        for i, pred in enumerate(predictions):
            formatted = processor.format_prediction(pred)
            formatted_predictions.append({
                'index': i + 1,
                'front_balls': pred[0],
                'back_balls': pred[1],
                'formatted': formatted
            })
        
        # è¿”å›è¯¦ç»†ç»“æœ
        return {
            'model_name': self.name,
            'count': count,
            'predictions': formatted_predictions,
            'confidence': confidence,
            'model_config': {
                'latent_dim': self.latent_dim,
                'generator_layers': self.generator_layers,
                'discriminator_layers': self.discriminator_layers
            },
            'timestamp': datetime.now().isoformat()
        }
    
    def evaluate_predictions(self, predictions: List[Tuple[List[int], List[int]]], 
                           actuals: List[Tuple[List[int], List[int]]]) -> Dict[str, Any]:
        """
        è¯„ä¼°é¢„æµ‹ç»“æœ
        
        Args:
            predictions: é¢„æµ‹ç»“æœåˆ—è¡¨
            actuals: å®é™…ç»“æœåˆ—è¡¨
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        from .prediction_utils import PredictionEvaluator
        
        evaluator = PredictionEvaluator()
        return evaluator.evaluate_multiple_predictions(predictions, actuals)


if __name__ == "__main__":
    # æµ‹è¯•GANé¢„æµ‹å™¨
    print("ğŸ® æµ‹è¯•GANé¢„æµ‹å™¨...")
    
    # åˆ›å»ºé¢„æµ‹å™¨
    gan = GANPredictor()
    
    # æ„å»ºæ¨¡å‹
    gan._build_model()
    
    # è®­ç»ƒæ¨¡å‹ï¼ˆå°è§„æ¨¡æµ‹è¯•ï¼‰
    gan.train(epochs=10, batch_size=16, sample_interval=2)
    
    # è¿›è¡Œé¢„æµ‹
    predictions = gan.predict(3)
    
    print("GANé¢„æµ‹ç»“æœ:")
    for i, (front, back) in enumerate(predictions):
        front_str = ' '.join([str(b).zfill(2) for b in front])
        back_str = ' '.join([str(b).zfill(2) for b in back])
        print(f"ç¬¬ {i+1} æ³¨: {front_str} + {back_str}")