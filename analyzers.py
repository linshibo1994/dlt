#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¤§ä¹é€åˆ†æå™¨é›†åˆ
æ•´åˆæ‰€æœ‰åˆ†æåŠŸèƒ½ï¼šåŸºç¡€åˆ†æã€é«˜çº§åˆ†æã€æ•°æ®åˆ†æã€æ··åˆåˆ†æ
"""

import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict, Counter
from scipy import stats
from scipy.fft import fft
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False


class BaseAnalyzer:
    """åˆ†æå™¨åŸºç±»"""
    
    def __init__(self, data_file, output_dir="./output"):
        self.data_file = data_file
        self.output_dir = output_dir
        self.df = None
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # åŠ è½½æ•°æ®
        self.load_data()
    
    def load_data(self):
        """åŠ è½½æ•°æ®"""
        try:
            self.df = pd.read_csv(self.data_file)
            self.df = self.df.sort_values('issue', ascending=True)
            print(f"åˆ†æå™¨åŠ è½½æ•°æ®: {len(self.df)} æ¡è®°å½•")
            return True
        except Exception as e:
            print(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return False
    
    def parse_balls(self, balls_str):
        """è§£æå·ç å­—ç¬¦ä¸²"""
        return [int(ball.strip()) for ball in str(balls_str).split(",")]


class BasicAnalyzer(BaseAnalyzer):
    """åŸºç¡€åˆ†æå™¨ï¼šé¢‘ç‡ã€é—æ¼ã€çƒ­å†·å·åˆ†æ"""
    
    def __init__(self, data_file, output_dir="./output/basic"):
        super().__init__(data_file, output_dir)
        
        # è§£æå‰åŒºå’ŒååŒºå·ç 
        self._parse_ball_numbers()
    
    def _parse_ball_numbers(self):
        """è§£æå‰åŒºå’ŒååŒºå·ç """
        if self.df is None:
            return
        
        self.front_balls_lists = []
        self.back_balls_lists = []
        
        for _, row in self.df.iterrows():
            front_balls = self.parse_balls(row["front_balls"])
            back_balls = self.parse_balls(row["back_balls"])
            self.front_balls_lists.append(front_balls)
            self.back_balls_lists.append(back_balls)
    
    def analyze_frequency(self, save_result=True):
        """åˆ†æå·ç å‡ºç°é¢‘ç‡"""
        print("åˆ†æå·ç å‡ºç°é¢‘ç‡...")
        
        # ç»Ÿè®¡å‰åŒºå·ç é¢‘ç‡
        front_balls_flat = [ball for sublist in self.front_balls_lists for ball in sublist]
        front_counter = Counter(front_balls_flat)
        
        # ç¡®ä¿æ‰€æœ‰å¯èƒ½çš„å‰åŒºå·ç éƒ½åœ¨å­—å…¸ä¸­
        for i in range(1, 36):
            if i not in front_counter:
                front_counter[i] = 0
        
        # ç»Ÿè®¡ååŒºå·ç é¢‘ç‡
        back_balls_flat = [ball for sublist in self.back_balls_lists for ball in sublist]
        back_counter = Counter(back_balls_flat)
        
        # ç¡®ä¿æ‰€æœ‰å¯èƒ½çš„ååŒºå·ç éƒ½åœ¨å­—å…¸ä¸­
        for i in range(1, 13):
            if i not in back_counter:
                back_counter[i] = 0
        
        # è½¬æ¢ä¸ºå­—å…¸å¹¶æ’åº
        front_freq = dict(sorted(front_counter.items()))
        back_freq = dict(sorted(back_counter.items()))
        
        if save_result:
            # ä¿å­˜ç»“æœ
            result = {
                "front_frequency": front_freq,
                "back_frequency": back_freq,
                "analysis_date": pd.Timestamp.now().isoformat(),
                "total_periods": len(self.df)
            }
            
            with open(f"{self.output_dir}/frequency_analysis.json", 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            
            # ç”Ÿæˆé¢‘ç‡å›¾è¡¨
            self._plot_frequency(front_freq, back_freq)
        
        return front_freq, back_freq
    
    def _plot_frequency(self, front_freq, back_freq):
        """ç»˜åˆ¶é¢‘ç‡åˆ†å¸ƒå›¾"""
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))
        
        # å‰åŒºé¢‘ç‡å›¾
        front_balls = list(front_freq.keys())
        front_counts = list(front_freq.values())
        
        ax1.bar(front_balls, front_counts, color='skyblue', alpha=0.7)
        ax1.set_title('å‰åŒºå·ç é¢‘ç‡åˆ†å¸ƒ', fontsize=16, fontweight='bold')
        ax1.set_xlabel('å·ç ', fontsize=12)
        ax1.set_ylabel('å‡ºç°æ¬¡æ•°', fontsize=12)
        ax1.grid(True, alpha=0.3)
        
        # ååŒºé¢‘ç‡å›¾
        back_balls = list(back_freq.keys())
        back_counts = list(back_freq.values())
        
        ax2.bar(back_balls, back_counts, color='lightcoral', alpha=0.7)
        ax2.set_title('ååŒºå·ç é¢‘ç‡åˆ†å¸ƒ', fontsize=16, fontweight='bold')
        ax2.set_xlabel('å·ç ', fontsize=12)
        ax2.set_ylabel('å‡ºç°æ¬¡æ•°', fontsize=12)
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f"{self.output_dir}/frequency_distribution.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def analyze_missing_values(self, save_result=True):
        """åˆ†æå·ç é—æ¼å€¼"""
        print("åˆ†æå·ç é—æ¼å€¼...")
        
        # è®¡ç®—å‰åŒºé—æ¼å€¼
        front_missing = {}
        for ball in range(1, 36):
            missing_count = 0
            for front_list in self.front_balls_lists:
                if ball not in front_list:
                    missing_count += 1
                else:
                    break
            front_missing[ball] = missing_count
        
        # è®¡ç®—ååŒºé—æ¼å€¼
        back_missing = {}
        for ball in range(1, 13):
            missing_count = 0
            for back_list in self.back_balls_lists:
                if ball not in back_list:
                    missing_count += 1
                else:
                    break
            back_missing[ball] = missing_count
        
        if save_result:
            result = {
                "front_missing": front_missing,
                "back_missing": back_missing,
                "analysis_date": pd.Timestamp.now().isoformat(),
                "total_periods": len(self.df)
            }
            
            with open(f"{self.output_dir}/missing_analysis.json", 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
        
        return front_missing, back_missing
    
    def analyze_hot_cold_numbers(self, recent_periods=30, save_result=True):
        """åˆ†æçƒ­é—¨å’Œå†·é—¨å·ç """
        print(f"åˆ†ææœ€è¿‘{recent_periods}æœŸçƒ­é—¨å’Œå†·é—¨å·ç ...")
        
        # è·å–æœ€è¿‘NæœŸæ•°æ®
        recent_front_lists = self.front_balls_lists[:recent_periods]
        recent_back_lists = self.back_balls_lists[:recent_periods]
        
        # ç»Ÿè®¡å‰åŒºå·ç é¢‘ç‡
        front_balls_flat = [ball for sublist in recent_front_lists for ball in sublist]
        front_counter = Counter(front_balls_flat)
        
        # ç¡®ä¿æ‰€æœ‰å¯èƒ½çš„å‰åŒºå·ç éƒ½åœ¨å­—å…¸ä¸­
        for i in range(1, 36):
            if i not in front_counter:
                front_counter[i] = 0
        
        # ç»Ÿè®¡ååŒºå·ç é¢‘ç‡
        back_balls_flat = [ball for sublist in recent_back_lists for ball in sublist]
        back_counter = Counter(back_balls_flat)
        
        # ç¡®ä¿æ‰€æœ‰å¯èƒ½çš„ååŒºå·ç éƒ½åœ¨å­—å…¸ä¸­
        for i in range(1, 13):
            if i not in back_counter:
                back_counter[i] = 0
        
        # è·å–çƒ­é—¨å·ç ï¼ˆå‡ºç°é¢‘ç‡æœ€é«˜çš„å‰10ä¸ªï¼‰
        front_hot = [ball for ball, count in front_counter.most_common(10)]
        back_hot = [ball for ball, count in back_counter.most_common(6)]
        
        # è·å–å†·é—¨å·ç ï¼ˆå‡ºç°é¢‘ç‡æœ€ä½çš„å‰10ä¸ªï¼‰
        front_cold = [ball for ball, count in sorted(front_counter.items(), key=lambda x: x[1])[:10]]
        back_cold = [ball for ball, count in sorted(back_counter.items(), key=lambda x: x[1])[:6]]
        
        if save_result:
            result = {
                "recent_periods": recent_periods,
                "front_hot_numbers": front_hot,
                "back_hot_numbers": back_hot,
                "front_cold_numbers": front_cold,
                "back_cold_numbers": back_cold,
                "front_frequency": dict(front_counter),
                "back_frequency": dict(back_counter),
                "analysis_date": pd.Timestamp.now().isoformat()
            }
            
            with open(f"{self.output_dir}/hot_cold_analysis.json", 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
        
        return {
            'front_hot': front_hot,
            'back_hot': back_hot,
            'front_cold': front_cold,
            'back_cold': back_cold
        }
    
    def analyze_distribution(self, save_result=True):
        """åˆ†æå·ç åˆ†å¸ƒç‰¹å¾"""
        print("åˆ†æå·ç åˆ†å¸ƒç‰¹å¾...")
        
        # è®¡ç®—å’Œå€¼åˆ†å¸ƒ
        front_sums = [sum(front_list) for front_list in self.front_balls_lists]
        back_sums = [sum(back_list) for back_list in self.back_balls_lists]
        
        # è®¡ç®—å¥‡å¶åˆ†å¸ƒ
        front_odd_counts = [sum(1 for ball in front_list if ball % 2 == 1) for front_list in self.front_balls_lists]
        back_odd_counts = [sum(1 for ball in back_list if ball % 2 == 1) for back_list in self.back_balls_lists]
        
        # è®¡ç®—å¤§å°åˆ†å¸ƒï¼ˆå‰åŒºï¼š1-17ä¸ºå°ï¼Œ18-35ä¸ºå¤§ï¼›ååŒºï¼š1-6ä¸ºå°ï¼Œ7-12ä¸ºå¤§ï¼‰
        front_small_counts = [sum(1 for ball in front_list if ball <= 17) for front_list in self.front_balls_lists]
        back_small_counts = [sum(1 for ball in back_list if ball <= 6) for back_list in self.back_balls_lists]
        
        distribution_stats = {
            "front_sum": {
                "mean": float(np.mean(front_sums)),
                "std": float(np.std(front_sums)),
                "min": int(np.min(front_sums)),
                "max": int(np.max(front_sums))
            },
            "back_sum": {
                "mean": float(np.mean(back_sums)),
                "std": float(np.std(back_sums)),
                "min": int(np.min(back_sums)),
                "max": int(np.max(back_sums))
            },
            "front_odd_ratio": float(np.mean(front_odd_counts) / 5),
            "back_odd_ratio": float(np.mean(back_odd_counts) / 2),
            "front_small_ratio": float(np.mean(front_small_counts) / 5),
            "back_small_ratio": float(np.mean(back_small_counts) / 2)
        }
        
        if save_result:
            result = {
                "distribution_statistics": distribution_stats,
                "analysis_date": pd.Timestamp.now().isoformat(),
                "total_periods": len(self.df)
            }
            
            with open(f"{self.output_dir}/distribution_analysis.json", 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
        
        return distribution_stats
    
    def run_basic_analysis(self):
        """è¿è¡Œæ‰€æœ‰åŸºç¡€åˆ†æ"""
        print("å¼€å§‹åŸºç¡€åˆ†æ...")
        
        results = {}
        
        # åˆ†æå·ç å‡ºç°é¢‘ç‡
        front_freq, back_freq = self.analyze_frequency()
        results["frequency"] = {"front": front_freq, "back": back_freq}
        
        # åˆ†æå·ç é—æ¼å€¼
        front_missing, back_missing = self.analyze_missing_values()
        results["missing"] = {"front": front_missing, "back": back_missing}
        
        # åˆ†æçƒ­é—¨å’Œå†·é—¨å·ç 
        hot_cold = self.analyze_hot_cold_numbers()
        results["hot_cold"] = hot_cold
        
        # åˆ†æå·ç åˆ†å¸ƒ
        distribution_stats = self.analyze_distribution()
        results["distribution"] = distribution_stats
        
        print("åŸºç¡€åˆ†æå®Œæˆ")
        return results


class AdvancedAnalyzer(BaseAnalyzer):
    """é«˜çº§åˆ†æå™¨ï¼šé©¬å°”å¯å¤«é“¾ã€è´å¶æ–¯ã€æ¦‚ç‡åˆ†å¸ƒã€é¢‘ç‡æ¨¡å¼åˆ†æ"""

    def __init__(self, data_file, output_dir="./output/advanced"):
        super().__init__(data_file, output_dir)

        # è§£æå‰åŒºå’ŒååŒºå·ç 
        self._parse_ball_numbers()

    def _parse_ball_numbers(self):
        """è§£æå‰åŒºå’ŒååŒºå·ç """
        if self.df is None:
            return

        self.front_balls_lists = []
        self.back_balls_lists = []

        for _, row in self.df.iterrows():
            front_balls = self.parse_balls(row["front_balls"])
            back_balls = self.parse_balls(row["back_balls"])
            self.front_balls_lists.append(front_balls)
            self.back_balls_lists.append(back_balls)

    def analyze_statistical_features(self, save_result=True):
        """åˆ†æç»Ÿè®¡å­¦ç‰¹å¾"""
        print("åˆ†æç»Ÿè®¡å­¦ç‰¹å¾...")

        # è®¡ç®—å’Œå€¼ç»Ÿè®¡
        front_sums = [sum(front_list) for front_list in self.front_balls_lists]
        back_sums = [sum(back_list) for back_list in self.back_balls_lists]

        # è®¡ç®—æå·®ç»Ÿè®¡
        front_ranges = [max(front_list) - min(front_list) for front_list in self.front_balls_lists]
        back_ranges = [max(back_list) - min(back_list) for back_list in self.back_balls_lists]

        # è®¡ç®—ACå€¼ï¼ˆç®—æœ¯å¤æ‚æ€§ï¼‰
        front_ac_values = []
        for front_list in self.front_balls_lists:
            sorted_balls = sorted(front_list)
            ac_value = 0
            for i in range(len(sorted_balls) - 1):
                ac_value += abs(sorted_balls[i+1] - sorted_balls[i])
            front_ac_values.append(ac_value)

        # ç»Ÿè®¡ç‰¹å¾
        features = {
            "front_sum_stats": {
                "mean": float(np.mean(front_sums)),
                "std": float(np.std(front_sums)),
                "min": int(np.min(front_sums)),
                "max": int(np.max(front_sums)),
                "median": float(np.median(front_sums))
            },
            "back_sum_stats": {
                "mean": float(np.mean(back_sums)),
                "std": float(np.std(back_sums)),
                "min": int(np.min(back_sums)),
                "max": int(np.max(back_sums)),
                "median": float(np.median(back_sums))
            },
            "front_range_stats": {
                "mean": float(np.mean(front_ranges)),
                "std": float(np.std(front_ranges)),
                "min": int(np.min(front_ranges)),
                "max": int(np.max(front_ranges))
            },
            "back_range_stats": {
                "mean": float(np.mean(back_ranges)),
                "std": float(np.std(back_ranges)),
                "min": int(np.min(back_ranges)),
                "max": int(np.max(back_ranges))
            },
            "front_ac_stats": {
                "mean": float(np.mean(front_ac_values)),
                "std": float(np.std(front_ac_values)),
                "min": int(np.min(front_ac_values)),
                "max": int(np.max(front_ac_values))
            }
        }

        if save_result:
            with open(f"{self.output_dir}/statistical_features.json", 'w', encoding='utf-8') as f:
                json.dump(features, f, ensure_ascii=False, indent=2)

        return features

    def analyze_markov_chain(self, save_result=True):
        """åˆ†æé©¬å°”å¯å¤«é“¾"""
        print("åˆ†æé©¬å°”å¯å¤«é“¾...")

        # æ„å»ºè½¬ç§»çŸ©é˜µ
        front_transitions = defaultdict(lambda: defaultdict(int))
        back_transitions = defaultdict(lambda: defaultdict(int))

        # åˆ†æå‰åŒºè½¬ç§»
        for i in range(len(self.front_balls_lists) - 1):
            current_front = set(self.front_balls_lists[i])
            next_front = set(self.front_balls_lists[i + 1])

            for current_ball in current_front:
                for next_ball in next_front:
                    front_transitions[current_ball][next_ball] += 1

        # åˆ†æååŒºè½¬ç§»
        for i in range(len(self.back_balls_lists) - 1):
            current_back = set(self.back_balls_lists[i])
            next_back = set(self.back_balls_lists[i + 1])

            for current_ball in current_back:
                for next_ball in next_back:
                    back_transitions[current_ball][next_ball] += 1

        # è®¡ç®—è½¬ç§»æ¦‚ç‡
        front_transition_probs = {}
        for from_ball, to_dict in front_transitions.items():
            total = sum(to_dict.values())
            if total > 0:
                front_transition_probs[from_ball] = {
                    to_ball: count / total for to_ball, count in to_dict.items()
                }

        back_transition_probs = {}
        for from_ball, to_dict in back_transitions.items():
            total = sum(to_dict.values())
            if total > 0:
                back_transition_probs[from_ball] = {
                    to_ball: count / total for to_ball, count in to_dict.items()
                }

        markov_analysis = {
            "front_transition_matrix": dict(front_transitions),
            "back_transition_matrix": dict(back_transitions),
            "front_transition_probs": front_transition_probs,
            "back_transition_probs": back_transition_probs
        }

        if save_result:
            with open(f"{self.output_dir}/markov_chain.json", 'w', encoding='utf-8') as f:
                json.dump(markov_analysis, f, ensure_ascii=False, indent=2)

        return markov_analysis

    def analyze_probability_distribution(self, save_result=True):
        """åˆ†ææ¦‚ç‡åˆ†å¸ƒ"""
        print("åˆ†ææ¦‚ç‡åˆ†å¸ƒ...")

        # è®¡ç®—å„å·ç çš„ç†è®ºæ¦‚ç‡å’Œå®é™…æ¦‚ç‡
        front_theoretical_prob = 5 / 35  # å‰åŒºç†è®ºæ¦‚ç‡
        back_theoretical_prob = 2 / 12   # ååŒºç†è®ºæ¦‚ç‡

        # ç»Ÿè®¡å®é™…å‡ºç°æ¬¡æ•°
        front_counts = Counter()
        back_counts = Counter()

        for front_list in self.front_balls_lists:
            for ball in front_list:
                front_counts[ball] += 1

        for back_list in self.back_balls_lists:
            for ball in back_list:
                back_counts[ball] += 1

        total_periods = len(self.front_balls_lists)

        # è®¡ç®—å®é™…æ¦‚ç‡
        front_actual_probs = {}
        for ball in range(1, 36):
            front_actual_probs[ball] = front_counts.get(ball, 0) / total_periods

        back_actual_probs = {}
        for ball in range(1, 13):
            back_actual_probs[ball] = back_counts.get(ball, 0) / total_periods

        # å¡æ–¹æ£€éªŒ
        front_expected = [front_theoretical_prob * total_periods] * 35
        front_observed = [front_counts.get(i, 0) for i in range(1, 36)]
        front_chi2, front_p_value = stats.chisquare(front_observed, front_expected)

        back_expected = [back_theoretical_prob * total_periods] * 12
        back_observed = [back_counts.get(i, 0) for i in range(1, 13)]
        back_chi2, back_p_value = stats.chisquare(back_observed, back_expected)

        prob_analysis = {
            "front_theoretical_prob": float(front_theoretical_prob),
            "back_theoretical_prob": float(back_theoretical_prob),
            "front_actual_probs": {k: float(v) for k, v in front_actual_probs.items()},
            "back_actual_probs": {k: float(v) for k, v in back_actual_probs.items()},
            "front_chi2_test": {
                "chi2": float(front_chi2),
                "p_value": float(front_p_value),
                "is_random": bool(front_p_value > 0.05)
            },
            "back_chi2_test": {
                "chi2": float(back_chi2),
                "p_value": float(back_p_value),
                "is_random": bool(back_p_value > 0.05)
            }
        }

        if save_result:
            with open(f"{self.output_dir}/probability_distribution.json", 'w', encoding='utf-8') as f:
                json.dump(prob_analysis, f, ensure_ascii=False, indent=2)

        return prob_analysis

    def analyze_bayesian(self, save_result=True):
        """è´å¶æ–¯åˆ†æ"""
        print("è¿›è¡Œè´å¶æ–¯åˆ†æ...")

        # è®¡ç®—å…ˆéªŒæ¦‚ç‡ï¼ˆåŸºäºå†å²é¢‘ç‡ï¼‰
        front_prior = {}
        back_prior = {}

        for ball in range(1, 36):
            count = sum(1 for front_list in self.front_balls_lists if ball in front_list)
            front_prior[ball] = count / len(self.front_balls_lists)

        for ball in range(1, 13):
            count = sum(1 for back_list in self.back_balls_lists if ball in back_list)
            back_prior[ball] = count / len(self.back_balls_lists)

        # è®¡ç®—æœ€è¿‘æœŸæ•°çš„ä¼¼ç„¶æ¦‚ç‡
        recent_periods = min(50, len(self.front_balls_lists))
        recent_front_lists = self.front_balls_lists[:recent_periods]
        recent_back_lists = self.back_balls_lists[:recent_periods]

        front_likelihood = {}
        back_likelihood = {}

        for ball in range(1, 36):
            count = sum(1 for front_list in recent_front_lists if ball in front_list)
            front_likelihood[ball] = count / recent_periods

        for ball in range(1, 13):
            count = sum(1 for back_list in recent_back_lists if ball in back_list)
            back_likelihood[ball] = count / recent_periods

        # è®¡ç®—åéªŒæ¦‚ç‡ï¼ˆç®€åŒ–çš„è´å¶æ–¯æ›´æ–°ï¼‰
        front_posterior = {}
        back_posterior = {}

        for ball in range(1, 36):
            # ç®€åŒ–çš„è´å¶æ–¯æ›´æ–°ï¼šposterior âˆ likelihood Ã— prior
            front_posterior[ball] = front_likelihood[ball] * front_prior[ball]

        for ball in range(1, 13):
            back_posterior[ball] = back_likelihood[ball] * back_prior[ball]

        # å½’ä¸€åŒ–åéªŒæ¦‚ç‡
        front_total = sum(front_posterior.values())
        if front_total > 0:
            front_posterior = {ball: prob / front_total for ball, prob in front_posterior.items()}

        back_total = sum(back_posterior.values())
        if back_total > 0:
            back_posterior = {ball: prob / back_total for ball, prob in back_posterior.items()}

        bayesian_analysis = {
            "front_prior": front_prior,
            "back_prior": back_prior,
            "front_likelihood": front_likelihood,
            "back_likelihood": back_likelihood,
            "front_posterior": front_posterior,
            "back_posterior": back_posterior,
            "recent_periods": recent_periods
        }

        if save_result:
            with open(f"{self.output_dir}/bayesian_analysis.json", 'w', encoding='utf-8') as f:
                json.dump(bayesian_analysis, f, ensure_ascii=False, indent=2)

        return bayesian_analysis

    def run_advanced_analysis(self):
        """è¿è¡Œæ‰€æœ‰é«˜çº§åˆ†æ"""
        print("å¼€å§‹é«˜çº§åˆ†æ...")

        results = {}

        # åˆ†æç»Ÿè®¡å­¦ç‰¹å¾
        stats_results = self.analyze_statistical_features()
        results["statistical_features"] = stats_results

        # åˆ†ææ¦‚ç‡åˆ†å¸ƒ
        prob_results = self.analyze_probability_distribution()
        results["probability_distribution"] = prob_results

        # åˆ†æé©¬å°”å¯å¤«é“¾
        markov_results = self.analyze_markov_chain()
        results["markov_chain"] = markov_results

        # è´å¶æ–¯åˆ†æ
        bayesian_results = self.analyze_bayesian()
        results["bayesian"] = bayesian_results

        print("é«˜çº§åˆ†æå®Œæˆ")
        return results


class ComprehensiveAnalyzer(BaseAnalyzer):
    """ç»¼åˆåˆ†æå™¨ï¼šæ•´åˆæ‰€æœ‰åˆ†æåŠŸèƒ½"""

    def __init__(self, data_file, output_dir="./output"):
        super().__init__(data_file, output_dir)

        # åˆå§‹åŒ–å­åˆ†æå™¨
        self.basic_analyzer = BasicAnalyzer(data_file, f"{output_dir}/basic")
        self.advanced_analyzer = AdvancedAnalyzer(data_file, f"{output_dir}/advanced")

    def run_all_analysis(self, periods=0, save_results=True):
        """è¿è¡Œæ‰€æœ‰åˆ†æ"""
        print("ğŸ” å¼€å§‹ç»¼åˆåˆ†æ...")

        # å¦‚æœæŒ‡å®šäº†æœŸæ•°ï¼Œæˆªå–æ•°æ®
        if periods > 0 and self.df is not None:
            original_df = self.df.copy()
            self.df = self.df.tail(periods)
            self.basic_analyzer.df = self.df
            self.advanced_analyzer.df = self.df

            # é‡æ–°è§£æå·ç 
            self.basic_analyzer._parse_ball_numbers()
            self.advanced_analyzer._parse_ball_numbers()

        all_results = {}

        # åŸºç¡€åˆ†æ
        print("ğŸ“Š æ‰§è¡ŒåŸºç¡€åˆ†æ...")
        basic_results = self.basic_analyzer.run_basic_analysis()
        all_results["basic"] = basic_results

        # é«˜çº§åˆ†æ
        print("ğŸ§  æ‰§è¡Œé«˜çº§åˆ†æ...")
        advanced_results = self.advanced_analyzer.run_advanced_analysis()
        all_results["advanced"] = advanced_results

        # ç»¼åˆåˆ†ææŠ¥å‘Š
        print("ğŸ“‹ ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")
        summary = self._generate_summary(basic_results, advanced_results)
        all_results["summary"] = summary

        if save_results:
            # ä¿å­˜ç»¼åˆç»“æœ
            with open(f"{self.output_dir}/comprehensive_analysis.json", 'w', encoding='utf-8') as f:
                json.dump(all_results, f, ensure_ascii=False, indent=2)

            # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
            self._generate_text_report(all_results)

        # æ¢å¤åŸå§‹æ•°æ®
        if periods > 0 and 'original_df' in locals():
            self.df = original_df
            self.basic_analyzer.df = original_df
            self.advanced_analyzer.df = original_df

        print("âœ… ç»¼åˆåˆ†æå®Œæˆ")
        return all_results

    def _generate_summary(self, basic_results, advanced_results):
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        summary = {
            "analysis_date": datetime.now().isoformat(),
            "data_overview": {
                "total_periods": len(self.df) if self.df is not None else 0,
                "date_range": f"{self.df['date'].min()} - {self.df['date'].max()}" if self.df is not None else "N/A",
                "issue_range": f"{self.df['issue'].min()} - {self.df['issue'].max()}" if self.df is not None else "N/A"
            }
        }

        # åŸºç¡€åˆ†ææ‘˜è¦
        if "frequency" in basic_results:
            front_freq = basic_results["frequency"]["front"]
            back_freq = basic_results["frequency"]["back"]

            # æœ€çƒ­å’Œæœ€å†·å·ç 
            front_hot = max(front_freq.items(), key=lambda x: x[1])
            front_cold = min(front_freq.items(), key=lambda x: x[1])
            back_hot = max(back_freq.items(), key=lambda x: x[1])
            back_cold = min(back_freq.items(), key=lambda x: x[1])

            summary["frequency_summary"] = {
                "front_hottest": {"number": front_hot[0], "count": front_hot[1]},
                "front_coldest": {"number": front_cold[0], "count": front_cold[1]},
                "back_hottest": {"number": back_hot[0], "count": back_hot[1]},
                "back_coldest": {"number": back_cold[0], "count": back_cold[1]}
            }

        # é«˜çº§åˆ†ææ‘˜è¦
        if "probability_distribution" in advanced_results:
            prob_dist = advanced_results["probability_distribution"]
            summary["randomness_test"] = {
                "front_is_random": prob_dist["front_chi2_test"]["is_random"],
                "back_is_random": prob_dist["back_chi2_test"]["is_random"],
                "front_p_value": prob_dist["front_chi2_test"]["p_value"],
                "back_p_value": prob_dist["back_chi2_test"]["p_value"]
            }

        return summary

    def _generate_text_report(self, all_results):
        """ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š"""
        report_path = f"{self.output_dir}/analysis_report.txt"

        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("å¤§ä¹é€æ•°æ®ç»¼åˆåˆ†ææŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")

            # æ•°æ®æ¦‚è§ˆ
            summary = all_results.get("summary", {})
            data_overview = summary.get("data_overview", {})

            f.write("æ•°æ®æ¦‚è§ˆ:\n")
            f.write(f"  æ€»æœŸæ•°: {data_overview.get('total_periods', 'N/A')}\n")
            f.write(f"  æ—¥æœŸèŒƒå›´: {data_overview.get('date_range', 'N/A')}\n")
            f.write(f"  æœŸå·èŒƒå›´: {data_overview.get('issue_range', 'N/A')}\n")
            f.write(f"  åˆ†ææ—¶é—´: {summary.get('analysis_date', 'N/A')}\n\n")

            # é¢‘ç‡åˆ†ææ‘˜è¦
            freq_summary = summary.get("frequency_summary", {})
            if freq_summary:
                f.write("é¢‘ç‡åˆ†ææ‘˜è¦:\n")
                f.write(f"  å‰åŒºæœ€çƒ­å·ç : {freq_summary.get('front_hottest', {}).get('number', 'N/A')}å· ")
                f.write(f"({freq_summary.get('front_hottest', {}).get('count', 'N/A')}æ¬¡)\n")
                f.write(f"  å‰åŒºæœ€å†·å·ç : {freq_summary.get('front_coldest', {}).get('number', 'N/A')}å· ")
                f.write(f"({freq_summary.get('front_coldest', {}).get('count', 'N/A')}æ¬¡)\n")
                f.write(f"  ååŒºæœ€çƒ­å·ç : {freq_summary.get('back_hottest', {}).get('number', 'N/A')}å· ")
                f.write(f"({freq_summary.get('back_hottest', {}).get('count', 'N/A')}æ¬¡)\n")
                f.write(f"  ååŒºæœ€å†·å·ç : {freq_summary.get('back_coldest', {}).get('number', 'N/A')}å· ")
                f.write(f"({freq_summary.get('back_coldest', {}).get('count', 'N/A')}æ¬¡)\n\n")

            # éšæœºæ€§æ£€éªŒ
            randomness = summary.get("randomness_test", {})
            if randomness:
                f.write("éšæœºæ€§æ£€éªŒ:\n")
                f.write(f"  å‰åŒºéšæœºæ€§: {'é€šè¿‡' if randomness.get('front_is_random', False) else 'ä¸é€šè¿‡'}\n")
                f.write(f"  ååŒºéšæœºæ€§: {'é€šè¿‡' if randomness.get('back_is_random', False) else 'ä¸é€šè¿‡'}\n")
                f.write(f"  å‰åŒºPå€¼: {randomness.get('front_p_value', 'N/A'):.4f}\n")
                f.write(f"  ååŒºPå€¼: {randomness.get('back_p_value', 'N/A'):.4f}\n\n")

            f.write("è¯¦ç»†åˆ†æç»“æœè¯·æŸ¥çœ‹å¯¹åº”çš„JSONæ–‡ä»¶ã€‚\n")

        print(f"æ–‡æœ¬æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="å¤§ä¹é€åˆ†æå™¨é›†åˆ")
    parser.add_argument("-d", "--data", default="data/dlt_data_all.csv", help="æ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("-o", "--output", default="output", help="è¾“å‡ºç›®å½•")
    parser.add_argument("-m", "--method",
                       choices=['basic', 'advanced', 'comprehensive'],
                       default='comprehensive', help="åˆ†ææ–¹æ³•")
    parser.add_argument("-p", "--periods", type=int, default=0, help="åˆ†ææœŸæ•°ï¼Œ0è¡¨ç¤ºå…¨éƒ¨")

    args = parser.parse_args()

    if not os.path.exists(args.data):
        print(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.data}")
        return

    # æ ¹æ®é€‰æ‹©çš„æ–¹æ³•è¿›è¡Œåˆ†æ
    if args.method == 'basic':
        analyzer = BasicAnalyzer(args.data, f"{args.output}/basic")
        results = analyzer.run_basic_analysis()
        print(f"åŸºç¡€åˆ†æå®Œæˆï¼Œç»“æœä¿å­˜åœ¨: {args.output}/basic")

    elif args.method == 'advanced':
        analyzer = AdvancedAnalyzer(args.data, f"{args.output}/advanced")
        results = analyzer.run_advanced_analysis()
        print(f"é«˜çº§åˆ†æå®Œæˆï¼Œç»“æœä¿å­˜åœ¨: {args.output}/advanced")

    elif args.method == 'comprehensive':
        analyzer = ComprehensiveAnalyzer(args.data, args.output)
        results = analyzer.run_all_analysis(periods=args.periods)
        print(f"ç»¼åˆåˆ†æå®Œæˆï¼Œç»“æœä¿å­˜åœ¨: {args.output}")


if __name__ == "__main__":
    main()
