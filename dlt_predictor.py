#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¤§ä¹é€æ™ºèƒ½é¢„æµ‹ç³»ç»Ÿ - ä¸»ç¨‹åº
æ•´åˆæ‰€æœ‰åŠŸèƒ½ï¼šé¢„æµ‹ã€åˆ†æã€æ•°æ®ç®¡ç†ã€å¯è§†åŒ–
"""

import os
import sys
import argparse
import pandas as pd
import numpy as np
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥æ•´åˆåçš„æ¨¡å—
try:
    from predictors import TraditionalPredictor, AdvancedPredictor, SuperPredictor, CompoundPredictor
    PREDICTORS_AVAILABLE = True
except ImportError:
    PREDICTORS_AVAILABLE = False
    print("é¢„æµ‹å™¨æ¨¡å—ä¸å¯ç”¨")

try:
    from analyzers import BasicAnalyzer, AdvancedAnalyzer, ComprehensiveAnalyzer
    ANALYZERS_AVAILABLE = True
except ImportError:
    ANALYZERS_AVAILABLE = False
    print("åˆ†æå™¨æ¨¡å—ä¸å¯ç”¨")

try:
    from data_manager import DataCrawler, DataProcessor, DataUtils
    DATA_MANAGER_AVAILABLE = True
except ImportError:
    DATA_MANAGER_AVAILABLE = False
    print("æ•°æ®ç®¡ç†å™¨æ¨¡å—ä¸å¯ç”¨")

try:
    from visualization import VisualizationTool
    VISUALIZATION_AVAILABLE = True
except ImportError:
    VISUALIZATION_AVAILABLE = False
    print("å¯è§†åŒ–æ¨¡å—ä¸å¯ç”¨")


class DLTPredictor:
    """å¤§ä¹é€æ™ºèƒ½é¢„æµ‹ç³»ç»Ÿä¸»ç±»"""
    
    def __init__(self, data_file="data/dlt_data_all.csv"):
        self.data_file = data_file
        self.check_data_file()
    
    def check_data_file(self):
        """æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        if not os.path.exists(self.data_file):
            print(f"æ•°æ®æ–‡ä»¶ {self.data_file} ä¸å­˜åœ¨")
            if DATA_MANAGER_AVAILABLE:
                print("å°è¯•è‡ªåŠ¨è·å–æ•°æ®...")
                self.auto_fetch_data()
            else:
                print("æ•°æ®ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œæ— æ³•è‡ªåŠ¨è·å–æ•°æ®")
                return False
        return True
    
    def auto_fetch_data(self):
        """è‡ªåŠ¨è·å–æ•°æ®"""
        try:
            data_dir = os.path.dirname(self.data_file)
            if not os.path.exists(data_dir):
                os.makedirs(data_dir)
            
            crawler = DataCrawler()
            results = crawler.get_history_data(count=300)
            if results:
                crawler.save_to_csv(results, os.path.basename(self.data_file))
                print("æ•°æ®è·å–æˆåŠŸ")
                return True
            else:
                print("æ•°æ®è·å–å¤±è´¥")
                return False
        except Exception as e:
            print(f"è‡ªåŠ¨è·å–æ•°æ®å¤±è´¥: {e}")
            return False
    
    def check_system_status(self):
        """æ£€æŸ¥ç³»ç»ŸçŠ¶æ€"""
        print("ğŸ”§ ç³»ç»ŸçŠ¶æ€æ£€æŸ¥")
        print("=" * 50)
        
        # æ£€æŸ¥æ¨¡å—çŠ¶æ€
        print("ğŸ“¦ é¢„æµ‹æ¨¡å—çŠ¶æ€:")
        print(f"  é¢„æµ‹å™¨æ¨¡å—: {'âœ… å¯ç”¨' if PREDICTORS_AVAILABLE else 'âŒ ä¸å¯ç”¨'}")
        print(f"  åˆ†æå™¨æ¨¡å—: {'âœ… å¯ç”¨' if ANALYZERS_AVAILABLE else 'âŒ ä¸å¯ç”¨'}")
        print(f"  æ•°æ®ç®¡ç†å™¨: {'âœ… å¯ç”¨' if DATA_MANAGER_AVAILABLE else 'âŒ ä¸å¯ç”¨'}")
        print(f"  å¯è§†åŒ–å·¥å…·: {'âœ… å¯ç”¨' if VISUALIZATION_AVAILABLE else 'âŒ ä¸å¯ç”¨'}")
        
        # æ£€æŸ¥æ•°æ®çŠ¶æ€
        print("\nğŸ“Š æ•°æ®çŠ¶æ€:")
        if os.path.exists(self.data_file):
            try:
                df = pd.read_csv(self.data_file)
                print(f"  æ•°æ®æ–‡ä»¶: âœ… {self.data_file}")
                print(f"  è®°å½•æ•°é‡: {len(df)} æ¡")
                print(f"  æ•°æ®èŒƒå›´: {df['issue'].min()} - {df['issue'].max()}")
            except Exception as e:
                print(f"  æ•°æ®æ–‡ä»¶: âŒ è¯»å–å¤±è´¥ - {e}")
        else:
            print(f"  æ•°æ®æ–‡ä»¶: âŒ ä¸å­˜åœ¨")
        
        # æ£€æŸ¥å¯ç”¨åŠŸèƒ½
        print("\nğŸ® å¯ç”¨åŠŸèƒ½:")
        if PREDICTORS_AVAILABLE:
            print("  âœ… ä¼ ç»Ÿé›†æˆ    âœ… æ··åˆåˆ†æ      âœ… é©¬å°”å¯å¤«é“¾")
            print("  âœ… ç»Ÿè®¡åˆ†æ    âœ… é«˜çº§é›†æˆ      âœ… LSTMæ·±åº¦å­¦ä¹ ")
            print("  âœ… é›†æˆå­¦ä¹     âœ… è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ  âœ… èšç±»åˆ†æ")
            print("  âœ… ç»ˆæé›†æˆ    âœ… å¤å¼æŠ•æ³¨é¢„æµ‹")
        else:
            print("  âŒ é¢„æµ‹åŠŸèƒ½ä¸å¯ç”¨")
        
        if ANALYZERS_AVAILABLE:
            print("  âœ… åŸºç¡€åˆ†æ    âœ… é«˜çº§åˆ†æ      âœ… ç»¼åˆåˆ†æ")
        else:
            print("  âŒ åˆ†æåŠŸèƒ½ä¸å¯ç”¨")
        
        if DATA_MANAGER_AVAILABLE:
            print("  âœ… æ•°æ®çˆ¬å–    âœ… æ•°æ®å¤„ç†      âœ… æ•°æ®éªŒè¯")
        else:
            print("  âŒ æ•°æ®ç®¡ç†åŠŸèƒ½ä¸å¯ç”¨")
        
        if VISUALIZATION_AVAILABLE:
            print("  âœ… å¯è§†åŒ–å›¾è¡¨  âœ… ç»Ÿè®¡æŠ¥å‘Š")
        else:
            print("  âŒ å¯è§†åŒ–åŠŸèƒ½ä¸å¯ç”¨")
    
    def predict_compound(self, combinations=None, method="hybrid", periods=150):
        """å¤å¼æŠ•æ³¨é¢„æµ‹"""
        if not PREDICTORS_AVAILABLE:
            print("é¢„æµ‹å™¨æ¨¡å—ä¸å¯ç”¨")
            return {}

        if not os.path.exists(self.data_file):
            print("æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œé¢„æµ‹")
            return {}

        try:
            predictor = CompoundPredictor(self.data_file)

            if combinations is None:
                combinations = [(6, 2), (7, 3)]  # é»˜è®¤ç»„åˆ

            # å¦‚æœä¼ å…¥çš„æ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼Œè½¬æ¢ä¸ºå…ƒç»„æ ¼å¼
            if isinstance(combinations[0], str):
                combo_tuples = []
                for combo in combinations:
                    front_count, back_count = map(int, combo.split('+'))
                    combo_tuples.append((front_count, back_count))
                combinations = combo_tuples

            print("ğŸ° å¤å¼æŠ•æ³¨é¢„æµ‹")
            print("=" * 50)
            print(f"ğŸ“‹ ä½¿ç”¨æ–¹æ³•: {method}")
            print(f"ğŸ“Š åˆ†ææœŸæ•°: {periods}")
            print()

            results = predictor.predict_compound_combinations(
                periods=periods,
                combinations=combinations,
                method=method,
                explain=True
            )

            return results

        except Exception as e:
            print(f"å¤å¼é¢„æµ‹å¤±è´¥: {e}")
            return {}

    def predict(self, method="ultimate_ensemble", count=1):
        """é¢„æµ‹å·ç """
        if not PREDICTORS_AVAILABLE:
            print("é¢„æµ‹å™¨æ¨¡å—ä¸å¯ç”¨")
            return []
        
        if not os.path.exists(self.data_file):
            print("æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œé¢„æµ‹")
            return []
        
        try:
            if method == "quick":
                predictor = SuperPredictor(self.data_file)
                predictions = predictor.quick_predict(count=count)
                
                print("âš¡ å¿«é€Ÿé¢„æµ‹æ¨¡å¼")
                print("=" * 50)
                print("\nğŸ¯ å¿«é€Ÿé¢„æµ‹ç»“æœï¼ˆæ··åˆåˆ†æï¼‰:")
                
            elif method == "traditional_ensemble":
                predictor = TraditionalPredictor(self.data_file)
                predictions = predictor.ensemble_predict(count=count)
                
                print("ğŸ”¬ ä¼ ç»Ÿæ–¹æ³•é›†æˆé¢„æµ‹")
                print("=" * 50)
                print("ğŸ“‹ ä½¿ç”¨æ–¹æ³•: æ··åˆåˆ†æ + é©¬å°”å¯å¤«é“¾ + ç»Ÿè®¡åˆ†æ")
                print("\nâœ… ä¼ ç»Ÿæ–¹æ³•é¢„æµ‹å®Œæˆ")
                print("\nğŸ† ä¼ ç»Ÿé›†æˆé¢„æµ‹ç»“æœ:")
                
            elif method == "advanced_ensemble":
                predictor = AdvancedPredictor(self.data_file)
                predictions = predictor.ensemble_predict(count=count)
                
                print("ğŸ§  é«˜çº§æ–¹æ³•é›†æˆé¢„æµ‹")
                print("=" * 50)
                print("ğŸ“‹ ä½¿ç”¨æ–¹æ³•: LSTM + é›†æˆå­¦ä¹  + è’™ç‰¹å¡æ´› + èšç±»åˆ†æ")
                print("\nâœ… é«˜çº§æ–¹æ³•é¢„æµ‹å®Œæˆ")
                print("\nğŸ† é«˜çº§é›†æˆé¢„æµ‹ç»“æœ:")
                
            elif method == "ultimate_ensemble":
                predictor = SuperPredictor(self.data_file)
                predictions = predictor.ultimate_ensemble_predict(count=count)
                
                print("ğŸŒŸ ç»ˆæé›†æˆé¢„æµ‹")
                print("=" * 50)
                print("ğŸ“‹ èåˆæ–¹æ³•: ä¼ ç»Ÿé¢„æµ‹ + é«˜çº§é¢„æµ‹")
                print("\nâœ… ä¼ ç»Ÿæ–¹æ³•é¢„æµ‹å®Œæˆ")
                print("âœ… é«˜çº§æ–¹æ³•é¢„æµ‹å®Œæˆ")
                print("\nğŸ† ç»ˆæé›†æˆé¢„æµ‹ç»“æœ:")
                
            elif method == "compare":
                predictor = SuperPredictor(self.data_file)
                results = predictor.compare_all_methods(count=1)
                
                print("ğŸ“Š å…¨æ–¹æ³•é¢„æµ‹å¯¹æ¯”")
                print("=" * 80)
                print("\nğŸ”¬ ä¼ ç»Ÿé¢„æµ‹æ–¹æ³•:")
                print("æ–¹æ³•              æƒé‡       çŠ¶æ€       ç¬¬1æ³¨é¢„æµ‹")
                print("-" * 70)
                
                traditional_methods = ['hybrid', 'markov', 'statistical']
                traditional_weights = {'hybrid': 50.0, 'markov': 30.0, 'statistical': 20.0}
                
                for method_name in traditional_methods:
                    if method_name in results and results[method_name]:
                        front, back = results[method_name][0]
                        front_str = ' '.join([str(b).zfill(2) for b in front])
                        back_str = ' '.join([str(b).zfill(2) for b in back])
                        weight = traditional_weights.get(method_name.upper(), 0)
                        print(f"{method_name.upper():<12} {weight:>6.1f}%    âœ…å¯ç”¨      å‰åŒº {front_str} | ååŒº {back_str}")
                    else:
                        print(f"{method_name.upper():<12} {'0.0':>6}%    âŒä¸å¯ç”¨    é¢„æµ‹å¤±è´¥")
                
                print("\nğŸ§  é«˜çº§é¢„æµ‹æ–¹æ³•:")
                advanced_methods = ['lstm', 'ensemble_ml', 'monte_carlo', 'clustering']
                for method_name in advanced_methods:
                    if method_name in results and results[method_name]:
                        front, back = results[method_name][0]
                        front_str = ' '.join([str(b).zfill(2) for b in front])
                        back_str = ' '.join([str(b).zfill(2) for b in back])
                        print(f"{method_name.upper()}é¢„æµ‹:")
                        print(f"  ç¬¬ 1 æ³¨: å‰åŒº {front_str} | ååŒº {back_str}")
                    else:
                        print(f"{method_name.upper()}é¢„æµ‹: é¢„æµ‹å¤±è´¥")
                
                if 'ultimate_ensemble' in results and results['ultimate_ensemble']:
                    front, back = results['ultimate_ensemble'][0]
                    front_str = ' '.join([str(b).zfill(2) for b in front])
                    back_str = ' '.join([str(b).zfill(2) for b in back])
                    print(f"\nğŸŒŸ ç»ˆæé›†æˆé¢„æµ‹:")
                    print(f"ç¬¬ 1 æ³¨: å‰åŒº {front_str} | ååŒº {back_str}")
                
                return results
            
            else:
                print(f"æœªçŸ¥çš„é¢„æµ‹æ–¹æ³•: {method}")
                return []
            
            # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
            for i, (front, back) in enumerate(predictions, 1):
                front_str = ' '.join([str(b).zfill(2) for b in front])
                back_str = ' '.join([str(b).zfill(2) for b in back])
                print(f"ç¬¬ {i} æ³¨: å‰åŒº {front_str} | ååŒº {back_str}")
            
            if method == "ultimate_ensemble":
                print(f"\nğŸ’¡ æƒé‡é…ç½®:")
                print(f"   ä¼ ç»Ÿæ–¹æ³•: 60%")
                print(f"   é«˜çº§æ–¹æ³•: 40%")
            
            return predictions
            
        except Exception as e:
            print(f"é¢„æµ‹å¤±è´¥: {e}")
            return []
    
    def analyze(self, method="comprehensive", periods=0):
        """æ•°æ®åˆ†æ"""
        if not ANALYZERS_AVAILABLE:
            print("åˆ†æå™¨æ¨¡å—ä¸å¯ç”¨")
            return {}
        
        if not os.path.exists(self.data_file):
            print("æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
            return {}
        
        try:
            if method == "basic":
                analyzer = BasicAnalyzer(self.data_file)
                results = analyzer.run_basic_analysis()
                print("åŸºç¡€åˆ†æå®Œæˆ")
            elif method == "advanced":
                analyzer = AdvancedAnalyzer(self.data_file)
                results = analyzer.run_advanced_analysis()
                print("é«˜çº§åˆ†æå®Œæˆ")
            elif method == "comprehensive":
                analyzer = ComprehensiveAnalyzer(self.data_file)
                results = analyzer.run_all_analysis(periods=periods)
                print("ç»¼åˆåˆ†æå®Œæˆ")
            else:
                print(f"æœªçŸ¥çš„åˆ†ææ–¹æ³•: {method}")
                return {}
            
            return results
            
        except Exception as e:
            print(f"åˆ†æå¤±è´¥: {e}")
            return {}
    
    def generate_numbers(self, count=5, strategy="hybrid"):
        """ç”Ÿæˆå·ç """
        if not PREDICTORS_AVAILABLE:
            print("é¢„æµ‹å™¨æ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨éšæœºç”Ÿæˆ")
            for i in range(count):
                front, back = DataUtils.generate_random_numbers()
                print(f"[{i+1}] {DataUtils.format_numbers(front, back)}")
            return
        
        if strategy == "random":
            for i in range(count):
                front, back = DataUtils.generate_random_numbers()
                print(f"[{i+1}] {DataUtils.format_numbers(front, back)}")
        else:
            predictions = self.predict(method="quick", count=count)
            for i, (front, back) in enumerate(predictions, 1):
                print(f"[{i}] {DataUtils.format_numbers(front, back)}")
    
    def get_latest_result(self, compare=False):
        """è·å–æœ€æ–°å¼€å¥–ç»“æœ"""
        if not os.path.exists(self.data_file):
            print("æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
            return
        
        try:
            df = pd.read_csv(self.data_file)
            if len(df) == 0:
                print("æ•°æ®æ–‡ä»¶ä¸ºç©º")
                return
            
            # è·å–æœ€æ–°ä¸€æœŸæ•°æ®
            latest_row = df.iloc[0]  # å‡è®¾æ•°æ®æŒ‰æœŸå·é™åºæ’åˆ—
            
            issue = latest_row['issue']
            date = latest_row['date']
            front_balls = latest_row['front_balls']
            back_balls = latest_row['back_balls']
            
            print(f"\næœ€æ–°å¼€å¥–ç»“æœ (æœŸå·: {issue}, æ—¥æœŸ: {date})")
            print(f"å‰åŒºå·ç : {front_balls}")
            print(f"ååŒºå·ç : {back_balls}")
            
            # å¦‚æœéœ€è¦æ¯”å¯¹
            if compare:
                self._compare_with_latest(front_balls, back_balls)
                
        except Exception as e:
            print(f"è·å–æœ€æ–°å¼€å¥–ç»“æœå¤±è´¥: {e}")
    
    def _compare_with_latest(self, front_balls_latest, back_balls_latest):
        """å°†ç”¨æˆ·è¾“å…¥çš„å·ç ä¸æœ€æ–°å¼€å¥–ç»“æœè¿›è¡Œæ¯”å¯¹"""
        try:
            print("\nè¯·è¾“å…¥æ‚¨çš„å·ç è¿›è¡Œæ¯”å¯¹:")
            
            # è¾“å…¥å‰åŒºå·ç 
            front_input = input("è¯·è¾“å…¥å‰åŒº5ä¸ªå·ç ï¼ˆç”¨ç©ºæ ¼åˆ†éš”ï¼‰: ")
            front_balls = [int(x.strip()) for x in front_input.split()]
            
            if len(front_balls) != 5:
                print("å‰åŒºå·ç å¿…é¡»æ˜¯5ä¸ª")
                return
            
            # è¾“å…¥ååŒºå·ç 
            back_input = input("è¯·è¾“å…¥ååŒº2ä¸ªå·ç ï¼ˆç”¨ç©ºæ ¼åˆ†éš”ï¼‰: ")
            back_balls = [int(x.strip()) for x in back_input.split()]
            
            if len(back_balls) != 2:
                print("ååŒºå·ç å¿…é¡»æ˜¯2ä¸ª")
                return
            
            # è§£ææœ€æ–°å¼€å¥–å·ç 
            latest_front = [int(x.strip()) for x in front_balls_latest.split(',')]
            latest_back = [int(x.strip()) for x in back_balls_latest.split(',')]
            
            # æ’åº
            front_balls.sort()
            back_balls.sort()
            
            # æ¯”å¯¹
            front_match = len(set(front_balls) & set(latest_front))
            back_match = len(set(back_balls) & set(latest_back))
            
            print(f"\næ‚¨çš„å·ç : å‰åŒº {front_balls}, ååŒº {back_balls}")
            print(f"å¼€å¥–å·ç : å‰åŒº {latest_front}, ååŒº {latest_back}")
            print(f"åŒ¹é…ç»“æœ: å‰åŒºåŒ¹é… {front_match} ä¸ª, ååŒºåŒ¹é… {back_match} ä¸ª")
            
            # åˆ¤æ–­ä¸­å¥–ç­‰çº§
            prize_level = DataUtils.calculate_prize(front_balls, back_balls, latest_front, latest_back)
            if prize_level > 0:
                print(f"æ­å–œæ‚¨ä¸­å¾— {DataUtils.get_prize_name(prize_level)}ï¼")
            else:
                print("å¾ˆé—æ†¾ï¼Œæ‚¨æœªä¸­å¥–")
                
        except Exception as e:
            print(f"æ¯”å¯¹å¤±è´¥: {e}")
    
    def crawl_data(self, count=100, get_all=False, update=False):
        """çˆ¬å–æ•°æ®"""
        if not DATA_MANAGER_AVAILABLE:
            print("æ•°æ®ç®¡ç†å™¨æ¨¡å—ä¸å¯ç”¨")
            return
        
        try:
            crawler = DataCrawler()
            
            if update:
                crawler.update_data(self.data_file, count)
            else:
                if get_all:
                    results = crawler.get_history_data(get_all=True)
                else:
                    results = crawler.get_history_data(count=count)
                
                if results:
                    crawler.save_to_csv(results, os.path.basename(self.data_file))
                    print(f"æ•°æ®çˆ¬å–å®Œæˆï¼Œä¿å­˜åˆ° {self.data_file}")
                else:
                    print("æœªè·å–åˆ°æ•°æ®")
        except Exception as e:
            print(f"çˆ¬å–æ•°æ®å¤±è´¥: {e}")
    
    def visualize(self, output_dir="output/visualization"):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        if not VISUALIZATION_AVAILABLE:
            print("å¯è§†åŒ–æ¨¡å—ä¸å¯ç”¨")
            return
        
        if not os.path.exists(self.data_file):
            print("æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
            return
        
        try:
            viz = VisualizationTool(self.data_file)
            viz.generate_all_charts(output_dir)
            print(f"å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆåˆ°: {output_dir}")
        except Exception as e:
            print(f"ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¤§ä¹é€æ™ºèƒ½é¢„æµ‹ç³»ç»Ÿ")
    subparsers = parser.add_subparsers(dest="command", help="å­å‘½ä»¤")
    
    # é¢„æµ‹å­å‘½ä»¤
    predict_parser = subparsers.add_parser("predict", help="é¢„æµ‹å·ç ")
    predict_parser.add_argument("-m", "--method",
                               choices=['quick', 'traditional_ensemble', 'advanced_ensemble', 'ultimate_ensemble', 'compare'],
                               default='ultimate_ensemble', help="é¢„æµ‹æ–¹æ³•")
    predict_parser.add_argument("-n", "--num", type=int, default=1, help="é¢„æµ‹æ³¨æ•°")
    predict_parser.add_argument("-d", "--data", default="data/dlt_data_all.csv", help="æ•°æ®æ–‡ä»¶è·¯å¾„")

    # å¤å¼é¢„æµ‹å­å‘½ä»¤
    compound_parser = subparsers.add_parser("compound", help="å¤å¼æŠ•æ³¨é¢„æµ‹")
    compound_parser.add_argument("-c", "--combinations", nargs='+', default=["6+2", "7+3"],
                                help="å¤å¼ç»„åˆï¼Œæ ¼å¼ï¼š6+2 7+3 è¡¨ç¤º6+2å’Œ7+3ä¸¤æ³¨")
    compound_parser.add_argument("-m", "--method", choices=['hybrid', 'markov', 'advanced'],
                                default='hybrid', help="é¢„æµ‹æ–¹æ³•")
    compound_parser.add_argument("-p", "--periods", type=int, default=150, help="åˆ†ææœŸæ•°")
    compound_parser.add_argument("-d", "--data", default="data/dlt_data_all.csv", help="æ•°æ®æ–‡ä»¶è·¯å¾„")
    
    # åˆ†æå­å‘½ä»¤
    analyze_parser = subparsers.add_parser("analyze", help="åˆ†ææ•°æ®")
    analyze_parser.add_argument("-m", "--method", choices=['basic', 'advanced', 'comprehensive'], 
                               default='comprehensive', help="åˆ†ææ–¹æ³•")
    analyze_parser.add_argument("-p", "--periods", type=int, default=0, help="åˆ†ææœŸæ•°ï¼Œ0è¡¨ç¤ºå…¨éƒ¨")
    analyze_parser.add_argument("-d", "--data", default="data/dlt_data_all.csv", help="æ•°æ®æ–‡ä»¶è·¯å¾„")
    
    # ç”Ÿæˆå·ç å­å‘½ä»¤
    generate_parser = subparsers.add_parser("generate", help="ç”Ÿæˆå·ç ")
    generate_parser.add_argument("-c", "--count", type=int, default=5, help="ç”Ÿæˆå·ç æ³¨æ•°")
    generate_parser.add_argument("-s", "--strategy", choices=["random", "hybrid"], default="hybrid", help="ç”Ÿæˆç­–ç•¥")
    generate_parser.add_argument("-d", "--data", default="data/dlt_data_all.csv", help="æ•°æ®æ–‡ä»¶è·¯å¾„")
    
    # æœ€æ–°å¼€å¥–å­å‘½ä»¤
    latest_parser = subparsers.add_parser("latest", help="æ˜¾ç¤ºæœ€æ–°å¼€å¥–ç»“æœ")
    latest_parser.add_argument("-c", "--compare", action="store_true", help="ä¸è‡ªé€‰å·ç æ¯”å¯¹")
    latest_parser.add_argument("-d", "--data", default="data/dlt_data_all.csv", help="æ•°æ®æ–‡ä»¶è·¯å¾„")
    
    # çˆ¬è™«å­å‘½ä»¤
    crawl_parser = subparsers.add_parser("crawl", help="çˆ¬å–æ•°æ®")
    crawl_parser.add_argument("-c", "--count", type=int, default=100, help="çˆ¬å–æœŸæ•°")
    crawl_parser.add_argument("-a", "--all", action="store_true", help="çˆ¬å–æ‰€æœ‰å†å²æ•°æ®")
    crawl_parser.add_argument("-u", "--update", action="store_true", help="æ›´æ–°ç°æœ‰æ•°æ®")
    crawl_parser.add_argument("-d", "--data", default="data/dlt_data_all.csv", help="æ•°æ®æ–‡ä»¶è·¯å¾„")
    
    # å¯è§†åŒ–å­å‘½ä»¤
    viz_parser = subparsers.add_parser("visualize", help="ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
    viz_parser.add_argument("-o", "--output", default="output/visualization", help="è¾“å‡ºç›®å½•")
    viz_parser.add_argument("-d", "--data", default="data/dlt_data_all.csv", help="æ•°æ®æ–‡ä»¶è·¯å¾„")
    
    # çŠ¶æ€æ£€æŸ¥å­å‘½ä»¤
    status_parser = subparsers.add_parser("status", help="æ£€æŸ¥ç³»ç»ŸçŠ¶æ€")
    status_parser.add_argument("-d", "--data", default="data/dlt_data_all.csv", help="æ•°æ®æ–‡ä»¶è·¯å¾„")
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()
    
    # å¦‚æœæ²¡æœ‰å­å‘½ä»¤ï¼Œæ˜¾ç¤ºå¸®åŠ©
    if not args.command:
        parser.print_help()
        return
    
    # åˆ›å»ºé¢„æµ‹å™¨å®ä¾‹
    data_file = getattr(args, 'data', 'data/dlt_data_all.csv')
    predictor = DLTPredictor(data_file)
    
    # æ ¹æ®å­å‘½ä»¤æ‰§è¡Œç›¸åº”çš„åŠŸèƒ½
    if args.command == "predict":
        predictor.predict(method=args.method, count=args.num)
    elif args.command == "compound":
        predictor.predict_compound(combinations=args.combinations, method=args.method, periods=args.periods)
    elif args.command == "analyze":
        predictor.analyze(method=args.method, periods=args.periods)
    elif args.command == "generate":
        predictor.generate_numbers(count=args.count, strategy=args.strategy)
    elif args.command == "latest":
        predictor.get_latest_result(compare=args.compare)
    elif args.command == "crawl":
        predictor.crawl_data(count=args.count, get_all=args.all, update=args.update)
    elif args.command == "visualize":
        predictor.visualize(output_dir=args.output)
    elif args.command == "status":
        predictor.check_system_status()


if __name__ == "__main__":
    main()
