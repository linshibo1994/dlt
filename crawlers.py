#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ•°æ®çˆ¬è™«æ¨¡å—
æ”¯æŒä»ä¸­å½©ç½‘ã€500å½©ç¥¨ç½‘ç­‰æ•°æ®æºçˆ¬å–å¤§ä¹é€å†å²å¼€å¥–æ•°æ®
"""

import os
import re
import time
import requests
import pandas as pd
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from typing import List, Dict, Optional

from core_modules import logger_manager, data_manager


class BaseCrawler:
    """çˆ¬è™«åŸºç±»"""
    
    def __init__(self, data_file="data/dlt_data_all.csv"):
        self.data_file = data_file
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(data_file), exist_ok=True)
    
    def save_data(self, data: List[Dict]):
        """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶ï¼ˆæŒ‰æœŸå·å€’åºæ’åˆ—ï¼Œæœ€æ–°æœŸåœ¨å‰ï¼‰"""
        if not data:
            logger_manager.warning("æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
            return

        df = pd.DataFrame(data)

        # ç¡®ä¿æœŸå·æ˜¯æ•´æ•°ç±»å‹ç”¨äºæ­£ç¡®æ’åº
        df['issue'] = df['issue'].astype(int)

        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆå¹¶æ•°æ®
        if os.path.exists(self.data_file):
            existing_df = pd.read_csv(self.data_file)
            existing_df['issue'] = existing_df['issue'].astype(int)

            # åˆå¹¶æ•°æ®å¹¶å»é‡ï¼Œä¿ç•™æœ€æ–°æ•°æ®
            combined_df = pd.concat([existing_df, df]).drop_duplicates(subset=['issue'], keep='last')
        else:
            combined_df = df

        # æŒ‰æœŸå·å€’åºæ’åˆ—ï¼ˆæœ€æ–°æœŸåœ¨å‰ï¼‰
        combined_df = combined_df.sort_values('issue', ascending=False).reset_index(drop=True)

        combined_df.to_csv(self.data_file, index=False, encoding='utf-8')
        logger_manager.info(f"æ•°æ®å·²ä¿å­˜åˆ° {self.data_file}ï¼Œå…± {len(combined_df)} æœŸ")
    
    def get_latest_issue(self) -> Optional[int]:
        """è·å–æœ¬åœ°æ•°æ®ä¸­çš„æœ€æ–°æœŸå·"""
        if os.path.exists(self.data_file):
            try:
                df = pd.read_csv(self.data_file)
                if not df.empty:
                    # æ•°æ®æŒ‰æœŸå·å€’åºæ’åˆ—ï¼Œç¬¬ä¸€è¡Œå°±æ˜¯æœ€æ–°æœŸ
                    return int(df.iloc[0]['issue'])
            except Exception as e:
                logger_manager.error("è¯»å–æœ¬åœ°æ•°æ®å¤±è´¥", e)
        return None

    def crawl_recent_data(self, max_pages=3):
        """çˆ¬å–æœ€è¿‘çš„æ•°æ®ï¼ˆå¢é‡æ›´æ–°ï¼‰"""
        logger_manager.info(f"å¼€å§‹å¢é‡æ›´æ–°ï¼Œæœ€å¤§é¡µæ•°: {max_pages}")

        all_data = []
        latest_local_issue = self.get_latest_issue()

        # è·å–æœ€æ–°çš„è¿œç¨‹æ•°æ®æ¥ç¡®å®šéœ€è¦æ›´æ–°çš„æœŸæ•°
        first_page_data = self.crawl_page(1)
        if not first_page_data:
            logger_manager.warning("æ— æ³•è·å–è¿œç¨‹æ•°æ®")
            return 0

        latest_remote_issue = int(first_page_data[0]['issue'])

        if latest_local_issue:
            if latest_remote_issue <= latest_local_issue:
                logger_manager.info(f"æœ¬åœ°æ•°æ®å·²æ˜¯æœ€æ–°ï¼ˆæœ¬åœ°: {latest_local_issue}, è¿œç¨‹: {latest_remote_issue}ï¼‰")
                return 0

            missing_count = latest_remote_issue - latest_local_issue
            logger_manager.info(f"å‘ç° {missing_count} æœŸæ–°æ•°æ®éœ€è¦æ›´æ–°")
        else:
            logger_manager.info("æœ¬åœ°æ— æ•°æ®ï¼Œå¼€å§‹åˆå§‹åŒ–")

        # çˆ¬å–æ•°æ®ç›´åˆ°è·å–æ‰€æœ‰ç¼ºå¤±çš„æœŸæ•°
        for page in range(1, max_pages + 1):
            page_data = self.crawl_page(page)

            if not page_data:
                break

            # å¦‚æœæœ‰æœ¬åœ°æœ€æ–°æœŸå·ï¼Œåªä¿ç•™æ¯”å®ƒæ–°çš„æ•°æ®
            if latest_local_issue:
                new_data = []
                for item in page_data:
                    current_issue = int(item['issue'])
                    if current_issue > latest_local_issue:
                        new_data.append(item)
                    else:
                        # é‡åˆ°å·²å­˜åœ¨çš„æœŸå·ï¼Œåœæ­¢çˆ¬å–
                        logger_manager.info(f"é‡åˆ°å·²å­˜åœ¨æœŸå· {current_issue}ï¼Œåœæ­¢çˆ¬å–")
                        break

                all_data.extend(new_data)

                # å¦‚æœè¿™ä¸€é¡µæ²¡æœ‰æ–°æ•°æ®ï¼Œåœæ­¢çˆ¬å–
                if not new_data:
                    break
            else:
                all_data.extend(page_data)

        if all_data:
            self.save_data(all_data)
            logger_manager.info(f"å¢é‡æ›´æ–°å®Œæˆï¼Œæ–°å¢ {len(all_data)} æœŸæ•°æ®")
        else:
            logger_manager.info("æ²¡æœ‰æ–°æ•°æ®éœ€è¦æ›´æ–°")

        return len(all_data)

    def crawl_all_data(self, max_pages=100):
        """çˆ¬å–æ‰€æœ‰æ•°æ®"""
        logger_manager.info(f"å¼€å§‹çˆ¬å–æ‰€æœ‰æ•°æ®ï¼Œæœ€å¤§é¡µæ•°: {max_pages}")

        all_data = []

        for page in range(1, max_pages + 1):
            page_data = self.crawl_page(page)

            if not page_data:
                logger_manager.info(f"ç¬¬ {page} é¡µæ²¡æœ‰æ•°æ®ï¼Œåœæ­¢çˆ¬å–")
                break

            all_data.extend(page_data)
            logger_manager.info(f"å·²çˆ¬å– {len(all_data)} æœŸæ•°æ®")

            # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(0.5)

        if all_data:
            self.save_data(all_data)
            logger_manager.info(f"çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(all_data)} æœŸæ•°æ®")

        return len(all_data)


class ZhcwCrawler(BaseCrawler):
    """ä¸­å½©ç½‘çˆ¬è™«ï¼ˆä½¿ç”¨APIæ¥å£ï¼‰"""

    def __init__(self, data_file="data/dlt_data_all.csv"):
        super().__init__(data_file)
        self.api_url = "https://webapi.sporttery.cn/gateway/lottery/getHistoryPageListV1.qry"
        self.session.headers.update({
            'Referer': 'https://www.zhcw.com/'
        })

    def crawl_page(self, page: int = 1) -> List[Dict]:
        """çˆ¬å–æŒ‡å®šé¡µé¢çš„æ•°æ®"""
        try:
            # æ„å»ºAPIè¯·æ±‚å‚æ•°
            params = {
                'gameNo': '85',  # å¤§ä¹é€æ¸¸æˆç¼–å·
                'provinceId': '0',
                'pageSize': '30',
                'isVerify': '1',
                'pageNo': str(page)
            }

            # å‘é€APIè¯·æ±‚
            response = self.session.get(self.api_url, params=params, timeout=15)
            response.raise_for_status()

            # è§£æJSONå“åº”
            data_json = response.json()

            if not data_json.get('success'):
                logger_manager.error(f"APIè¯·æ±‚å¤±è´¥: {data_json.get('errorMessage', 'æœªçŸ¥é”™è¯¯')}")
                return []

            # æå–æ•°æ®åˆ—è¡¨
            value = data_json.get('value', {})
            lottery_list = value.get('list', [])

            if not lottery_list:
                logger_manager.warning(f"ç¬¬ {page} é¡µæ²¡æœ‰æ•°æ®")
                return []

            data = []
            for item in lottery_list:
                try:
                    # æå–æœŸå·
                    issue = item.get('lotteryDrawNum', '').strip()

                    # æå–å¼€å¥–æ—¥æœŸ
                    date = item.get('lotteryDrawTime', '').strip()

                    # æå–å¼€å¥–å·ç 
                    numbers_text = item.get('lotteryDrawResult', '').strip()

                    if issue and date and numbers_text:
                        # åˆ†å‰²å·ç 
                        numbers = numbers_text.split()

                        if len(numbers) >= 7:
                            front_balls = [num.zfill(2) for num in numbers[:5]]
                            back_balls = [num.zfill(2) for num in numbers[5:7]]

                            data.append({
                                'issue': issue,
                                'date': date,
                                'front_balls': ','.join(front_balls),
                                'back_balls': ','.join(back_balls)
                            })

                except Exception as e:
                    logger_manager.warning(f"è§£ææ•°æ®é¡¹å¤±è´¥: {e}")
                    continue

            logger_manager.info(f"é¡µé¢ {page} çˆ¬å–åˆ° {len(data)} æœŸæ•°æ®")
            return data

        except Exception as e:
            logger_manager.error(f"çˆ¬å–é¡µé¢ {page} å¤±è´¥: {e}")
            return []
    
    def crawl_recent_data(self, max_pages_or_periods) -> int:
        """çˆ¬å–æœ€è¿‘çš„æ•°æ®ï¼ˆæ”¯æŒæŒ‰é¡µæ•°æˆ–æœŸæ•°ï¼‰"""
        if isinstance(max_pages_or_periods, int) and max_pages_or_periods <= 10:
            # å¦‚æœæ˜¯å°æ•°å­—ï¼Œè®¤ä¸ºæ˜¯é¡µæ•°ï¼ˆå¢é‡æ›´æ–°ï¼‰
            return self._crawl_by_pages(max_pages_or_periods)
        else:
            # å¦åˆ™è®¤ä¸ºæ˜¯æœŸæ•°
            return self._crawl_by_periods(int(max_pages_or_periods))

    def _crawl_by_pages(self, max_pages: int) -> int:
        """æŒ‰é¡µæ•°çˆ¬å–ï¼ˆå¢é‡æ›´æ–°ï¼‰"""
        logger_manager.info(f"å¼€å§‹å¢é‡æ›´æ–°ï¼Œæœ€å¤§é¡µæ•°: {max_pages}")

        all_data = []
        latest_local_issue = self.get_latest_issue()

        # è·å–æœ€æ–°çš„è¿œç¨‹æ•°æ®æ¥ç¡®å®šéœ€è¦æ›´æ–°çš„æœŸæ•°
        first_page_data = self.crawl_page(1)
        if not first_page_data:
            logger_manager.warning("æ— æ³•è·å–è¿œç¨‹æ•°æ®")
            return 0

        latest_remote_issue = int(first_page_data[0]['issue'])

        if latest_local_issue:
            if latest_remote_issue <= latest_local_issue:
                logger_manager.info(f"æœ¬åœ°æ•°æ®å·²æ˜¯æœ€æ–°ï¼ˆæœ¬åœ°: {latest_local_issue}, è¿œç¨‹: {latest_remote_issue}ï¼‰")
                return 0

            missing_count = latest_remote_issue - latest_local_issue
            logger_manager.info(f"å‘ç° {missing_count} æœŸæ–°æ•°æ®éœ€è¦æ›´æ–°")
        else:
            logger_manager.info("æœ¬åœ°æ— æ•°æ®ï¼Œå¼€å§‹åˆå§‹åŒ–")

        # çˆ¬å–æ•°æ®ç›´åˆ°è·å–æ‰€æœ‰ç¼ºå¤±çš„æœŸæ•°
        for page in range(1, max_pages + 1):
            page_data = self.crawl_page(page)

            if not page_data:
                break

            # å¦‚æœæœ‰æœ¬åœ°æœ€æ–°æœŸå·ï¼Œåªä¿ç•™æ¯”å®ƒæ–°çš„æ•°æ®
            if latest_local_issue:
                new_data = []
                for item in page_data:
                    current_issue = int(item['issue'])

                    if current_issue > latest_local_issue:
                        new_data.append(item)
                    else:
                        # é‡åˆ°å·²å­˜åœ¨çš„æœŸå·ï¼Œåœæ­¢çˆ¬å–
                        logger_manager.info(f"é‡åˆ°å·²å­˜åœ¨æœŸå· {current_issue}ï¼Œåœæ­¢çˆ¬å–")
                        break

                all_data.extend(new_data)

                # å¦‚æœè¿™ä¸€é¡µæ²¡æœ‰æ–°æ•°æ®ï¼Œåœæ­¢çˆ¬å–
                if not new_data:
                    break
            else:
                all_data.extend(page_data)

            time.sleep(0.5)

        if all_data:
            self.save_data(all_data)
            logger_manager.info(f"å¢é‡æ›´æ–°å®Œæˆï¼Œæ–°å¢ {len(all_data)} æœŸæ•°æ®")
        else:
            logger_manager.info("æ²¡æœ‰æ–°æ•°æ®éœ€è¦æ›´æ–°")

        return len(all_data)

    def _crawl_by_periods(self, periods: int) -> int:
        """æŒ‰æœŸæ•°çˆ¬å–"""
        logger_manager.info(f"å¼€å§‹çˆ¬å–æœ€è¿‘ {periods} æœŸæ•°æ®...")

        all_data = []
        pages_needed = (periods // 30) + 1  # æ¯é¡µå¤§çº¦30æœŸæ•°æ®

        for page in range(1, pages_needed + 1):
            logger_manager.info(f"æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ...")

            page_data = self.crawl_page(page)
            if not page_data:
                logger_manager.warning(f"ç¬¬ {page} é¡µæ— æ•°æ®ï¼Œåœæ­¢çˆ¬å–")
                break

            all_data.extend(page_data)

            # å¦‚æœå·²ç»è·å–è¶³å¤Ÿçš„æ•°æ®ï¼Œåœæ­¢çˆ¬å–
            if len(all_data) >= periods:
                all_data = all_data[:periods]
                break

            # å»¶æ—¶é¿å…è¢«å°
            time.sleep(0.5)

        if all_data:
            self.save_data(all_data)
            logger_manager.info(f"çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(all_data)} æœŸæ•°æ®")
        else:
            logger_manager.info("æ²¡æœ‰è·å–åˆ°æ•°æ®")

        return len(all_data)

    def crawl_all_data(self, max_pages: int = 100) -> int:
        """çˆ¬å–æ‰€æœ‰æ•°æ®"""
        logger_manager.info("å¼€å§‹ä»ä¸­å½©ç½‘çˆ¬å–å¤§ä¹é€æ•°æ®...")

        all_data = []
        latest_local_issue = self.get_latest_issue()

        for page in range(1, max_pages + 1):
            logger_manager.info(f"æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ...")

            page_data = self.crawl_page(page)
            if not page_data:
                logger_manager.warning(f"ç¬¬ {page} é¡µæ— æ•°æ®ï¼Œåœæ­¢çˆ¬å–")
                break

            # æ£€æŸ¥æ˜¯å¦å·²ç»çˆ¬å–åˆ°æœ¬åœ°æœ€æ–°æ•°æ®
            if latest_local_issue:
                page_issues = [item['issue'] for item in page_data]
                if latest_local_issue in page_issues:
                    # åªä¿ç•™æ¯”æœ¬åœ°æœ€æ–°æœŸå·æ›´æ–°çš„æ•°æ®
                    new_data = [item for item in page_data if item['issue'] > latest_local_issue]
                    all_data.extend(new_data)
                    logger_manager.info(f"å·²çˆ¬å–åˆ°æœ¬åœ°æœ€æ–°æ•°æ® {latest_local_issue}ï¼Œåœæ­¢çˆ¬å–")
                    break

            all_data.extend(page_data)

            # å»¶æ—¶é¿å…è¢«å°
            time.sleep(1)

        if all_data:
            self.save_data(all_data)
            logger_manager.info(f"çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(all_data)} æœŸæ–°æ•°æ®")
        else:
            logger_manager.info("æ²¡æœ‰æ–°æ•°æ®éœ€è¦æ›´æ–°")

        return len(all_data)


class Crawler500(BaseCrawler):
    """500å½©ç¥¨ç½‘çˆ¬è™«"""
    
    def __init__(self, data_file="data/dlt_data_all.csv"):
        super().__init__(data_file)
        self.base_url = "https://www.500.com"
        self.dlt_url = "https://www.500.com/static/public/dlt/xml/dltdata.xml"
    
    def crawl_xml_data(self) -> List[Dict]:
        """ä»XMLæ¥å£çˆ¬å–æ•°æ®"""
        try:
            response = self.session.get(self.dlt_url, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            # è§£æXMLæ•°æ®
            from xml.etree import ElementTree as ET
            root = ET.fromstring(response.text)
            
            data = []
            for row in root.findall('.//row'):
                try:
                    # è§£æå±æ€§
                    issue = row.get('expect')
                    date = row.get('opentime')
                    opencode = row.get('opencode')
                    
                    if opencode:
                        # è§£æå·ç  (æ ¼å¼: 01,02,03,04,05+06,07)
                        parts = opencode.split('+')
                        if len(parts) == 2:
                            front_balls = parts[0]
                            back_balls = parts[1]
                            
                            data.append({
                                'issue': issue,
                                'date': date,
                                'front_balls': front_balls,
                                'back_balls': back_balls
                            })
                
                except Exception as e:
                    logger_manager.error(f"è§£æXMLè¡Œæ•°æ®å¤±è´¥: {e}")
                    continue
            
            logger_manager.info(f"ä»500å½©ç¥¨ç½‘çˆ¬å–åˆ° {len(data)} æœŸæ•°æ®")
            return data
        
        except Exception as e:
            logger_manager.error("ä»500å½©ç¥¨ç½‘çˆ¬å–æ•°æ®å¤±è´¥", e)
            return []
    
    def crawl_all_data(self) -> int:
        """çˆ¬å–æ‰€æœ‰æ•°æ®"""
        logger_manager.info("å¼€å§‹ä»500å½©ç¥¨ç½‘çˆ¬å–å¤§ä¹é€æ•°æ®...")
        
        all_data = self.crawl_xml_data()
        
        if all_data:
            # è¿‡æ»¤æ–°æ•°æ®
            latest_local_issue = self.get_latest_issue()
            if latest_local_issue:
                new_data = [item for item in all_data if int(item['issue']) > latest_local_issue]
                if new_data:
                    self.save_data(new_data)
                    logger_manager.info(f"çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(new_data)} æœŸæ–°æ•°æ®")
                    return len(new_data)
                else:
                    logger_manager.info("æ²¡æœ‰æ–°æ•°æ®éœ€è¦æ›´æ–°")
                    return 0
            else:
                self.save_data(all_data)
                logger_manager.info(f"çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(all_data)} æœŸæ•°æ®")
                return len(all_data)
        else:
            logger_manager.warning("æœªè·å–åˆ°ä»»ä½•æ•°æ®")
            return 0

    def crawl_recent_data(self, max_pages_or_periods) -> int:
        """çˆ¬å–æœ€è¿‘çš„æ•°æ®ï¼ˆ500å½©ç¥¨ç½‘åªæ”¯æŒå…¨é‡è·å–ï¼‰"""
        logger_manager.info("500å½©ç¥¨ç½‘çˆ¬è™«æ‰§è¡Œå¢é‡æ›´æ–°...")
        return self.crawl_all_data()


def update_data(source: str = "zhcw") -> int:
    """æ›´æ–°æ•°æ®çš„ä¾¿æ·å‡½æ•°"""
    if source == "zhcw":
        crawler = ZhcwCrawler()
    elif source == "500":
        crawler = Crawler500()
    else:
        logger_manager.error(f"ä¸æ”¯æŒçš„æ•°æ®æº: {source}")
        return 0
    
    return crawler.crawl_all_data()


if __name__ == "__main__":
    # æµ‹è¯•çˆ¬è™«
    print("ğŸ•·ï¸ æµ‹è¯•æ•°æ®çˆ¬è™«...")
    
    # æµ‹è¯•ä¸­å½©ç½‘çˆ¬è™«
    print("ğŸ“Š æµ‹è¯•ä¸­å½©ç½‘çˆ¬è™«...")
    zhcw_crawler = ZhcwCrawler()
    zhcw_count = zhcw_crawler.crawl_all_data(max_pages=2)
    print(f"ä¸­å½©ç½‘çˆ¬å–ç»“æœ: {zhcw_count} æœŸ")
    
    # æµ‹è¯•500å½©ç¥¨ç½‘çˆ¬è™«
    print("ğŸ“Š æµ‹è¯•500å½©ç¥¨ç½‘çˆ¬è™«...")
    crawler500 = Crawler500()
    count500 = crawler500.crawl_all_data()
    print(f"500å½©ç¥¨ç½‘çˆ¬å–ç»“æœ: {count500} æœŸ")
    
    print("âœ… çˆ¬è™«æ¨¡å—æµ‹è¯•å®Œæˆ")
