#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ•°æ®çˆ¬è™«æ¨¡å—
æ”¯æŒä»ä¸­å½©ç½‘ã€500å½©ç¥¨ç½‘ç­‰æ•°æ®æºçˆ¬å–å¤§ä¹é€å†å²å¼€å¥–æ•°æ®
"""

import os
import re
import time
import requests
import pandas as pd
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from typing import List, Dict, Optional

from core_modules import logger_manager, data_manager


class BaseCrawler:
    """çˆ¬è™«åŸºç±»"""
    
    def __init__(self, data_file="data/dlt_data_all.csv"):
        self.data_file = data_file
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(data_file), exist_ok=True)
    
    def save_data(self, data: List[Dict]):
        """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶"""
        if not data:
            logger_manager.warning("æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
            return
        
        df = pd.DataFrame(data)
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆå¹¶æ•°æ®
        if os.path.exists(self.data_file):
            existing_df = pd.read_csv(self.data_file)
            # å»é‡ï¼Œä¿ç•™æœ€æ–°æ•°æ®
            combined_df = pd.concat([existing_df, df]).drop_duplicates(subset=['issue'], keep='last')
            combined_df = combined_df.sort_values('issue').reset_index(drop=True)
        else:
            combined_df = df.sort_values('issue').reset_index(drop=True)
        
        combined_df.to_csv(self.data_file, index=False, encoding='utf-8')
        logger_manager.info(f"æ•°æ®å·²ä¿å­˜åˆ° {self.data_file}ï¼Œå…± {len(combined_df)} æœŸ")
    
    def get_latest_issue(self) -> Optional[str]:
        """è·å–æœ¬åœ°æ•°æ®ä¸­çš„æœ€æ–°æœŸå·"""
        if os.path.exists(self.data_file):
            try:
                df = pd.read_csv(self.data_file)
                if not df.empty:
                    return df.iloc[-1]['issue']
            except Exception as e:
                logger_manager.error("è¯»å–æœ¬åœ°æ•°æ®å¤±è´¥", e)
        return None


class ZhcwCrawler(BaseCrawler):
    """ä¸­å½©ç½‘çˆ¬è™«"""
    
    def __init__(self, data_file="data/dlt_data_all.csv"):
        super().__init__(data_file)
        self.base_url = "https://www.zhcw.com"
        self.dlt_url = "https://www.zhcw.com/kjxx/dlt/"
    
    def crawl_page(self, page: int = 1) -> List[Dict]:
        """çˆ¬å–æŒ‡å®šé¡µé¢çš„æ•°æ®"""
        url = f"{self.dlt_url}?page={page}"
        
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾å¼€å¥–æ•°æ®è¡¨æ ¼
            table = soup.find('table', class_='kjjg_table')
            if not table:
                logger_manager.warning(f"é¡µé¢ {page} æœªæ‰¾åˆ°æ•°æ®è¡¨æ ¼")
                return []
            
            data = []
            rows = table.find('tbody').find_all('tr')
            
            for row in rows:
                cols = row.find_all('td')
                if len(cols) >= 4:
                    try:
                        # è§£ææœŸå·
                        issue = cols[0].text.strip()
                        
                        # è§£æå¼€å¥–æ—¥æœŸ
                        date = cols[1].text.strip()
                        
                        # è§£æå¼€å¥–å·ç 
                        balls_td = cols[2]
                        ball_spans = balls_td.find_all('span')
                        
                        if len(ball_spans) >= 7:
                            front_balls = [span.text.strip() for span in ball_spans[:5]]
                            back_balls = [span.text.strip() for span in ball_spans[5:7]]
                            
                            data.append({
                                'issue': issue,
                                'date': date,
                                'front_balls': ','.join(front_balls),
                                'back_balls': ','.join(back_balls)
                            })
                    
                    except Exception as e:
                        logger_manager.error(f"è§£æè¡Œæ•°æ®å¤±è´¥: {e}")
                        continue
            
            logger_manager.info(f"é¡µé¢ {page} çˆ¬å–åˆ° {len(data)} æœŸæ•°æ®")
            return data
        
        except Exception as e:
            logger_manager.error(f"çˆ¬å–é¡µé¢ {page} å¤±è´¥", e)
            return []
    
    def crawl_recent_data(self, periods: int) -> int:
        """çˆ¬å–æœ€è¿‘æŒ‡å®šæœŸæ•°çš„æ•°æ®"""
        logger_manager.info(f"å¼€å§‹ä»ä¸­å½©ç½‘çˆ¬å–æœ€è¿‘ {periods} æœŸæ•°æ®...")

        all_data = []
        pages_needed = (periods // 20) + 1  # æ¯é¡µå¤§çº¦20æœŸæ•°æ®

        for page in range(1, pages_needed + 1):
            logger_manager.info(f"æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ...")

            page_data = self.crawl_page(page)
            if not page_data:
                logger_manager.warning(f"ç¬¬ {page} é¡µæ— æ•°æ®ï¼Œåœæ­¢çˆ¬å–")
                break

            all_data.extend(page_data)

            # å¦‚æœå·²ç»è·å–è¶³å¤Ÿçš„æ•°æ®ï¼Œåœæ­¢çˆ¬å–
            if len(all_data) >= periods:
                all_data = all_data[:periods]
                break

            # å»¶æ—¶é¿å…è¢«å°
            time.sleep(1)

        if all_data:
            self.save_data(all_data)
            logger_manager.info(f"çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(all_data)} æœŸæ•°æ®")
        else:
            logger_manager.info("æ²¡æœ‰è·å–åˆ°æ•°æ®")

        return len(all_data)

    def crawl_all_data(self, max_pages: int = 100) -> int:
        """çˆ¬å–æ‰€æœ‰æ•°æ®"""
        logger_manager.info("å¼€å§‹ä»ä¸­å½©ç½‘çˆ¬å–å¤§ä¹é€æ•°æ®...")

        all_data = []
        latest_local_issue = self.get_latest_issue()

        for page in range(1, max_pages + 1):
            logger_manager.info(f"æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ...")

            page_data = self.crawl_page(page)
            if not page_data:
                logger_manager.warning(f"ç¬¬ {page} é¡µæ— æ•°æ®ï¼Œåœæ­¢çˆ¬å–")
                break

            # æ£€æŸ¥æ˜¯å¦å·²ç»çˆ¬å–åˆ°æœ¬åœ°æœ€æ–°æ•°æ®
            if latest_local_issue:
                page_issues = [item['issue'] for item in page_data]
                if latest_local_issue in page_issues:
                    # åªä¿ç•™æ¯”æœ¬åœ°æœ€æ–°æœŸå·æ›´æ–°çš„æ•°æ®
                    new_data = [item for item in page_data if item['issue'] > latest_local_issue]
                    all_data.extend(new_data)
                    logger_manager.info(f"å·²çˆ¬å–åˆ°æœ¬åœ°æœ€æ–°æ•°æ® {latest_local_issue}ï¼Œåœæ­¢çˆ¬å–")
                    break

            all_data.extend(page_data)

            # å»¶æ—¶é¿å…è¢«å°
            time.sleep(1)

        if all_data:
            self.save_data(all_data)
            logger_manager.info(f"çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(all_data)} æœŸæ–°æ•°æ®")
        else:
            logger_manager.info("æ²¡æœ‰æ–°æ•°æ®éœ€è¦æ›´æ–°")

        return len(all_data)


class Crawler500(BaseCrawler):
    """500å½©ç¥¨ç½‘çˆ¬è™«"""
    
    def __init__(self, data_file="data/dlt_data_all.csv"):
        super().__init__(data_file)
        self.base_url = "https://www.500.com"
        self.dlt_url = "https://www.500.com/static/public/dlt/xml/dltdata.xml"
    
    def crawl_xml_data(self) -> List[Dict]:
        """ä»XMLæ¥å£çˆ¬å–æ•°æ®"""
        try:
            response = self.session.get(self.dlt_url, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            # è§£æXMLæ•°æ®
            from xml.etree import ElementTree as ET
            root = ET.fromstring(response.text)
            
            data = []
            for row in root.findall('.//row'):
                try:
                    # è§£æå±æ€§
                    issue = row.get('expect')
                    date = row.get('opentime')
                    opencode = row.get('opencode')
                    
                    if opencode:
                        # è§£æå·ç  (æ ¼å¼: 01,02,03,04,05+06,07)
                        parts = opencode.split('+')
                        if len(parts) == 2:
                            front_balls = parts[0]
                            back_balls = parts[1]
                            
                            data.append({
                                'issue': issue,
                                'date': date,
                                'front_balls': front_balls,
                                'back_balls': back_balls
                            })
                
                except Exception as e:
                    logger_manager.error(f"è§£æXMLè¡Œæ•°æ®å¤±è´¥: {e}")
                    continue
            
            logger_manager.info(f"ä»500å½©ç¥¨ç½‘çˆ¬å–åˆ° {len(data)} æœŸæ•°æ®")
            return data
        
        except Exception as e:
            logger_manager.error("ä»500å½©ç¥¨ç½‘çˆ¬å–æ•°æ®å¤±è´¥", e)
            return []
    
    def crawl_all_data(self) -> int:
        """çˆ¬å–æ‰€æœ‰æ•°æ®"""
        logger_manager.info("å¼€å§‹ä»500å½©ç¥¨ç½‘çˆ¬å–å¤§ä¹é€æ•°æ®...")
        
        all_data = self.crawl_xml_data()
        
        if all_data:
            # è¿‡æ»¤æ–°æ•°æ®
            latest_local_issue = self.get_latest_issue()
            if latest_local_issue:
                new_data = [item for item in all_data if item['issue'] > latest_local_issue]
                if new_data:
                    self.save_data(new_data)
                    logger_manager.info(f"çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(new_data)} æœŸæ–°æ•°æ®")
                    return len(new_data)
                else:
                    logger_manager.info("æ²¡æœ‰æ–°æ•°æ®éœ€è¦æ›´æ–°")
                    return 0
            else:
                self.save_data(all_data)
                logger_manager.info(f"çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(all_data)} æœŸæ•°æ®")
                return len(all_data)
        else:
            logger_manager.warning("æœªè·å–åˆ°ä»»ä½•æ•°æ®")
            return 0


def update_data(source: str = "zhcw") -> int:
    """æ›´æ–°æ•°æ®çš„ä¾¿æ·å‡½æ•°"""
    if source == "zhcw":
        crawler = ZhcwCrawler()
    elif source == "500":
        crawler = Crawler500()
    else:
        logger_manager.error(f"ä¸æ”¯æŒçš„æ•°æ®æº: {source}")
        return 0
    
    return crawler.crawl_all_data()


if __name__ == "__main__":
    # æµ‹è¯•çˆ¬è™«
    print("ğŸ•·ï¸ æµ‹è¯•æ•°æ®çˆ¬è™«...")
    
    # æµ‹è¯•ä¸­å½©ç½‘çˆ¬è™«
    print("ğŸ“Š æµ‹è¯•ä¸­å½©ç½‘çˆ¬è™«...")
    zhcw_crawler = ZhcwCrawler()
    zhcw_count = zhcw_crawler.crawl_all_data(max_pages=2)
    print(f"ä¸­å½©ç½‘çˆ¬å–ç»“æœ: {zhcw_count} æœŸ")
    
    # æµ‹è¯•500å½©ç¥¨ç½‘çˆ¬è™«
    print("ğŸ“Š æµ‹è¯•500å½©ç¥¨ç½‘çˆ¬è™«...")
    crawler500 = Crawler500()
    count500 = crawler500.crawl_all_data()
    print(f"500å½©ç¥¨ç½‘çˆ¬å–ç»“æœ: {count500} æœŸ")
    
    print("âœ… çˆ¬è™«æ¨¡å—æµ‹è¯•å®Œæˆ")
