#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
é«˜çº§æ··åˆåˆ†æé¢„æµ‹ç³»ç»Ÿ
åŸºäºç»Ÿè®¡å­¦ã€æ¦‚ç‡è®ºã€é©¬å°”å¯å¤«é“¾ã€è´å¶æ–¯åˆ†æã€å†·çƒ­å·åˆ†å¸ƒè§„å¾‹ç­‰å¤šç§æ•°å­¦æ¨¡å‹
"""

import os
import json
import numpy as np
import pandas as pd
from collections import defaultdict, Counter
from datetime import datetime
import scipy.stats as stats
from scipy.fft import fft
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')


class AdvancedHybridAnalyzer:
    """é«˜çº§æ··åˆåˆ†æé¢„æµ‹å™¨"""
    
    def __init__(self, data_file, output_dir="output/hybrid"):
        """åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            data_file: æ•°æ®æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
        """
        self.data_file = data_file
        self.output_dir = output_dir
        self.df = None
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        # æ¨¡å‹æƒé‡é…ç½®ï¼ˆåŸºäºæŠ€æœ¯æ–‡æ¡£ï¼‰
        self.model_weights = {
            'statistical': 0.15,    # ç»Ÿè®¡å­¦åˆ†æ
            'probability': 0.20,    # æ¦‚ç‡è®ºåˆ†æ
            'markov': 0.25,         # é©¬å°”å¯å¤«é“¾åˆ†æï¼ˆæœ€é«˜æƒé‡ï¼‰
            'bayesian': 0.15,       # è´å¶æ–¯åˆ†æ
            'hot_cold': 0.15,       # å†·çƒ­å·åˆ†æ
            'cycle': 0.10,          # å‘¨æœŸæ€§åˆ†æ
            'correlation': 0.00     # ç›¸å…³æ€§åˆ†æï¼ˆéªŒè¯ç”¨ï¼‰
        }
        
        # ç¨³å®šæ€§é˜ˆå€¼é…ç½®
        self.stability_thresholds = {
            'front_position_transition': 5,
            'front_global_transition': 10,
            'back_transition': 3,
            'correlation_threshold': 0.3,
            'significance_level': 0.05
        }
        
        # åŠ è½½æ•°æ®
        self.load_data()
    
    def load_data(self):
        """åŠ è½½æ•°æ®"""
        try:
            self.df = pd.read_csv(self.data_file)
            # æŒ‰æœŸå·æ’åºï¼ˆä»æ—©åˆ°æ™šï¼‰
            self.df = self.df.sort_values('issue', ascending=True)
            print(f"æˆåŠŸåŠ è½½æ•°æ®ï¼Œå…± {len(self.df)} æ¡è®°å½•")
            print(f"æ•°æ®èŒƒå›´: {self.df.iloc[0]['issue']} - {self.df.iloc[-1]['issue']}")
            return True
        except Exception as e:
            print(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return False
    
    def parse_balls(self, balls_str):
        """è§£æå·ç å­—ç¬¦ä¸²"""
        return [int(ball.strip()) for ball in str(balls_str).split(",")]
    
    def predict_with_hybrid_analysis(self, periods=100, count=1, explain=True):
        """ä½¿ç”¨æ··åˆåˆ†æè¿›è¡Œé¢„æµ‹
        
        Args:
            periods: åˆ†ææœŸæ•°
            count: é¢„æµ‹æ³¨æ•°
            explain: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¿‡ç¨‹
        
        Returns:
            é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        if explain:
            print("=" * 80)
            print("ğŸ”¬ é«˜çº§æ··åˆåˆ†æé¢„æµ‹ç³»ç»Ÿ")
            print("=" * 80)
            print(f"ğŸ“Š åˆ†ææœŸæ•°: {periods} æœŸ")
            print(f"ğŸ¯ é¢„æµ‹æ³¨æ•°: {count} æ³¨")
            print(f"ğŸ“ˆ ä½¿ç”¨æ¨¡å‹: ç»Ÿè®¡å­¦ã€æ¦‚ç‡è®ºã€é©¬å°”å¯å¤«é“¾ã€è´å¶æ–¯ã€å†·çƒ­å·ã€å‘¨æœŸæ€§ã€ç›¸å…³æ€§")
            print()
        
        # è·å–åˆ†ææ•°æ®
        analysis_data = self.df.tail(periods).copy()
        
        if len(analysis_data) < 10:
            print("âŒ æ•°æ®ä¸è¶³ï¼Œè‡³å°‘éœ€è¦10æœŸæ•°æ®")
            return []
        
        # æ‰§è¡Œå¤šæ¨¡å‹åˆ†æ
        if explain:
            print("ğŸ” å¼€å§‹å¤šæ¨¡å‹å¹¶è¡Œåˆ†æ...")
        
        hybrid_analysis = self._run_hybrid_analysis(analysis_data, explain)
        
        # ç”Ÿæˆé¢„æµ‹
        predictions = self._generate_predictions(
            hybrid_analysis, analysis_data, count, explain
        )
        
        # ä¿å­˜åˆ†æç»“æœ
        self._save_analysis_results(hybrid_analysis, predictions, periods)
        
        if explain:
            print("\n" + "=" * 80)
            print("âœ… é«˜çº§æ··åˆåˆ†æå®Œæˆ")
            print("=" * 80)
        
        return predictions
    
    def _run_hybrid_analysis(self, data, explain=True):
        """è¿è¡Œæ··åˆåˆ†æ"""
        analysis_results = {}
        
        # 1. ç»Ÿè®¡å­¦åˆ†ææ¨¡å— (15%)
        if explain:
            print("ğŸ“ˆ 1. ç»Ÿè®¡å­¦åˆ†ææ¨¡å— (æƒé‡: 15%)")
        analysis_results['statistical'] = self._statistical_analysis(data, explain)
        
        # 2. æ¦‚ç‡è®ºåˆ†ææ¨¡å— (20%)
        if explain:
            print("\nğŸ² 2. æ¦‚ç‡è®ºåˆ†ææ¨¡å— (æƒé‡: 20%)")
        analysis_results['probability'] = self._probability_analysis(data, explain)
        
        # 3. é©¬å°”å¯å¤«é“¾åˆ†ææ¨¡å— (25%)
        if explain:
            print("\nğŸ”— 3. é©¬å°”å¯å¤«é“¾åˆ†ææ¨¡å— (æƒé‡: 25%)")
        analysis_results['markov'] = self._markov_analysis(data, explain)
        
        # 4. è´å¶æ–¯åˆ†ææ¨¡å— (15%)
        if explain:
            print("\nğŸ§® 4. è´å¶æ–¯åˆ†ææ¨¡å— (æƒé‡: 15%)")
        analysis_results['bayesian'] = self._bayesian_analysis(data, explain)
        
        # 5. å†·çƒ­å·åˆ†ææ¨¡å— (15%)
        if explain:
            print("\nğŸŒ¡ï¸ 5. å†·çƒ­å·åˆ†ææ¨¡å— (æƒé‡: 15%)")
        analysis_results['hot_cold'] = self._hot_cold_analysis(data, explain)
        
        # 6. å‘¨æœŸæ€§åˆ†ææ¨¡å— (10%)
        if explain:
            print("\nğŸ”„ 6. å‘¨æœŸæ€§åˆ†ææ¨¡å— (æƒé‡: 10%)")
        analysis_results['cycle'] = self._cycle_analysis(data, explain)
        
        # 7. ç›¸å…³æ€§åˆ†ææ¨¡å— (éªŒè¯ç”¨)
        if explain:
            print("\nğŸ” 7. ç›¸å…³æ€§åˆ†ææ¨¡å— (éªŒè¯ç”¨)")
        analysis_results['correlation'] = self._correlation_analysis(data, explain)
        
        return analysis_results
    
    def _statistical_analysis(self, data, explain=True):
        """ç»Ÿè®¡å­¦åˆ†ææ¨¡å—"""
        results = {}
        
        # è§£æå‰åŒºå’ŒååŒºå·ç 
        front_balls_lists = []
        back_balls_lists = []
        
        for _, row in data.iterrows():
            front_balls = self.parse_balls(row['front_balls'])
            back_balls = self.parse_balls(row['back_balls'])
            front_balls_lists.append(front_balls)
            back_balls_lists.append(back_balls)
        
        # è®¡ç®—å’Œå€¼ç»Ÿè®¡ç‰¹å¾
        front_sums = [sum(balls) for balls in front_balls_lists]
        
        # åŸºæœ¬ç»Ÿè®¡é‡
        stats_features = {
            'å‡å€¼': np.mean(front_sums),
            'æ ‡å‡†å·®': np.std(front_sums),
            'ä¸­ä½æ•°': np.median(front_sums),
            'ååº¦': stats.skew(front_sums),
            'å³°åº¦': stats.kurtosis(front_sums),
            'å˜å¼‚ç³»æ•°': np.std(front_sums) / np.mean(front_sums),
            'å››åˆ†ä½è·': np.percentile(front_sums, 75) - np.percentile(front_sums, 25)
        }
        
        # æ­£æ€æ€§æ£€éªŒ
        if len(front_sums) > 8:
            try:
                dagostino_stat, dagostino_p = stats.normaltest(front_sums)
                stats_features['æ­£æ€æ€§æ£€éªŒpå€¼'] = dagostino_p
                stats_features['æ˜¯å¦æ­£æ€åˆ†å¸ƒ'] = dagostino_p > 0.05
            except:
                stats_features['æ­£æ€æ€§æ£€éªŒpå€¼'] = 0.5
                stats_features['æ˜¯å¦æ­£æ€åˆ†å¸ƒ'] = True
        
        results['å’Œå€¼ç»Ÿè®¡'] = stats_features
        
        # è®¡ç®—å·ç è¯„åˆ†
        target_sum = stats_features['å‡å€¼']
        front_scores = {}
        
        for ball in range(1, 36):
            # åŸºäºç›®æ ‡å’Œå€¼çš„é€‚åº”æ€§è¯„åˆ†
            contribution_score = self._calculate_sum_contribution(ball, target_sum)
            front_scores[ball] = contribution_score
        
        results['å‰åŒºè¯„åˆ†'] = front_scores
        
        # ååŒºç»Ÿè®¡åˆ†æ
        back_counts = Counter([ball for balls in back_balls_lists for ball in balls])
        back_scores = {}
        for ball in range(1, 13):
            back_scores[ball] = back_counts.get(ball, 0) / len(data)
        
        results['ååŒºè¯„åˆ†'] = back_scores
        
        if explain:
            print(f"   ğŸ“Š å’Œå€¼å‡å€¼: {stats_features['å‡å€¼']:.2f}")
            print(f"   ğŸ“Š å’Œå€¼æ ‡å‡†å·®: {stats_features['æ ‡å‡†å·®']:.2f}")
            print(f"   ğŸ“Š åˆ†å¸ƒååº¦: {stats_features['ååº¦']:.3f}")
            print(f"   ğŸ“Š æ˜¯å¦æ­£æ€åˆ†å¸ƒ: {stats_features['æ˜¯å¦æ­£æ€åˆ†å¸ƒ']}")
        
        return results
    
    def _calculate_sum_contribution(self, ball, target_sum):
        """è®¡ç®—å·ç å¯¹ç›®æ ‡å’Œå€¼çš„è´¡çŒ®åº¦"""
        # å‡è®¾å…¶ä»–4ä¸ªå·ç çš„å¹³å‡å€¼
        other_avg = (target_sum - ball) / 4
        
        # è¯„ä¼°è¿™ä¸ªå·ç åœ¨ç›®æ ‡å’Œå€¼ä¸­çš„åˆç†æ€§
        if 1 <= other_avg <= 35:
            # å·ç è¶Šæ¥è¿‘ç†æƒ³åˆ†å¸ƒï¼Œå¾—åˆ†è¶Šé«˜
            ideal_contribution = target_sum / 5
            deviation = abs(ball - ideal_contribution)
            score = max(0, 1 - deviation / 35)
        else:
            score = 0.1  # ä¸åˆç†çš„ç»„åˆç»™ä½åˆ†
        
        return score
    
    def _probability_analysis(self, data, explain=True):
        """æ¦‚ç‡è®ºåˆ†ææ¨¡å—"""
        results = {}
        
        # è®¡ç®—å·ç å‡ºç°æ¦‚ç‡
        front_counts = Counter()
        back_counts = Counter()
        
        for _, row in data.iterrows():
            front_balls = self.parse_balls(row['front_balls'])
            back_balls = self.parse_balls(row['back_balls'])
            
            for ball in front_balls:
                front_counts[ball] += 1
            for ball in back_balls:
                back_counts[ball] += 1
        
        total_front_draws = len(data) * 5
        total_back_draws = len(data) * 2
        
        # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
        front_probs = {ball: count / total_front_draws for ball, count in front_counts.items()}
        back_probs = {ball: count / total_back_draws for ball, count in back_counts.items()}
        
        # è¡¥å……æœªå‡ºç°çš„å·ç 
        for ball in range(1, 36):
            if ball not in front_probs:
                front_probs[ball] = 0.001  # ç»™æå°æ¦‚ç‡
        
        for ball in range(1, 13):
            if ball not in back_probs:
                back_probs[ball] = 0.001
        
        # å¡æ–¹æ£€éªŒï¼ˆæ£€éªŒå‡åŒ€åˆ†å¸ƒå‡è®¾ï¼‰
        expected_front = total_front_draws / 35
        observed_front = [front_counts.get(i, 0) for i in range(1, 36)]
        
        try:
            chi2_stat, chi2_p = stats.chisquare(observed_front)
            is_uniform = chi2_p > 0.05
        except:
            chi2_stat, chi2_p = 0, 0.5
            is_uniform = True
        
        # ä¿¡æ¯ç†µè®¡ç®—
        front_entropy = -sum(p * np.log2(p) for p in front_probs.values() if p > 0)
        
        results['å‰åŒºæ¦‚ç‡'] = front_probs
        results['ååŒºæ¦‚ç‡'] = back_probs
        results['å¡æ–¹æ£€éªŒ'] = {'ç»Ÿè®¡é‡': chi2_stat, 'på€¼': chi2_p, 'æ˜¯å¦å‡åŒ€': is_uniform}
        results['ä¿¡æ¯ç†µ'] = front_entropy
        
        if explain:
            print(f"   ğŸ² å‰åŒºä¿¡æ¯ç†µ: {front_entropy:.3f}")
            print(f"   ğŸ² å¡æ–¹æ£€éªŒpå€¼: {chi2_p:.3f}")
            print(f"   ğŸ² åˆ†å¸ƒæ˜¯å¦å‡åŒ€: {is_uniform}")
        
        return results

    def _markov_analysis(self, data, explain=True):
        """é©¬å°”å¯å¤«é“¾åˆ†ææ¨¡å—"""
        results = {}

        # æ„å»ºçŠ¶æ€è½¬ç§»çŸ©é˜µ
        front_transitions = defaultdict(lambda: defaultdict(int))
        back_transitions = defaultdict(lambda: defaultdict(int))

        sorted_data = data.sort_values('issue', ascending=True).reset_index(drop=True)

        # åˆ†æå‰åŒºè½¬ç§»
        for i in range(len(sorted_data) - 1):
            current_front = self.parse_balls(sorted_data.iloc[i]['front_balls'])
            next_front = self.parse_balls(sorted_data.iloc[i + 1]['front_balls'])

            for current_ball in current_front:
                for next_ball in next_front:
                    front_transitions[current_ball][next_ball] += 1

        # åˆ†æååŒºè½¬ç§»
        for i in range(len(sorted_data) - 1):
            current_back = self.parse_balls(sorted_data.iloc[i]['back_balls'])
            next_back = self.parse_balls(sorted_data.iloc[i + 1]['back_balls'])

            for current_ball in current_back:
                for next_ball in next_back:
                    back_transitions[current_ball][next_ball] += 1

        # è®¡ç®—è½¬ç§»æ¦‚ç‡ï¼ˆå¸¦ç¨³å®šæ€§æƒé‡ï¼‰
        front_probs = self._calculate_stable_transition_probs(
            front_transitions, self.stability_thresholds['front_global_transition'], 35
        )
        back_probs = self._calculate_stable_transition_probs(
            back_transitions, self.stability_thresholds['back_transition'], 12
        )

        results['å‰åŒºè½¬ç§»æ¦‚ç‡'] = front_probs
        results['ååŒºè½¬ç§»æ¦‚ç‡'] = back_probs

        # è®¡ç®—ç¨³å®šæ€§ç»Ÿè®¡
        stable_states = sum(1 for probs in front_probs.values()
                          if any(info.get('ç¨³å®šæ€§æƒé‡', 0) >= 0.5 for info in probs.values()))

        results['ç¨³å®šæ€§ç»Ÿè®¡'] = {
            'æ€»çŠ¶æ€æ•°': len(front_probs),
            'ç¨³å®šçŠ¶æ€æ•°': stable_states,
            'ç¨³å®šæ€§æ¯”ä¾‹': stable_states / len(front_probs) if front_probs else 0
        }

        if explain:
            print(f"   ğŸ”— å‰åŒºçŠ¶æ€æ•°: {len(front_probs)}")
            print(f"   ğŸ”— ç¨³å®šçŠ¶æ€æ•°: {stable_states}")
            print(f"   ğŸ”— ç¨³å®šæ€§æ¯”ä¾‹: {stable_states / len(front_probs) * 100:.1f}%" if front_probs else "0%")

        return results

    def _calculate_stable_transition_probs(self, transitions, threshold, max_ball):
        """è®¡ç®—å¸¦ç¨³å®šæ€§æƒé‡çš„è½¬ç§»æ¦‚ç‡"""
        stable_probs = {}

        for current, nexts in transitions.items():
            total_transitions = sum(nexts.values())
            stability_weight = min(1.0, total_transitions / threshold)

            stable_probs[current] = {}
            for next_ball, count in nexts.items():
                base_prob = count / total_transitions if total_transitions > 0 else 0
                uniform_prob = 1 / max_ball

                # ç¨³å®šæ€§è°ƒæ•´æ¦‚ç‡
                stable_prob = (base_prob * stability_weight +
                             (1 - stability_weight) * uniform_prob)

                stable_probs[current][next_ball] = {
                    'æ¦‚ç‡': stable_prob,
                    'åŸå§‹æ¦‚ç‡': base_prob,
                    'å‡ºç°æ¬¡æ•°': count,
                    'æ€»è½¬ç§»æ¬¡æ•°': total_transitions,
                    'ç¨³å®šæ€§æƒé‡': stability_weight
                }

        return stable_probs

    def _bayesian_analysis(self, data, explain=True):
        """è´å¶æ–¯åˆ†ææ¨¡å—"""
        results = {}

        # å…ˆéªŒæ¦‚ç‡è®¾å®š
        FRONT_PRIOR = 1/35
        BACK_PRIOR = 1/12

        # Betaåˆ†å¸ƒå‚æ•°
        ALPHA_PRIOR = 1
        BETA_PRIOR_FRONT = 35
        BETA_PRIOR_BACK = 12

        # è®¡ç®—è§‚æµ‹æ•°æ®
        front_counts = {i: 1 for i in range(1, 36)}  # åŠ 1å¹³æ»‘
        back_counts = {i: 1 for i in range(1, 13)}

        total_front_draws = len(data) * 5
        total_back_draws = len(data) * 2

        for _, row in data.iterrows():
            front_balls = self.parse_balls(row['front_balls'])
            back_balls = self.parse_balls(row['back_balls'])

            for ball in front_balls:
                front_counts[ball] += 1
            for ball in back_balls:
                back_counts[ball] += 1

        # è´å¶æ–¯åéªŒåˆ†æ
        front_posterior = {}
        for ball in range(1, 36):
            successes = front_counts[ball] - 1  # å‡å»å¹³æ»‘é¡¹
            trials = total_front_draws

            # BetaåéªŒå‚æ•°
            alpha_post = ALPHA_PRIOR + successes
            beta_post = BETA_PRIOR_FRONT + trials - successes

            # åéªŒç»Ÿè®¡
            posterior_mean = alpha_post / (alpha_post + beta_post)

            # è´å¶æ–¯å› å­
            likelihood = successes / trials if trials > 0 else 0
            bayes_factor = likelihood / FRONT_PRIOR if FRONT_PRIOR > 0 else 1

            front_posterior[ball] = {
                'åéªŒå‡å€¼': posterior_mean,
                'è´å¶æ–¯å› å­': bayes_factor,
                'è§‚æµ‹æ¬¡æ•°': successes,
                'æ€»è¯•éªŒæ¬¡æ•°': trials
            }

        # ååŒºè´å¶æ–¯åˆ†æ
        back_posterior = {}
        for ball in range(1, 13):
            successes = back_counts[ball] - 1
            trials = total_back_draws

            alpha_post = ALPHA_PRIOR + successes
            beta_post = BETA_PRIOR_BACK + trials - successes

            posterior_mean = alpha_post / (alpha_post + beta_post)
            likelihood = successes / trials if trials > 0 else 0
            bayes_factor = likelihood / BACK_PRIOR if BACK_PRIOR > 0 else 1

            back_posterior[ball] = {
                'åéªŒå‡å€¼': posterior_mean,
                'è´å¶æ–¯å› å­': bayes_factor,
                'è§‚æµ‹æ¬¡æ•°': successes,
                'æ€»è¯•éªŒæ¬¡æ•°': trials
            }

        results['å‰åŒºåéªŒåˆ†æ'] = front_posterior
        results['ååŒºåéªŒåˆ†æ'] = back_posterior

        if explain:
            avg_bayes_factor = np.mean([info['è´å¶æ–¯å› å­'] for info in front_posterior.values()])
            print(f"   ğŸ§® å¹³å‡è´å¶æ–¯å› å­: {avg_bayes_factor:.3f}")
            print(f"   ğŸ§® å‰åŒºè§‚æµ‹æœŸæ•°: {len(data)}")

        return results

    def _hot_cold_analysis(self, data, explain=True):
        """å†·çƒ­å·åˆ†ææ¨¡å—"""
        results = {}

        # å¤šæ—¶é—´çª—å£åˆ†æ
        windows = [10, 20, 30]  # æœ€è¿‘10æœŸã€20æœŸã€30æœŸ

        front_heat_analysis = {}
        back_heat_analysis = {}

        for window in windows:
            if len(data) < window:
                continue

            recent_data = data.tail(window)

            # è®¡ç®—çƒ­åº¦æŒ‡æ•°
            front_counts = Counter()
            back_counts = Counter()

            for _, row in recent_data.iterrows():
                front_balls = self.parse_balls(row['front_balls'])
                back_balls = self.parse_balls(row['back_balls'])

                for ball in front_balls:
                    front_counts[ball] += 1
                for ball in back_balls:
                    back_counts[ball] += 1

            # è®¡ç®—çƒ­åº¦æŒ‡æ•° = å®é™…é¢‘ç‡ / æœŸæœ›é¢‘ç‡
            expected_front = window * 5 / 35
            expected_back = window * 2 / 12

            front_heat = {}
            for ball in range(1, 36):
                actual_freq = front_counts.get(ball, 0)
                heat_index = actual_freq / expected_front if expected_front > 0 else 0
                front_heat[ball] = heat_index

            back_heat = {}
            for ball in range(1, 13):
                actual_freq = back_counts.get(ball, 0)
                heat_index = actual_freq / expected_back if expected_back > 0 else 0
                back_heat[ball] = heat_index

            front_heat_analysis[f'{window}æœŸ'] = front_heat
            back_heat_analysis[f'{window}æœŸ'] = back_heat

        # ç»¼åˆçƒ­åº¦è¯„åˆ†ï¼ˆå¤šå‘¨æœŸåŠ æƒå¹³å‡ï¼‰
        front_comprehensive_heat = {}
        back_comprehensive_heat = {}

        for ball in range(1, 36):
            heat_scores = []
            for window_key, heat_data in front_heat_analysis.items():
                if ball in heat_data:
                    # ä¸­å¿ƒåŒ–å¤„ç†ï¼šå‡å»1.0ï¼Œä½¿å¾—å¹³å‡çƒ­åº¦ä¸º0
                    centered_heat = (heat_data[ball] - 1.0) * 0.5
                    heat_scores.append(centered_heat)

            front_comprehensive_heat[ball] = np.mean(heat_scores) if heat_scores else 0

        for ball in range(1, 13):
            heat_scores = []
            for window_key, heat_data in back_heat_analysis.items():
                if ball in heat_data:
                    centered_heat = (heat_data[ball] - 1.0) * 0.5
                    heat_scores.append(centered_heat)

            back_comprehensive_heat[ball] = np.mean(heat_scores) if heat_scores else 0

        results['å‰åŒºçƒ­åº¦åˆ†æ'] = front_heat_analysis
        results['ååŒºçƒ­åº¦åˆ†æ'] = back_heat_analysis
        results['å‰åŒºç»¼åˆçƒ­åº¦'] = front_comprehensive_heat
        results['ååŒºç»¼åˆçƒ­åº¦'] = back_comprehensive_heat

        # åˆ†ç±»å†·çƒ­å·
        front_hot = [ball for ball, heat in front_comprehensive_heat.items() if heat > 0.3]
        front_cold = [ball for ball, heat in front_comprehensive_heat.items() if heat < -0.3]

        if explain:
            print(f"   ğŸŒ¡ï¸ å‰åŒºçƒ­å·: {len(front_hot)} ä¸ª")
            print(f"   ğŸŒ¡ï¸ å‰åŒºå†·å·: {len(front_cold)} ä¸ª")
            if front_hot:
                print(f"   ğŸŒ¡ï¸ çƒ­å·ç¤ºä¾‹: {sorted(front_hot)[:5]}")

        return results

    def _cycle_analysis(self, data, explain=True):
        """å‘¨æœŸæ€§åˆ†ææ¨¡å—"""
        results = {}

        # æ„å»ºæ—¶é—´åºåˆ—æ•°æ®
        front_series = []
        back_series = []

        for _, row in data.iterrows():
            front_balls = self.parse_balls(row['front_balls'])
            back_balls = self.parse_balls(row['back_balls'])

            # ä½¿ç”¨å’Œå€¼ä½œä¸ºæ—¶é—´åºåˆ—ç‰¹å¾
            front_series.append(sum(front_balls))
            back_series.append(sum(back_balls))

        # FFTé¢‘åŸŸåˆ†æ
        if len(front_series) >= 16:  # è‡³å°‘éœ€è¦16ä¸ªæ•°æ®ç‚¹
            front_fft = np.abs(fft(front_series))
            back_fft = np.abs(fft(back_series))

            # æ‰¾åˆ°ä¸»è¦é¢‘ç‡æˆåˆ†
            front_freqs = np.fft.fftfreq(len(front_series))
            back_freqs = np.fft.fftfreq(len(back_series))

            # æ’é™¤ç›´æµåˆ†é‡ï¼Œæ‰¾åˆ°æœ€å¼ºçš„å‘¨æœŸ
            front_main_freq_idx = np.argmax(front_fft[1:len(front_fft)//2]) + 1
            back_main_freq_idx = np.argmax(back_fft[1:len(back_fft)//2]) + 1

            front_period = 1 / abs(front_freqs[front_main_freq_idx]) if front_freqs[front_main_freq_idx] != 0 else float('inf')
            back_period = 1 / abs(back_freqs[back_main_freq_idx]) if back_freqs[back_main_freq_idx] != 0 else float('inf')

            results['å‰åŒºä¸»å‘¨æœŸ'] = front_period
            results['ååŒºä¸»å‘¨æœŸ'] = back_period
            results['å‰åŒºé¢‘è°±å¼ºåº¦'] = front_fft[front_main_freq_idx]
            results['ååŒºé¢‘è°±å¼ºåº¦'] = back_fft[back_main_freq_idx]
        else:
            results['å‰åŒºä¸»å‘¨æœŸ'] = float('inf')
            results['ååŒºä¸»å‘¨æœŸ'] = float('inf')
            results['å‰åŒºé¢‘è°±å¼ºåº¦'] = 0
            results['ååŒºé¢‘è°±å¼ºåº¦'] = 0

        # è‡ªç›¸å…³åˆ†æ
        front_autocorr = self._calculate_autocorrelation(front_series, max_lag=min(20, len(front_series)//2))
        back_autocorr = self._calculate_autocorrelation(back_series, max_lag=min(10, len(back_series)//2))

        results['å‰åŒºè‡ªç›¸å…³'] = front_autocorr
        results['ååŒºè‡ªç›¸å…³'] = back_autocorr

        # è¶‹åŠ¿åˆ†æ
        front_trend = np.polyfit(range(len(front_series)), front_series, 1)[0]
        back_trend = np.polyfit(range(len(back_series)), back_series, 1)[0]

        results['å‰åŒºè¶‹åŠ¿'] = front_trend
        results['ååŒºè¶‹åŠ¿'] = back_trend

        if explain:
            print(f"   ğŸ”„ å‰åŒºä¸»å‘¨æœŸ: {front_period:.1f} æœŸ" if front_period != float('inf') else "   ğŸ”„ å‰åŒºä¸»å‘¨æœŸ: æ— æ˜æ˜¾å‘¨æœŸ")
            print(f"   ğŸ”„ å‰åŒºè¶‹åŠ¿: {'ä¸Šå‡' if front_trend > 0 else 'ä¸‹é™' if front_trend < 0 else 'å¹³ç¨³'}")

        return results

    def _calculate_autocorrelation(self, series, max_lag):
        """è®¡ç®—è‡ªç›¸å…³å‡½æ•°"""
        autocorr = {}
        series = np.array(series)
        n = len(series)

        # æ ‡å‡†åŒ–
        mean = np.mean(series)
        std = np.std(series)

        if std == 0:
            return {lag: 0 for lag in range(max_lag + 1)}

        normalized_series = (series - mean) / std

        for lag in range(max_lag + 1):
            if lag == 0:
                autocorr[lag] = 1.0
            elif lag < n:
                correlation = np.corrcoef(normalized_series[:-lag], normalized_series[lag:])[0, 1]
                autocorr[lag] = correlation if not np.isnan(correlation) else 0
            else:
                autocorr[lag] = 0

        return autocorr

    def _correlation_analysis(self, data, explain=True):
        """ç›¸å…³æ€§åˆ†ææ¨¡å—"""
        results = {}

        # æ„å»ºç‰¹å¾çŸ©é˜µ
        features = []

        for _, row in data.iterrows():
            front_balls = self.parse_balls(row['front_balls'])
            back_balls = self.parse_balls(row['back_balls'])

            # ç‰¹å¾å·¥ç¨‹
            feature_vector = [
                sum(front_balls),                    # å‰åŒºå’Œå€¼
                max(front_balls) - min(front_balls), # å‰åŒºè·¨åº¦
                len(set(front_balls)),               # å‰åŒºå”¯ä¸€æ•°ï¼ˆåº”è¯¥æ€»æ˜¯5ï¼‰
                sum(back_balls),                     # ååŒºå’Œå€¼
                max(back_balls) - min(back_balls),   # ååŒºè·¨åº¦
                # å¥‡å¶æ¯”ä¾‹
                sum(1 for ball in front_balls if ball % 2 == 1) / 5,  # å‰åŒºå¥‡æ•°æ¯”ä¾‹
                sum(1 for ball in back_balls if ball % 2 == 1) / 2,   # ååŒºå¥‡æ•°æ¯”ä¾‹
                # å¤§å°æ¯”ä¾‹
                sum(1 for ball in front_balls if ball > 17.5) / 5,    # å‰åŒºå¤§æ•°æ¯”ä¾‹
                sum(1 for ball in back_balls if ball > 6) / 2,        # ååŒºå¤§æ•°æ¯”ä¾‹
            ]
            features.append(feature_vector)

        features = np.array(features)

        if len(features) > 5:
            # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
            correlation_matrix = np.corrcoef(features.T)

            # PCAåˆ†æ
            scaler = StandardScaler()
            scaled_features = scaler.fit_transform(features)

            pca = PCA()
            pca_result = pca.fit_transform(scaled_features)

            # ä¸»æˆåˆ†è´¡çŒ®ç‡
            explained_variance_ratio = pca.explained_variance_ratio_

            results['ç›¸å…³æ€§çŸ©é˜µ'] = correlation_matrix.tolist()
            results['ä¸»æˆåˆ†è´¡çŒ®ç‡'] = explained_variance_ratio.tolist()
            results['ç´¯è®¡è´¡çŒ®ç‡'] = np.cumsum(explained_variance_ratio).tolist()

            # ç‰¹å¾é‡è¦æ€§
            feature_names = ['å‰åŒºå’Œå€¼', 'å‰åŒºè·¨åº¦', 'å‰åŒºå”¯ä¸€æ•°', 'ååŒºå’Œå€¼', 'ååŒºè·¨åº¦',
                           'å‰åŒºå¥‡æ•°æ¯”ä¾‹', 'ååŒºå¥‡æ•°æ¯”ä¾‹', 'å‰åŒºå¤§æ•°æ¯”ä¾‹', 'ååŒºå¤§æ•°æ¯”ä¾‹']

            # ç¬¬ä¸€ä¸»æˆåˆ†çš„ç‰¹å¾æƒé‡
            first_pc_weights = pca.components_[0]
            feature_importance = {name: abs(weight) for name, weight in zip(feature_names, first_pc_weights)}

            results['ç‰¹å¾é‡è¦æ€§'] = feature_importance

            if explain:
                print(f"   ğŸ” ç¬¬ä¸€ä¸»æˆåˆ†è´¡çŒ®ç‡: {explained_variance_ratio[0]:.3f}")
                most_important = max(feature_importance.items(), key=lambda x: x[1])
                print(f"   ğŸ” æœ€é‡è¦ç‰¹å¾: {most_important[0]} ({most_important[1]:.3f})")
        else:
            results['ç›¸å…³æ€§çŸ©é˜µ'] = []
            results['ä¸»æˆåˆ†è´¡çŒ®ç‡'] = []
            results['ç‰¹å¾é‡è¦æ€§'] = {}

        return results

    def _generate_predictions(self, hybrid_analysis, data, count, explain=True):
        """ç”Ÿæˆé¢„æµ‹ç»“æœ"""
        if explain:
            print("\nğŸ¯ å¼€å§‹ç”Ÿæˆé¢„æµ‹...")

        # è·å–æœ€è¿‘ä¸€æœŸçš„å·ç ä½œä¸ºé©¬å°”å¯å¤«é“¾çš„èµ·å§‹çŠ¶æ€
        latest_row = data.iloc[-1]
        latest_front = self.parse_balls(latest_row['front_balls'])
        latest_back = self.parse_balls(latest_row['back_balls'])

        predictions = []
        used_combinations = set()

        for prediction_num in range(count):
            if explain:
                print(f"\nğŸ”® ç”Ÿæˆç¬¬ {prediction_num + 1} æ³¨é¢„æµ‹...")

            # è®¡ç®—ç»¼åˆè¯„åˆ†
            front_scores, back_scores = self._calculate_comprehensive_scores(
                hybrid_analysis, latest_front, latest_back, prediction_num, explain
            )

            # é€‰æ‹©å·ç 
            front_balls, back_balls = self._select_numbers_with_diversity(
                front_scores, back_scores, prediction_num, used_combinations
            )

            # è®°å½•å·²ä½¿ç”¨çš„ç»„åˆ
            combination = (tuple(sorted(front_balls)), tuple(sorted(back_balls)))
            used_combinations.add(combination)

            predictions.append((front_balls, back_balls))

            if explain:
                front_str = ' '.join([str(b).zfill(2) for b in sorted(front_balls)])
                back_str = ' '.join([str(b).zfill(2) for b in sorted(back_balls)])
                print(f"   ç¬¬ {prediction_num + 1} æ³¨: å‰åŒº {front_str} | ååŒº {back_str}")

        return predictions

    def _calculate_comprehensive_scores(self, hybrid_analysis, latest_front, latest_back, prediction_num, explain=True):
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        front_scores = {i: 0.0 for i in range(1, 36)}
        back_scores = {i: 0.0 for i in range(1, 13)}

        if explain:
            print("   ğŸ“Š å¤šæ¨¡å‹è¯„åˆ†è®¡ç®—:")

        # 1. ç»Ÿè®¡å­¦æ¨¡å‹è¯„åˆ† (15%)
        self._apply_statistical_scoring(front_scores, back_scores, hybrid_analysis['statistical'], 0.15, explain)

        # 2. æ¦‚ç‡è®ºæ¨¡å‹è¯„åˆ† (20%)
        self._apply_probability_scoring(front_scores, back_scores, hybrid_analysis['probability'], 0.20, explain)

        # 3. é©¬å°”å¯å¤«é“¾æ¨¡å‹è¯„åˆ† (25%)
        self._apply_markov_scoring(front_scores, back_scores, hybrid_analysis['markov'],
                                 latest_front, latest_back, 0.25, explain)

        # 4. è´å¶æ–¯æ¨¡å‹è¯„åˆ† (15%)
        self._apply_bayesian_scoring(front_scores, back_scores, hybrid_analysis['bayesian'], 0.15, explain)

        # 5. å†·çƒ­å·æ¨¡å‹è¯„åˆ† (15%)
        self._apply_hot_cold_scoring(front_scores, back_scores, hybrid_analysis['hot_cold'], 0.15, explain)

        # 6. å‘¨æœŸæ€§æ¨¡å‹è¯„åˆ† (10%)
        self._apply_cycle_scoring(front_scores, back_scores, hybrid_analysis['cycle'], 0.10, explain)

        return front_scores, back_scores

    def _apply_statistical_scoring(self, front_scores, back_scores, analysis, weight, explain=True):
        """åº”ç”¨ç»Ÿè®¡å­¦è¯„åˆ†"""
        if 'å‰åŒºè¯„åˆ†' in analysis:
            for ball, score in analysis['å‰åŒºè¯„åˆ†'].items():
                front_scores[ball] += score * weight

        if 'ååŒºè¯„åˆ†' in analysis:
            for ball, score in analysis['ååŒºè¯„åˆ†'].items():
                back_scores[ball] += score * weight

        if explain:
            print(f"     âœ“ ç»Ÿè®¡å­¦è¯„åˆ† (æƒé‡: {weight:.0%})")

    def _apply_probability_scoring(self, front_scores, back_scores, analysis, weight, explain=True):
        """åº”ç”¨æ¦‚ç‡è®ºè¯„åˆ†"""
        if 'å‰åŒºæ¦‚ç‡' in analysis:
            for ball, prob in analysis['å‰åŒºæ¦‚ç‡'].items():
                front_scores[ball] += prob * weight * 20  # æ”¾å¤§æ¦‚ç‡å€¼

        if 'ååŒºæ¦‚ç‡' in analysis:
            for ball, prob in analysis['ååŒºæ¦‚ç‡'].items():
                back_scores[ball] += prob * weight * 20

        if explain:
            print(f"     âœ“ æ¦‚ç‡è®ºè¯„åˆ† (æƒé‡: {weight:.0%})")

    def _apply_markov_scoring(self, front_scores, back_scores, analysis, latest_front, latest_back, weight, explain=True):
        """åº”ç”¨é©¬å°”å¯å¤«é“¾è¯„åˆ†"""
        # å‰åŒºé©¬å°”å¯å¤«è¯„åˆ†
        if 'å‰åŒºè½¬ç§»æ¦‚ç‡' in analysis:
            front_transitions = analysis['å‰åŒºè½¬ç§»æ¦‚ç‡']
            for current_ball in latest_front:
                if current_ball in front_transitions:
                    for next_ball, info in front_transitions[current_ball].items():
                        prob = info.get('æ¦‚ç‡', 0)
                        front_scores[next_ball] += prob * weight

        # ååŒºé©¬å°”å¯å¤«è¯„åˆ†
        if 'ååŒºè½¬ç§»æ¦‚ç‡' in analysis:
            back_transitions = analysis['ååŒºè½¬ç§»æ¦‚ç‡']
            for current_ball in latest_back:
                if current_ball in back_transitions:
                    for next_ball, info in back_transitions[current_ball].items():
                        prob = info.get('æ¦‚ç‡', 0)
                        back_scores[next_ball] += prob * weight

        if explain:
            print(f"     âœ“ é©¬å°”å¯å¤«é“¾è¯„åˆ† (æƒé‡: {weight:.0%})")

    def _apply_bayesian_scoring(self, front_scores, back_scores, analysis, weight, explain=True):
        """åº”ç”¨è´å¶æ–¯è¯„åˆ†"""
        if 'å‰åŒºåéªŒåˆ†æ' in analysis:
            for ball, info in analysis['å‰åŒºåéªŒåˆ†æ'].items():
                posterior_mean = info.get('åéªŒå‡å€¼', 0)
                front_scores[ball] += posterior_mean * weight * 10  # æ”¾å¤§åéªŒæ¦‚ç‡

        if 'ååŒºåéªŒåˆ†æ' in analysis:
            for ball, info in analysis['ååŒºåéªŒåˆ†æ'].items():
                posterior_mean = info.get('åéªŒå‡å€¼', 0)
                back_scores[ball] += posterior_mean * weight * 10

        if explain:
            print(f"     âœ“ è´å¶æ–¯è¯„åˆ† (æƒé‡: {weight:.0%})")

    def _apply_hot_cold_scoring(self, front_scores, back_scores, analysis, weight, explain=True):
        """åº”ç”¨å†·çƒ­å·è¯„åˆ†"""
        if 'å‰åŒºç»¼åˆçƒ­åº¦' in analysis:
            for ball, heat in analysis['å‰åŒºç»¼åˆçƒ­åº¦'].items():
                front_scores[ball] += heat * weight

        if 'ååŒºç»¼åˆçƒ­åº¦' in analysis:
            for ball, heat in analysis['ååŒºç»¼åˆçƒ­åº¦'].items():
                back_scores[ball] += heat * weight

        if explain:
            print(f"     âœ“ å†·çƒ­å·è¯„åˆ† (æƒé‡: {weight:.0%})")

    def _apply_cycle_scoring(self, front_scores, back_scores, analysis, weight, explain=True):
        """åº”ç”¨å‘¨æœŸæ€§è¯„åˆ†"""
        # åŸºäºè¶‹åŠ¿çš„ç®€å•è¯„åˆ†
        front_trend = analysis.get('å‰åŒºè¶‹åŠ¿', 0)
        back_trend = analysis.get('ååŒºè¶‹åŠ¿', 0)

        # å¦‚æœæœ‰ä¸Šå‡è¶‹åŠ¿ï¼Œç»™è¾ƒå¤§å·ç æ›´é«˜åˆ†æ•°
        if front_trend > 0:
            for ball in range(18, 36):
                front_scores[ball] += weight * 0.1
        elif front_trend < 0:
            for ball in range(1, 18):
                front_scores[ball] += weight * 0.1

        if back_trend > 0:
            for ball in range(7, 13):
                back_scores[ball] += weight * 0.1
        elif back_trend < 0:
            for ball in range(1, 7):
                back_scores[ball] += weight * 0.1

        if explain:
            print(f"     âœ“ å‘¨æœŸæ€§è¯„åˆ† (æƒé‡: {weight:.0%})")

    def _select_numbers_with_diversity(self, front_scores, back_scores, prediction_num, used_combinations):
        """å¸¦å¤šæ ·æ€§çš„å·ç é€‰æ‹©"""
        # å·®å¼‚åŒ–é€‰æ‹©ç­–ç•¥
        choice_offset = prediction_num * 0.1

        # å‰åŒºå·ç é€‰æ‹©
        sorted_front = sorted(front_scores.items(), key=lambda x: x[1], reverse=True)

        front_balls = []
        for i, (ball, score) in enumerate(sorted_front):
            # å¼•å…¥éšæœºæ€§ï¼Œé¿å…æ€»æ˜¯é€‰æ‹©æœ€é«˜åˆ†
            if len(front_balls) < 5:
                # ç¬¬1æ³¨é€‰æ‹©æœ€é«˜åˆ†ï¼Œåç»­æ³¨æ•°å¼•å…¥åç§»
                if prediction_num == 0 or np.random.random() > choice_offset:
                    front_balls.append(ball)
                elif i < len(sorted_front) - 1:
                    # è·³è¿‡å½“å‰é€‰æ‹©ï¼Œé€‰æ‹©ä¸‹ä¸€ä¸ª
                    continue

        # å¦‚æœé€‰æ‹©ä¸è¶³5ä¸ªï¼Œè¡¥å……å‰©ä½™çš„é«˜åˆ†å·ç 
        if len(front_balls) < 5:
            remaining_balls = [ball for ball, _ in sorted_front if ball not in front_balls]
            front_balls.extend(remaining_balls[:5-len(front_balls)])

        front_balls = sorted(front_balls[:5])

        # ååŒºå·ç é€‰æ‹©
        sorted_back = sorted(back_scores.items(), key=lambda x: x[1], reverse=True)

        back_balls = []
        for i, (ball, score) in enumerate(sorted_back):
            if len(back_balls) < 2:
                if prediction_num == 0 or np.random.random() > choice_offset:
                    back_balls.append(ball)
                elif i < len(sorted_back) - 1:
                    continue

        if len(back_balls) < 2:
            remaining_balls = [ball for ball, _ in sorted_back if ball not in back_balls]
            back_balls.extend(remaining_balls[:2-len(back_balls)])

        back_balls = sorted(back_balls[:2])

        # æ£€æŸ¥æ˜¯å¦ä¸å·²æœ‰ç»„åˆé‡å¤
        combination = (tuple(front_balls), tuple(back_balls))
        if combination in used_combinations:
            # å¦‚æœé‡å¤ï¼Œéšæœºè°ƒæ•´ä¸€ä¸ªå·ç 
            import random
            if random.random() < 0.5 and len(front_balls) > 0:
                # è°ƒæ•´å‰åŒº
                replace_idx = random.randint(0, len(front_balls) - 1)
                available_balls = [ball for ball in range(1, 36) if ball not in front_balls]
                if available_balls:
                    front_balls[replace_idx] = random.choice(available_balls)
                    front_balls = sorted(front_balls)
            else:
                # è°ƒæ•´ååŒº
                if len(back_balls) > 0:
                    replace_idx = random.randint(0, len(back_balls) - 1)
                    available_balls = [ball for ball in range(1, 13) if ball not in back_balls]
                    if available_balls:
                        back_balls[replace_idx] = random.choice(available_balls)
                        back_balls = sorted(back_balls)

        return front_balls, back_balls

    def _save_analysis_results(self, hybrid_analysis, predictions, periods):
        """ä¿å­˜åˆ†æç»“æœ"""
        try:
            # ä¿å­˜è¯¦ç»†åˆ†æç»“æœ
            analysis_file = os.path.join(self.output_dir, f"hybrid_analysis_{periods}periods.json")

            # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
            serializable_analysis = self._make_serializable(hybrid_analysis)

            with open(analysis_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_analysis, f, ensure_ascii=False, indent=2, default=str)

            # ä¿å­˜é¢„æµ‹ç»“æœ
            predictions_file = os.path.join(self.output_dir, f"predictions_{periods}periods.json")

            predictions_data = {
                'timestamp': datetime.now().isoformat(),
                'periods': periods,
                'model_weights': self.model_weights,
                'predictions': [
                    {
                        'index': i + 1,
                        'front_balls': front_balls,
                        'back_balls': back_balls,
                        'formatted': f"å‰åŒº {' '.join([str(b).zfill(2) for b in sorted(front_balls)])} | ååŒº {' '.join([str(b).zfill(2) for b in sorted(back_balls)])}"
                    }
                    for i, (front_balls, back_balls) in enumerate(predictions)
                ]
            }

            with open(predictions_file, 'w', encoding='utf-8') as f:
                json.dump(predictions_data, f, ensure_ascii=False, indent=2)

            print(f"\nğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜:")
            print(f"   ğŸ“„ è¯¦ç»†åˆ†æ: {analysis_file}")
            print(f"   ğŸ¯ é¢„æµ‹ç»“æœ: {predictions_file}")

        except Exception as e:
            print(f"ä¿å­˜åˆ†æç»“æœå¤±è´¥: {e}")

    def _make_serializable(self, obj):
        """å°†å¯¹è±¡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼"""
        if isinstance(obj, dict):
            return {key: self._make_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.integer, np.floating)):
            return obj.item()
        elif isinstance(obj, np.bool_):
            return bool(obj)
        else:
            return obj


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="é«˜çº§æ··åˆåˆ†æé¢„æµ‹ç³»ç»Ÿ")
    parser.add_argument("-d", "--data", default="data/dlt_data_all.csv", help="æ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("-p", "--periods", type=int, default=100, help="åˆ†ææœŸæ•°")
    parser.add_argument("-c", "--count", type=int, default=1, help="é¢„æµ‹æ³¨æ•°")
    parser.add_argument("--explain", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†åˆ†æè¿‡ç¨‹")

    args = parser.parse_args()

    if not os.path.exists(args.data):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.data}")
        print("è¯·å…ˆè¿è¡Œæ•°æ®çˆ¬è™«è·å–æ•°æ®")
        return

    # åˆ›å»ºåˆ†æå™¨
    analyzer = AdvancedHybridAnalyzer(args.data)

    # æ‰§è¡Œæ··åˆåˆ†æé¢„æµ‹
    predictions = analyzer.predict_with_hybrid_analysis(
        periods=args.periods,
        count=args.count,
        explain=args.explain
    )

    if predictions:
        print(f"\nğŸ‰ é«˜çº§æ··åˆåˆ†æé¢„æµ‹å®Œæˆï¼")
        print(f"ğŸ“Š åŸºäº {args.periods} æœŸæ•°æ®çš„ {len(predictions)} æ³¨é¢„æµ‹:")

        for i, (front_balls, back_balls) in enumerate(predictions, 1):
            front_str = ' '.join([str(b).zfill(2) for b in sorted(front_balls)])
            back_str = ' '.join([str(b).zfill(2) for b in sorted(back_balls)])
            print(f"ç¬¬ {i} æ³¨: å‰åŒº {front_str} | ååŒº {back_str}")
    else:
        print("âŒ é¢„æµ‹å¤±è´¥")


if __name__ == "__main__":
    main()
